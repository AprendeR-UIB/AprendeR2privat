--- 
title: "AprendeR: Parte II"
author: "The AprendeR team"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: 
   bookdown::pdf_book: default
documentclass: book
#bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: cescrossello/AprendeR-II
description: "Apuntes AprendeR bookdown::gitbook."
---

```{r global_options,  include=FALSE}
library(knitr)
#library(xtable)
#library(printr)
knitr::opts_chunk$set(
  fig.width=5, 
  fig.asp = 1,
  fig.align="center", 
  fig.show = "hold",
  echo=TRUE, 
  warning=FALSE, 
  message=FALSE
#  tidy.opts=list(width.cutoff=60),
#  tidy=TRUE
)
knitr::opts_knit$set(global.par=TRUE)
options(knitr.graphics.auto_pdf=TRUE)
warning.length=100
```

```{r,  include=FALSE}
par(cex.main=0.9,cex.axis=0.8,cex.lab=0.8)
```


```{r, paquets, eval=FALSE, include=FALSE}
#https://liao961120.github.io/2019/03/10/include_graphics2.html
library(linguisticsdown)
library(MASS)
library(epitools)
library(EnvStats)
library(boot)
library(BSDA)
library(TeachingDemos)
library(pwr)
library(car)
library(ape)
library(nortest)
library(fBasics)
library(RCurl)
library(scatterplot3d)
library(gplots)
library(RColorBrewer)
library(faraway)
library(UsingR)
library(agricolae)
library(WRS2)
library(leaps)
library(mlBench)
library(AppliedPredictiveModeling)
library(linguisticsdown)
```

\renewcommand\chaptername{Lección}
\renewcommand{\leq}{\leqslant}
\renewcommand{\geq}{\geqslant}


# Presentación {-}

Esto es una edición preliminar en línea de la 2a parte del libro "AprendeR".

El libro está escrito en *R Markdown*, usando *RStudio* como editor de texto y el paquete **bookdown** para convertir los ficheros *markdown* en un libro. 

Este trabajo se publica bajo licencia [Atribución-No Comercial-SinDerivadas 4.0](https://creativecommons.org/licenses/by-nc-nd/4.0/)


<!--chapter:end:index.Rmd-->

# (PART\*) Parte II: Estadística inferencial {-}

# Distribuciones de probabilidad {#chap:distr}

R conoce los tipos de distribución de probabilidad más importantes, incluyendo las que mostramos en la  tabla siguiente: 


```{r, echo=FALSE}
df.aux=cbind( 
Distribución=c("Binomial",
"Geométrica",
"Hipergeomètrica",
"Poisson",
"Uniforme",
"Exponencial",
"Normal",
"Khi cuadrado",
"t de Student",
"F de Fisher"),
Nombre=c("binom","geom","hyper","pois","unif","exp","norm","chisq","t","f"),
Parámetros=c("medida de la muestra, probabilidad de éxito",
"probabilidad",
"N,M,n",
"Esperanza",
"mínimo, máximo",
"lambda",
"media, desviación típica",
"grados de libertad",
"grados de libertad",
"los dos grados de libertad")
)
```

```{r dist, include=FALSE}
knitr::kable(
df.aux,
  caption = 'Nombres de distribuciones en R', booktabs = FALSE
)
```


$$
\begin{array}{lll}
\hline
\textbf{Distribución} & {\textbf{Nombre en R}} & {\textbf{Parámetros}}\\ \hline
\mbox{Binomial} &{\texttt{binom}} & \mbox{tamaño de la muestra $n$, probabilidad $p$}\\
\mbox{Geométrica} & {\texttt{geom}} & \mbox{$p$}\\
\mbox{Hipergeométrica} & {\texttt{hyper}} & \mbox{tamaño de la población $N$, número poblacional}\\[-0.75ex] 
& & \mbox{de éxitos $M$, tamaño de la muestra $n$}\\
\mbox{Poisson} & {\texttt{pois}} & \mbox{esperanza $\lambda$}\\
\mbox{Uniforme} & {\texttt{unif}} & \mbox{mínimo, máximo}\\
\mbox{Exponencial} & {\texttt{exp}} & \lambda\\
\mbox{Normal} & {\texttt{norm}} & \mbox{media $\mu$, desviación típica $\sigma$}\\
\mbox{Khi cuadrado} & {\texttt{chisq}} & \mbox{número de grados de libertad $df$}\\
\mbox{t de Student} & {\texttt{t}} & \mbox{número de grados de libertad $df$}\\
\mbox{F de Fisher} & {\texttt{f}} & \mbox{los dos números de  grados de libertad}
\\ \hline
\end{array}
$$

Para cada una de estas distribuciones, R sabe calcular cuatro funciones, que se obtienen añadiendo un prefijo al nombre de la distribución: 

* La función de densidad, con el prefijo **d**.

* La función de distribución de probabilidad, con el prefijo **p**; esta función dispone además del parámetro `lower.tail` que igualado a `FALSE` calcula la **función de distribución de cola superior**: la probabilidad de que una variable aleatoria con esta distribución de probabilidad tome un valor estrictamente mayor que uno dado.

* Los cuantiles, con el prefijo **q**.

* Vectores de números aleatorios con esta distribución, con el prefijo **r**.

La función correspondiente se aplica entonces al valor sobre el que queremos calcular la función y a los parámetros de la distribución (en este orden, y los parámetros en el orden en que los damos en la tabla anterior, cuando hay más de uno).

Por ejemplo, sea $X$ una variable aleatoria binomial $B(20,0.3)$, es decir,  de tamaño $n=20$ y probabilidad $p=0.3$,  y sean $f_X$ su función de densidad y $F_X$ su función de distribución. Calculemos algunos valores de funciones asociadas a esta variable aleatoria.

* $f_X(5)=P(X=5)$:

```{r}
dbinom(5,20,0.3)
```

|        Comprobémoslo, recordando que si $X\sim B(n,k)$, entonces $P(X=k)=\binom{n}{k}p^k(1-p)^{n-k}$:

```{r}
choose(20,5)*0.3^5*0.7^15
```



* $f_X(8)=P(X=8)$:

```{r}
dbinom(8,20,0.3)  
```

* $F_X(5)=P(X\leq 5)$:
```{r}
pbinom(5,20,0.3)  
```

|        Comprobémoslo, usando que $P(X\leq 5)=\sum_{k=0}^5 P(X=k)$:

```{r}
sum(dbinom(0:5,20,0.3))
```


* $F_X(8)=P(X\leq 8)$:

```{r}
pbinom(8,20,0.3)  
```

* $P(X>8)$

```{r}
pbinom(8,20,0.3,lower.tail=FALSE)
```

|        En efecto:

```{r}

1-pbinom(8,20,0.3)
```



* El cuantil de orden $0.5$ de $X$, o sea, su **mediana**: el valor $x$ más pequeño tal que $P(X\leq x)\geq 0.5$

```{r}
qbinom(0.5,20,0.3)  
```

|        Comprobemos que $P(X\leq `r qbinom(0.5,20,0.3)`)\geq 0.5$ y en cambio $P(X\leq `r qbinom(0.5,20,0.3)-1`)< 0.5$:

```{r}
pbinom(6,20,0.3) 
pbinom(5,20,0.3) 
```


* El cuantil de orden $0.25$ de $X$, es decir, su **primer cuartil**:


```{r}
qbinom(0.25,20,0.3)  
```

* Un vector aleatorio de 10 valores generado con la variable aleatoria $X$:

```{r}
rbinom(10,20,0.3)
```

* Dos vectores aleatorios más, de 10 valores cada uno, generados con la variable aleatoria $X$:


```{r}
rbinom(10,20,0.3) 
rbinom(10,20,0.3)
```



Del mismo modo, si estamos trabajando con una variable aleatoria $Y$ de Poisson con parámetro $\lambda=5$:


* $P(Y=8)$:

```{r}
dpois(8,5) 
```


* $P(Y\leq 8)$:

```{r}
ppois(8,5) 
```

* El cuantil de orden 0.6 de $Y$:

```{r}
qpois(0.6,5)
```

* Un vector aleatorio de 20 valores generado con la variable aleatoria $Y$:

```{r}
rpois(20,5)
```


Si no entramos ningún parámetro en las funciones asociadas a la distribución normal, R entiende que se trata de la normal estándar (con media $\mu=0$ y desviación típica $\sigma=1$): por ejemplo, las dos instrucciones siguientes nos dan el valor $f_Z(0.3)$ de la función densidad de una normal estándar $Z$ aplicada a 0.3 (que *no* es igual a $P(Z=0.3)$):

```{r}
dnorm(0.3)
dnorm(0.3,0,1)
```

Las funciones densidad y distribución de una variable aleatoria se pueden dibujar con la función `curve`. Así, la función siguiente dibuja la gráfica de la densidad de una variable normal estándard de la Figura \@ref(fig:dnorm):

```{r, fig.cap="Función densidad de una variable N(0,1).", label=dnorm}
curve(dnorm(x,0,1.5),-5,5,xlab="",ylab="",main="")
```

De manera similar, la función siguiente dibuja la gráfica de la función de distribución de una variable normal estándard de la Figura \@ref(fig:pnorm):

```{r, fig.cap="Función distribución de una variable N(0,1).", label=pnorm}
curve(pnorm(x,0,1.5),-5,5,xlab="",ylab="",main="")
```

## Ejercicios


### Test {-}

*(1)* Sea $f$ la función de densidad de una variable aleatoria normal con $\mu=0.2$ y $\sigma=1.2$. Dad el valor de $f(0.5)$ redondeado a 4 cifras decimales. 

*(2)* Sea $X$ una variable aleatoria normal con $\mu=0.2$ y $\sigma=1.2$. Dad el valor de $P(3\leq X\leq 7)$ redondeado a 4 cifras decimales. 

*(3)* Sea $X$ una variable aleatoria $B(10,0.2)$. Dad el valor de $P(3\leq X\leq 7)$ redondeado a 4 cifras decimales. 

*(4)* Dad una instrucción que calcule la mediana de una lista de 20 números aleatorios generados con distribución $B(10,0.2)$. No deis el resultado, solo la instrucción.

<!--
### Problemas {-}

**(1)** Como sabéis, si $X$ es una variable aleatoria Bernoulli $Be(p)$, es decir, de probabilidad de éxito $p$, e $Y$ es la variable aleatoria que nos da el número de fracasos antes de obtener el primer éxito en una secuencia de repeticiones independientes de $X$, entonce $Y$ sigue una  **distribución geométrica** $Ge(p)$.  

Por lo tanto, si efectuamos una serie de lanzamientos independientes de una moneda no trucada, las longitudes de las **rachas de cruces** (las secuencias de cruces entre pares de caras consecutivas) que nos salgan seguirán una distribución geométrica $Ge(0.5)$, porque nos dan los números de cruces (fracasos) obtenidos antes de sacar una cara (éxito) en repeticiones independientes de una v.a. $Be(0.5)$, reiniciando el contador en cada cara. Vamos a comprobarlo a mediante una simulación sencilla. 

*(a)* Generad un vector aleatorio binario $X$ de longitud 10^6^ con distribución Bernoulli $Be(0.5)$, que representará un millón de lanzamientos de una moneda honrada (0 cruz, 1 cara). Quedaos con el vector que va de su primer 1 a su último 1, y seguid llamándole $X$ al vector resultante.

*(b)* Calculad las longitudes de todas las secuencias maximales de 0s en $X$ (es decir, de las secuencias de 0s que separan pares de 1s consecutivos). Por lo que hemos explicado, estas longitudes deberían seguir una distribución geométrica $Ge(0.5)$.

*(c)* Dibujad un histograma de líneas de las frecuencias relativas de las longitudes de cadenas maximales de ceros obtenidas de esta manera, y superponed un diagrama poligonal de las probabilidades de dichas longitudes según la distribución $Ge(0.5)$. Para mejorar la visibilidad del gráfico, representad solo las longitudes $k$ cuya probabilidad bajo una distribución $Ge(0.5)$ sea al menos  0.0001. Comentad el gráfico obtenido.
-->

### Respuestas al test {-}

*(1)* `r round(dnorm(0.5,0.2,1.2),4)`

Nosotros lo hemos calculado con `round(dnorm(0.5,0.2,1.2),4)`.

*(2)* `r round(pnorm(7,0.2,1.2)-pnorm(3,0.2,1.2),4)`

Nosotros lo hemos calculado con `round(pnorm(7,0.2,1.2)-pnorm(3,0.2,1.2),4)`.

*(3)* `r round(pbinom(7,10,0.2)-pbinom(2,10,0.2),4)`

Nosotros lo hemos calculado con `round(pbinom(7,10,0.2)-pbinom(2,10,0.2),4)`. También se obtiene el resultado correcto con `round(sum(dbinom(3:7,10,0.2)),4)`. En cambio, con  `round(pbinom(7,10,0.2)-pbinom(3,10,0.2),4)` no se obtiene el resultado correcto: da `r round(pbinom(7,10,0.2)-pbinom(3,10,0.2),4)`. Pensad por qué aquí hay que restar `pbinom(2,10,0.2)` y en la pregunta anterior restábamos `pnorm(3,0.2,1.2)`.

*(4)* `median(rbinom(20,10,0.2))`

También sería correcto `median(rbinom(20,size=10,prob=0.2))`, pero no es necesario dar los nombres de los parámetros si entráis sus valores en el orden correcto, y por eso no los hemos explicado.

<!--
### Soluciones sucintas de los problemas  {-}

```{r,include=FALSE}
set.seed(42)
```

**(1)** *(a)* Como la distribución $Be(p)$ es la distribución binomial $B(1,p)$, podemos construir $X$ por medio de: 
```{r}
X=rbinom(10^6,1,0.5)
X=X[min(which(X==1)):max(which(X==1))]
```

*(b)* Las longitudes de cadenas maximales de 0s en $X$ serán las diferencias entre posiciones consecutivas de 1s en $X$, menos 1 (si, digamos, hay un 1 en la posición 5a y el siguiente 1 está en la posición 13a, en medio hay $13-5-1=`r 13-5-1`$ 0s). Por lo tanto, el vector $Y$ de longitudes de cadenas maximales de 0s en $X$ se puede calcular con:
```{r}
Exitos=which(X==1)
Y=diff(Exitos)-1
```

*(c)* Calculemos el mayor número natural $M$ tal que si $Y$ es una v.a. $Ge(0.5)$, $P(Y=M)\geq 10^{-4}$:
```{r}
M=max(which(dgeom(0:100,0.5)>=10^(-4)))-1
M
```

El gráfico pedido es entonces:
```{r}
n=0:M
plot(prop.table(table(Y)[n]),col="blue",type="h",xlab="Longitudes de cadenas maximales de 0s",ylab="Probabilidades")
points(n,dgeom(n,0.5),col="red",type="o",pch=20,cex=0.5)
```
-->

<!--chapter:end:01-Distribuciones.Rmd-->

# Conceptos básicos de muestreo {#chap:muestreo}

```{r, include=FALSE}
options(digits = 17)
```

En todo estudio estadístico hemos de distinguir entre **población**, que es un conjunto de sujetos con una o varias características que podemos medir y deseamos estudiar, y **muestra**, un subconjunto de una población. Por ejemplo, si quisiéramos estudiar alguna característica de los estudiantes de grado de la UIB, entenderíamos que estos forman la población de interés, y si entonces escogiéramos al azar 10 estudiantes de cada grado, obtendríamos una muestra de esta población. Pero también podríamos considerar los estudiantes de grado de la UIB como una muestra de la población de los estudiantes universitarios españoles: depende del estudio que queramos realizar.

Recordad que, cuando disponemos de un conjunto de datos obtenidos midiendo una o varias características sobre los sujetos de una muestra, podemos llevar a cabo dos tipos de análisis estadístico:

*  **Exploratorio** o  **descriptivo**: su objetivo es resumir, representar y explicar los datos de la muestra. Para llevarlo a cabo, se usan técnicas de  **estadística  descriptiva** como las que hemos descrito en lecciones anteriores. 

* **Inferencial** o **confirmatorio**: su objetivo es deducir (**inferir**),  a partir de los datos de la muestra, información significativa sobre el total de la población. A menudo esta inferencia pasa por contrastar una hipótesis sobre alguna propiedad de la población. Las técnicas que se usan en los análisis inferenciales forman la **estadística  inferencial**.

Por ejemplo, supongamos que hemos tomado una muestra de estudiantes de la UIB y sabemos sus calificaciones en un semestre concreto y sus números de hermanos. En un estudio exploratorio simplemente describiríamos estos datos mediante estadísticos y gráficos, mientras que usaríamos técnicas de estadística inferencial para deducir información sobre la población de todos los  estudiantes de la UIB a partir de esta muestra: ¿Cuál estimamos que ha sido la nota media de los estudiantes de la UIB en el semestre en cuestión? La distribución de los números de hermanos en estudiantes de la UIB, ¿es similar a la del conjunto de la población española? ¿Es verdad que los estudiantes de la UIB con más hermanos tienen tendencia a tener mejores  notas?

Un estudio inferencial suele desglosarse en los pasos siguientes:
  
1. Establecer la característica que se desea estimar o la hipótesis que se desea contrastar.

2. Determinar la información (los datos) que se necesita para hacerlo.

3. Diseñar un experimento que permita recoger estos datos; este paso incluye:

    * Decidir qué tipo de muestra se va a tomar y su tamaño.
   
    * Elegir las técnicas adecuadas para realizar las inferencias deseadas a partir de la muestra que se tomará.
   
4. Tomar una muestra y medir los datos deseados sobre los individuos que la forman.

5. Aplicar las técnicas de inferencia elegidas con el *software* adecuado.

6. Obtener conclusiones.

7.  Si las conclusiones son fiables y suficientes, redactar un informe; en caso contrario, volver a empezar.

En la próxima sección nos centraremos en las **técnicas de muestreo**: los métodos generales para seleccionar muestras representativas de una población que tenemos a nuestra disposición en el tercer paso de la lista anterior.

## Tipos de muestreo {#sec:muestreo}

Existen muchos tipos de muestreo, cada uno de los cuales proporciona una muestra representativa de la población en algún sentido. A continuación describimos de forma breve algunas de estas técnicas.

#### Muestreo aleatorio con y sin reposición {-}

Un **muestreo aleatorio** consiste en seleccionar una muestra de la población de manera que todas las muestras del mismo tamaño sean **equiprobables**; es decir, que si fijamos el número de individuos de la muestra, cualquier conjunto de ese número de individuos tenga la misma probabilidad de ser seleccionado. 

Hay dos tipos  básicos de muestreo aleatorio que vale la pena distinguir.   Para ilustrarlos, supongamos que disponemos de una urna con 100 bolas numeradas del 1 al 100, de la que queremos extraer una muestra de 15 bolas. La Figura \@ref(fig:base) representa dicha urna.



```{r, echo=FALSE, label=base,fig.cap="Una urna de 100 bolas"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/basev.png")
```


Una manera de hacerlo sería repetir 15 veces el proceso de sacar una bola de la urna, anotar su número y devolverla a la urna. El tipo de muestra obtenida de esta manera recibe el nombre de **muestra aleatoria con reposición**, o **simple** (una **m.a.s.**, para abreviar). Observad que con este procedimiento una misma bola puede aparecer varias veces en una muestra, y que todos los subconjuntos de 15 bolas "con posibles repeticiones"  tienen la misma probabilidad de obtenerse. Un posible resultado serían las bolas azules de la Figura \@ref(fig:simple); la bola azul más oscuro ha sido escogida dos veces en la muestra.


```{r, echo=FALSE, label=simple,fig.cap="Una muestra aleatoria simple"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/simplev.png")
```


Otra manera de extraer nuestra muestra sería repetir 15 veces el proceso de sacar una bola de la urna  pero ahora sin devolverla. Esto es equivalente a extraer de golpe 15 bolas de la urna. Estas muestras no tienen bolas repetidas, y cualquier selección de 15 bolas diferentes tiene la misma probabilidad de ser la obtenida.  En este caso se habla de una **muestra aleatoria sin reposición**. Un posible resultado serían las bolas azules de la Figura \@ref(fig:sinrep).


```{r, echo=FALSE, label=sinrep,fig.cap="Una muestra aleatoria sin reposición"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/sinrepv.png")
```


Cuando el tamaño de la población es muy grande en relación a la muestra, la probabilidad de que haya repeticiones en una muestra aleatoria simple es muy pequeña. Esto nos permite entender en este caso que los muestreos aleatorios con y sin reposición son equivalentes en el sentido siguiente: puesto que si la población es muy, muy grande, un muestreo con reposición daría muy probablemente una muestra con todos sus elementos diferentes, si tomamos directamente la muestra sin reposición podemos aceptar que permitíamos repeticiones, pero que no se han dado, y que por tanto es simple. 

A modo de ejemplo, vamos a calcular la probabilidad de al menos una repetición en muestras aleatorias simples de diferentes tamaños de una población de 12,000 individuos (aproximadamente, el número de estudiantes de la UIB) y representar estas probabilidades en un gráfico. Recordad que la probabilidad de que los sujetos de una muestra aleatoria simple de tamaño $n$  tomada de una población de $N$ individuos *no* sean todos diferentes es
$$
1-\frac{N(N-1)(N-2)\cdots (N-n+1)}{N^n}.
$$
Esta probabilidad es la que calcula la función `f(N,n)` del  bloque de código siguiente, y su gráfica es la de la Figura \@ref(fig:UIB). La curva negra representa las probabilidades deseadas. Hemos añadido al gráfico una línea horizontal que marca la probabilidad 0.01 y que muestra que la probabilidad de alguna repetición en una m.a.s. de 16 o menos estudiantes de la UIB es inferior al 1%: en más de 99 de cada 100 veces que tomemos una m.a.s. de a lo sumo 16 estudiantes, nos saldrán todos diferentes

```{r, fig.cap="Probabilidad de repetición en una m.a.s. de n estudiantes de la UIB", label="UIB"}
f=function(N,n){1-prod((N:(N-n+1))/N)}
prob=sapply(1:200,f,N=12000)
plot(1:200,prob,type="l",lwd=2,xlab="n",ylab="probabilidad",
  main="",xaxp=c(0,200,20),yaxp=c(0,1,10))
abline(h=0.01,col="red")
text(160,0.04,labels="probabilidad 0.01",col="red",cex=0.7)
```

Así, por ejemplo, una muestra aleatoria de 10 estudiantes diferentes de la UIB podría haberse obtenido perfectamente tomando los estudiantes con reposición, porque la probabilidad de alguna repetición en una m.a.s. como esta es muy pequeña: `r round(f(12000,10),3)`. En cambio,  es difícil de creer que una muestra aleatoria de 200 estudiantes diferentes de la UIB sea simple, porque la probabilidad de alguna repetición en una m.a.s. como esta es grande: `r round(f(12000,200),3)`. 

La mayoría de técnicas de estadística inferencial que se pueden usar para muestras aleatorias simples se pueden considerar igualmente válidas para muestras aleatorias sin reposición si el tamaño de la población es muy grande en relación al de la muestra (por dar una regla, digamos que,  al menos, unas 1000 veces mayor).  Si el tamaño de la población es relativamente pequeño por comparación a la muestra, algunas de estas técnicas se pueden salvar aplicando correcciones adecuadas para compensar la pequeñez de la población, y otras directamente pierden toda validez. 

En todo caso, conviene ser consciente de que si queremos tomar una muestra aleatoria con o sin reposición de una población, es necesario disponer de una lista completa de todos sus individuos  para poder sortear a quién vamos a seleccionar. Esto no siempre es posible. ¿Alguien tiene la  lista completa de, pongamos, todos los diabéticos de España? ¿Que incluya los que no saben que lo son? Por lo tanto, en la vida real no siempre podemos tomar muestras aleatorias en el sentido que hemos explicado.



#### Muestreo sistemático {-}

Una manera muy sencilla de obtener una muestra de una población cuando disponemos de una lista ordenada de sus individuos es tomarlos a intervalos constantes: cada quinto individuo, cada décimo individuo. Podemos añadir una componente aleatoria escogiendo al azar el primer individuo que elegimos, y a partir del cual empezamos a contar. Así, por ejemplo, si de una clase de 100 estudiantes quisiéramos escoger una muestra de 10, podríamos elegir un estudiante al azar, y a partir de él, por orden alfabético, elegir el décimo estudiante, el vigésimo, el trigésimo, etc.; si al llegar al final de la lista de clase no hubiéramos completado la muestra, volveríamos al principio de la misma. A esta técnica se la llama **muestreo sistemático**, **aleatorio** si además el primer sujeto se escoge de manera aleatoria. Por ejemplo, la Figura \@ref(fig:sist) describe una muestra aleatoria sistemática de 15 bolas de nuestra urna de 100 bolas: hemos empezado a escoger por la bola roja oscura, que ha sido elegida al azar, y a partir de ella hemos tomado 1 de cada 7 bolas, volviendo al principio cuando hemos llegado al final de la lista de bolas


```{r, echo=FALSE, label=sist,fig.cap="Una muestra aleatoria sistemática"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/sistv.png")
```


Cuando no disponemos de una lista de toda la población pero sí que tenemos una manera de acceder de manera ordenada a sujetos de la misma (por ejemplo, enfermos que acuden a un hospital), podemos realizar un muestreo sistemático tomando los sujetos a intervalos constantes a medida que los encontramos y hasta completar el tamaño deseado de la muestra. Por ejemplo, para escoger una muestra de 10 estudiantes de la UIB, podríamos escoger cada décimo estudiante que entrase en un edificio del Campus por una puerta concreta hasta llegar a los 10.

Cuando el orden de los individuos de la población en la lista es aleatorio, el muestreo sistemático aleatorio es equivalente al muestreo aleatorio sin reposición. Pero en general este no es el caso, y se pueden producir sesgos. Por poner un caso extremo, si una clase de 100 estudiantes estuviera formada por 50 parejas de hermanos y tomáramos una muestra sistemática de 50 estudiantes, eligiéndolos por orden alfabético de los apellidos uno sí, uno no, es seguro que no aparecería ninguna pareja de hermanos en la muestra (porque dos hermanos son siempre consecutivos en la lista, y en nuestra muestra no habría ningún par de sujetos consecutivos). En cambio, la probabilidad de que una muestra aleatoria sin reposición del mismo tamaño contuviera una pareja de hermanos es prácticamente 1; en concreto esta probabilidad sería

$$
\frac{100\times 98\times 96\times\cdots\times 2}{100\times 99\times 98\times\cdots\times 51}=\frac{2^{50}\cdot 50!^2}{100!}=`r 1-2^50/choose(100,50)`.
$$

   
   
#### Muestreo aleatorio estratificado {-}

Este tipo de muestreo se utiliza cuando la población está clasificada en  **estratos** que son de interés para la propiedad estudiada. En este caso, se toma una muestra aleatoria de cada estrato y se unen en una muestra global. A este proceso se le llama **muestreo aleatorio estratificado**. Normalmente, se impone que la composición por estratos de la muestra global mantenga las proporciones de la población original; es decir, que el tamaño de la muestra de cada estrato represente el mismo porcentaje del total de la muestra que el estrato correspondiente en la población completa.  Por ejemplo, los estratos podrían ser grupos de edad, y entonces la muestra de cada grupo de edad se tomaría proporcional a la fracción que representa dicho grupo de edad en la población total. O podrían ser los sexos anatómicos, y procuraríamos que nuestra muestra estuviera formada por un 50% de hombres y un 50% de mujeres. O, en las Islas Baleares, los estratos podrían ser las islas, de manera que el número de representantes de cada isla en la muestra fuera proporcional a su población relativa dentro del conjunto total de la comunidad autónoma. 

Por continuar con nuestra urna de 100 bolas, supongamos que contiene 40 bolas de un color y 60 de otro color según muestra la Figura \@ref(fig:estratprevi).


```{r, echo=FALSE, label=estratprevi,fig.cap="Nuestra urna ahora tiene 2 estratos"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/estratprevi.png")
```
 



Para tomar una muestra aleatoria estratificada de 15 bolas, considerando como estratos los dos colores, tomaríamos una muestra aleatoria de 6 bolas del primer color y una muestra aleatoria de 9 bolas del segundo color. De esta manera, los porcentajes de colores en la muestra serían los mismos que en la urna. La Figura \@ref(fig:estrat) describe una muestra obtenida de esta manera.


```{r, echo=FALSE, label=estrat,fig.cap="Una muestra aleatoria estratificada"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/estrat.png")
```
 

En todo caso, el muestreo por estratos solo es necesario si esperamos que las características de la propiedad poblacional que queremos estudiar varíen según el estrato. Por ejemplo, si queremos tomar una muestra para estimar la altura media de los españoles adultos y no creemos que la altura de un español adulto dependa de su provincia de origen, no hay ninguna necesidad de esforzarse en tomar una muestra de cada provincia de manera que todas las provincias estén representadas proporcionalmente en la muestra.


#### Muestreo por conglomerados {-}

El proceso de obtener y estudiar una muestra aleatoria en algunos casos es caro o difícil, incluso aunque dispongamos de la lista completa de la población. Imaginemos que quisiéramos estudiar los hábitos de alimentación de los estudiantes de Primaria de Baleares.  Para ello, previo permiso de la autoridad competente, tendríamos que seleccionar una muestra representativa de los escolares de Baleares. Seguramente podríamos disponer de su lista completa y por lo tanto podríamos tomar una muestra aleatoria, pero entonces acceder a las niñas y niños que la formasen seguramente significaría  contactar con unos pocos alumnos de muchos centros de primaria, lo que volvería el proceso lento y costoso. Y eso si la *Conselleria d'Educació* nos facilitase la lista completa de alumnos.

Una alternativa posible sería, en vez de extraer una muestra aleatoria de todos los estudiantes de Primaria, escoger primero al azar unas pocas aulas de primaria de colegios de las Baleares, a las que llamamos en este contexto **conglomerados** (*clusters*), y formar entonces nuestra muestra con todos los alumnos de estas aulas. Y es que es mucho más sencillo poseer la lista completa de estudiantes de unas pocas aulas que conseguir la lista completa de todos los estudiantes de todos los colegios, y mucho más barato ir a unos pocos colegios concretos que ir a todos los colegios de las Islas a entrevistar a unos pocos estudiantes en cada centro. 

Por poner otro ejemplo, efectuamos también un muestreo por conglomerados cuando para medir algunas características de los ejemplares de una planta en un bosque concreto, cuadriculamos la superficie del bosque, escogemos una muestra aleatoria de sectores de la cuadrícula (serían los conglomerados de este ejemplo) y estudiamos las plantas de interés contenidas en los sectores elegidas.

Volviendo de nuevo a nuestra urna, supongamos que sus 100 bolas se agrupan en 20 conglomerados de 5 bolas cada uno según las franjas verticales de la Figura \@ref(fig:clustprevi) (donde mantenemos la clasificación en dos colores para poder comparar el resultado del muestreo por conglomerados con el estratificado).


```{r, echo=FALSE, label=clustprevi,fig.cap="Nuestra urna ahora tiene 2 estratos y 20 conglomerados"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/clusterprevi.png")
```

Para obtener una muestra aleatoria por conglomerados de tamaño 15, escogeríamos al azar 3 conglomerados y la muestra estaría formada por sus bolas. 
 La Figura \@ref(fig:clust) describe una muestra obtenida de esta manera: los conglomerados escogidos están marcados en azul.


```{r, echo=FALSE, label=clust,fig.cap="Una muestra aleatoria por conglomerados"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/cluster.png")
```


Observad la diferencia entre el muestreo estratificado y el muestreo por conglomerados:

* En una muestra **estratificada** se escoge una muestra aleatoria de cada estrato existente.

* En una muestra **por conglomerados** se escogen algunos conglomerados al azar y se incluye en la muestra todos sus elementos.


#### Muestreos no aleatorios {-}

Cuando la selección de la muestra no es aleatoria, se habla de  **muestreo no aleatorio**. En realidad es el tipo más frecuente de muestreo porque casi siempre nos tenemos que conformar con los sujetos disponibles.  Por ejemplo, en la UIB, para estimar la opinión que de un profesor tienen los alumnos de una clase, se consulta solo a los estudiantes que voluntariamente  rellenan la encuesta de opinión,  que de ninguna manera forman una  muestra aleatoria: el perfil del estudiante que contesta voluntariamente una encuesta de este tipo está muy definido y no viene determinado por el azar. En este caso se trataría de una **muestra autoseleccionada**. 

Otro tipo de muestras no aleatorias son las **oportunistas**. Este es el caso, por ejemplo, si para estimar la opinión que de un profesor tienen los alumnos de una asignatura se visita un día la clase y se pasa la encuesta a los estudiantes que ese día asistieron a clase. De nuevo, puede que los alumnos presentes no sean representativos del alumnado de la asignatura (pueden ser los más aplicados, o los que no tienen la gripe, o a los que la asignatura no les coincide con otra). Veamos otros ejemplos de muestreo oportunista. Supongamos que queremos estudiar una característica de los animales de una determinada especie en un hábitat, y la medimos en los animales que capturamos. Estos ejemplares no tienen por qué ser representativos de la población: a lo mejor son los menos espabilados. O imaginad que tenéis una bolsa con bolas de diferentes tamaños. Si las removéis bien, las pequeñas tenderán a ir a parar al fondo y las grandes a quedar en la parte superior. Por lo tanto, si tomáis una muestra de la capa superior (que será lo más cómodo), no será representativa del total de la bolsa. 

La Figura \@ref(fig:oport) describe una muestra oportunista de nuestra urna: sus 15 primeras bolas. Aunque toda muestra de un mismo tamaño tiene la misma probabilidad de obtenerse por medio de un muestreo aleatorio sin reposición, es difícil de creer que esta muestra sea aleatoria; basta que calculéis cuál es la probabilidad de que en una muestra aleatoria de 15 bolas de nuestra urna todas tengan el mismo color:

```{r,include=FALSE}
options(scipen=999)
```


$$
\frac{40\times 39\times \cdots\times 26+60\times 59\times \cdots\times 46}{100\times 99\times \cdots\times 86}=`r round((prod(26:40)+prod(46:60))/prod(86:100),5)`
$$

```{r,include=FALSE}
options(digits=7,scipen=0)
```


```{r, echo=FALSE, label=oport,fig.cap="Una muestra oportunista"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/oport.png")
```


Las técnicas de estadística inferencial *no se pueden aplicar a muestras no aleatorias*, pero normalmente son las únicas que podemos conseguir. En este caso, lo que se suele hacer es describir en detalle las características de la muestra para justificar que, pese a no ser aleatoria, es representativa de la población y podría haber sido aleatoria. Por ejemplo, la muestra oportunista anterior de nuestra urna no es de ninguna manera representativa de su contenido por lo que refiere al color de las bolas.

#### Muestreo polietápico {-}

En el ejemplo de los estudiantes de Primaria, la muestra final de estudiantes ha estado formada por todos los individuos de las aulas elegidas. Otra opción podría haber sido, tras seleccionar la muestra aleatoria de conglomerados, tomar de alguna manera una muestra aleatoria de cada uno de ellos. Por ejemplo, algunos estudios poblacionales a nivel estatal se realizan solamente en algunas provincias escogidas aleatoriamente, en las que luego se encuesta una muestra aleatoria de habitantes. Este sería un ejemplo de **muestreo polietápico**, en el que la muestra no se obtiene en un solo paso, sino mediante diversas elecciones sucesivas. La Figura \@ref(fig:poli) muestra un ejemplo sencillo de muestreo polietápico de nuestra urna: hemos elegido al azar 5 conglomerados (marcados en azul) y de cada uno de ellos hemos elegido 3 bolas al azar sin reposición.


```{r, echo=FALSE, label=poli,fig.cap="Una muestra polietápica"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/poli.png")
```


Otro ejemplo enrevesado (pero real) de muestreo polietápico sería, para elegir una muestra de adolescentes de una ciudad grande, escoger en primer lugar 4 secciones censales al azar; a continuación, escoger al azar 10 manzanas de cada una de estas secciones censales y una esquina de cada manzana; finalmente, recorrer cada manzana en sentido horario a partir de la esquina seleccionada y visitar un portal de cada tres, entrevistando todos los habitantes de 13 a 19 años en las casas o fincas visitadas. En este proceso, hemos realizado tres muestreos aleatorios sin reposición (de secciones censales, de manzanas y de esquinas) y un muestreo sistemático (los portales). Si además los adolescentes que estudiamos al final no son todos los que viven en los portales seleccionados sino solo los que encontramos en casa el día que los visitamos, este muestreo oportunista significaría un cuarto paso en la formación de la muestra.


Existen otros tipos de muestreo, solo hemos explicado los más comunes. En cualquier caso, lo importante es recordar que el estudio estadístico que se realice *a posteriori* deberá ser diferente según el tipo de muestreo usado. Por ejemplo, no se pueden usar las mismas técnicas para analizar una muestra aleatoria simple que una muestra por conglomerados. 




## Muestreo aleatorio con R

En este curso estudiaremos las propiedades de las diferentes técnicas de estimación solamente para el caso de  **muestreo aleatorio simple**, es decir, al azar y con reposición, o al azar sin reposición si la población es muy, muy  grande en comparación con la muestra. Recordemos que un método de selección al azar de muestras de **tamaño** $n$ (es decir, formadas por $n$ individuos)  de una cierta población produce **muestras aleatorias simples** (**m.a.s.**) cuando todas las muestras posibles de $n$ individuos (con posibles repeticiones) tienen la misma probabilidad de ser elegidas. El tener una m.a.s. de una población junto con un tamaño muestral adecuado $n$ nos asegurará que la estimación que hagamos sea muy probablemente correcta.

La manera más  sencilla de llevar  a cabo un muestreo aleatorio simple es numerar todos los individuos de una población y sortearlos eligiendo números de uno en uno como si se tratase de una lotería, por ejemplo con  algún generador de números aleatorios. Esto se puede llevar a cabo fácilmente con R.

R dispone de un generador de muestras aleatorias de un vector. La función básica es

```{r,eval=FALSE}
sample(x, n, replace=...)
```

donde:

* `x` es un vector o un número natural $x$, en cuyo caso R entiende que representa el vector 1,2,...,$x$;

* `n` es el tamaño de la muestra que deseamos extraer; 

* el parámetro `replace` puede igualarse a `TRUE`, y será una muestra aleatoria con reposición, es decir, simple, o a `FALSE`, y será una muestra aleatoria sin reposición. Este último es su valor por defecto, por lo que no es necesario especificarlo si se quiere obtener una muestra sin reposición.


Los dos primeros parámetros han de entrarse en este orden o igualados a los parámetros `x` y `size`, respectivamente.

Así, por ejemplo, para obtener una m.a.s. de 15 números entre 1 y 100, podemos entrar:

```{r} 
sample(100,15,replace=TRUE)
```

Naturalmente, y como ya nos encontramos en la Lección \@ref(chap:distr) cuando generábamos vectores aleatorios con una distribución dada, cada ejecución de `sample` con los mismos parámetros puede dar lugar a muestras diferentes, y todas ellas tienen la misma probabilidad de aparecer:

```{r} 
sample(100,15,replace=TRUE)
sample(100,15,replace=TRUE)
sample(100,15,replace=TRUE)
```

Veamos cómo extraer una m.a.s de una tabla de datos. Recordemos el *data frame* `iris`, que recoge medidas de pétalos y sépalos de 150 flores de tres especies de iris.

```{r} 
str(iris)
```

Si queremos extraer una m.a.s. de 15 ejemplares (filas) de esta tabla de datos, podemos generar con `sample` una m.a.s. de índices de filas de la tabla (recordad que `dim` aplicado a un  `dataframe` nos da un vector con sus dimensiones, es decir, sus números de filas y de columnas, en este orden; por lo tanto, `dim(iris)[1]` es el número de filas de `iris`): 

```{r} 
x=sample(dim(iris)[1],15,replace=TRUE) 
```

y a continuación crear un *data frame* que contenga solo estas filas:

```{r} 
muestra_iris=iris[x,]
muestra_iris
```

Si solo quisiéramos una muestra aleatoria de longitudes de pétalos, podríamos aplicar directamente la función `sample` al vector correspondiente:

```{r} 
muestra_long_pet=sample(iris$Petal.Length,15,replace=TRUE)
muestra_long_pet
```

El hecho de que funciones como `sample` o los generadores de vectores aleatorios con una cierta distribución de probabilidad fijada, como `rnorm` o `rbinom`, produzcan... pues eso, vectores aleatorios, puede tener inconvenientes a la hora de reproducir una simulación. R permite "fijar" el resultado de una función aleatoria con la instrucción `set.seed`. Sin entrar en detalles sobre cómo funcionan, los diferentes algoritmos que usa R para generar números aleatorios usan una **semilla de aleatoriedad**, que se modifica después de la ejecución del algoritmo, y por eso cada vez dan un resultado distinto. Pero, para una semilla fija, el algoritmo da el mismo resultado siempre. Lo que hace la función  `set.seed` es igualar esta semilla al valor que le entramos. Si tras aplicar esta función a un número concreto ejecutamos una instrucción que genere un vector aleatorio de una longitud fija con una distribución fija, el resultado será siempre el mismo. Veamos un ejemplo de su efecto, generando muestras aleatorias simples de 10 longitudes de pétalos de flores iris con diferentes semillas de aleatoriedad:

```{r} 
sample(iris$Petal.Length,10,replace=TRUE)
set.seed(20)
sample(iris$Petal.Length,10,replace=TRUE)
set.seed(20)
sample(iris$Petal.Length,10,replace=TRUE)
sample(iris$Petal.Length,10,replace=TRUE)
set.seed(10)
sample(iris$Petal.Length,10,replace=TRUE)
set.seed(10)
sample(iris$Petal.Length,10,replace=TRUE)
```

Ejecutado inmediatamente después de  `set.seed(20)`, `sample(iris$Petal.Length,10,replace=TRUE)` siempre da lo mismo. Y ejecutado después de `set.seed(10)`, `sample(iris$Petal.Length,10,replace=TRUE)` vuelve a dar siempre lo mismo, pero diferente de con  `set.seed(20)`. 

La función `set.seed` no solo fija el resultado de la primera instrucción tras ella que genere un vector aleatorio, sino que, como fija la semilla de aleatoriedad y las funciones posteriores la modificarán  de manera determinista, también fija los resultados de todas las instrucciones siguientes que generen vectores aleatorios.

```{r} 
set.seed(100)
sample(10,3)
sample(10,3)
sample(10,3)
set.seed(100)
sample(10,3)
sample(10,3)
sample(10,3)
```

Si queréis volver a "reiniciar" la semilla de la aleatoriedad tras haber usado un `set.seed`, podéis usar `set.seed(NULL)`.

```{r} 
set.seed(100)
sample(10,3)
set.seed(NULL)
sample(10,3)
set.seed(100)
sample(10,3)
set.seed(NULL)
sample(10,3)
```


A veces querremos tomar diversas muestras aleatorias de una misma población y calcular algo sobre ellas. Para hacerlo podemos usar la función `replicate`. La sintaxis básica es

```{r,eval=FALSE}
replicate(n, instrucción)
```

donde `n` es el número de repeticiones de la `instrucción`. Por ejemplo, para tomar 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podemos hacer:

```{r} 
muestras=replicate(10, sample(iris$Petal.Length,15,replace=TRUE))
muestras
```
 
Observad que R  ha organizado los 10 vectores generados con el `replicate` como columnas de una matriz. 

Si solo nos hubiera interesado calcular las medias, redondeadas a 2 cifras decimales, de 10 muestras aleatorias simples de 15 longitudes de pétalos de flores iris, podríamos haber hecho

```{r} 
medias=replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2))
medias
```

En este caso, como el resultado de la instrucción que iteramos es un solo número, los resultados del `replicate` forman un vector.

¿Y si quisiéramos la media y la desviación típica muestral de 10 muestras de estas? No podemos usar sin más dos `replicate`, como en

```{r}
replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2))
replicate(10,round(sd(sample(iris$Petal.Length,15,replace=TRUE)),2))
```

porque es muy probable que el conjunto de muestras de las que hemos calculado la media en el primer `replicate` sea diferente del conjunto de muestras de las que hemos calculado la desviación típica en el segundo `replicate`. Lo más adecuado es definir una función que calcule un vector con estos dos valores, y luego usarla dentro de un único `replicate`. 

```{r} 
info=function(x){round(c(mean(x),sd(x)),2)}
info_lp=replicate(10,info(sample(iris$Petal.Length,15,replace=TRUE)))
info_lp
```

En este último caso, R ha organizado la información obtenida como columnas de una matriz: la primera fila son las medias y la segunda las desviaciones típicas.

Naturalmente, la función `set.seed` permite "fijar" el resultado de un `replicate` que incluya la generación de números aleatorios:

```{r} 
set.seed(1000)
replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2))
set.seed(1000)
replicate(10,round(mean(sample(iris$Petal.Length,15,replace=TRUE)),2))
```

Un último comentario sobre la función `sample`. Aunque aquí la vamos a usar principalmente para tomar muestras aleatorias  en las que todos los sujetos de la población tengan la misma probabilidad de ser escogidos, también podemos emplearla para obtener muestras en las que diferentes sujetos puedan tener probabilidades diferentes de salir. Estas probabilidades se especifican con el parámetro `prob` igualado a un vector de probabilidades (o de pesos proporcionales a probabilidades) de la misma longitud que el vector `x` al cual apliquemos `sample`. De esta manera, la primera entrada de `prob` representa la probabilidad del primer elemento de `x`, la segunda entrada de  `prob` representa la probabilidad del segundo elemento de `x`, etc. 

Por ejemplo, si queremos tomar una muestra aleatoria de tamaño 10 del vector $(1,2,3)$ de manera que cada elemento de este vector tenga probabilidad de ser escogido proporcional a su valor (es decir, el 2 tiene el doble de probabilidades de aparecer en la muestra que el 1, y el 3, el triple), podemos usar:

```{r, error=TRUE, warning=TRUE}
sample(1:3,10,prob=1:3)
```

¡Ups! Para tomar una muestra de 10 elementos de una población de 3 sujetos, habrá que permitir repeticiones

```{r, error=FALSE, warning=TRUE}
sample(1:3,10,replace=TRUE,prob=1:3)
```

Para terminar esta lección, damos una función sencilla para efectuar muestreos sistemáticos aleatorios. El objetivo es, dado un vector de longitud $N$, obtener una muestra de tamaño $n$. Lo que haremos será tomar el cociente por exceso $k=\lceil N/n\rceil$ de $N$ entre $n$ para determinar el período con el que tenemos que tomar los elementos de manera que todos los elementos puedan ser escogidos. A continuación elegimos al azar un elemento del vector con `sample` y a partir de él generamos una progresión aritmética de $n$ elementos y paso $k$, volviendo al inicio del vector si llegamos al final sin haber completado la muestra (lo que especificamos tomando los valores de la progresión aritmética módulo $N$). 

```{r}
sist.sample=function(N,n){
  k=ceiling(N/n)
  x0=sample(N,1)
  seq(x0,length.out=n,by=k)%%N
 }
```

Por ejemplo, una muestra sistemática de 10 flores iris se podría obtener de la manera siguiente:

```{r}
x=sist.sample(dim(iris)[1],10) #Los índices de la muestra sistemática
muestra_sist_iris=iris[x,] #La muestra de la tabla iris
muestra_sist_iris
```

Como 150/10=15, podemos observar que los índices avanzan de 15 en 15 a partir del que ha sido escogido al azar en primer lugar.


<!--
```{example, label=muestreo2}
Por si tenéis curiosidad, el código siguiente ha producido las diferentes muestras aleatorias de las figuras de la Sección \@ref(sec:muestreo):
  
```


* La muestra aleatoria simple de la Figura \@ref(fig:simple):

```{r}
set.seed(42)
sort(sample(100,15,rep=TRUE))  
```

* La muestra aleatoria sin reposición de la Figura \@ref(fig:sinrep):

```{r}
set.seed(42)
sort(sample(100,15,rep=FALSE))
```

* La muestra aleatoria sistemática de la Figura \@ref(fig:sist):

```{r}
set.seed(42)
X0=sample(100,1)
(X0+7*(0:14))%%100
```


* La muestra aleatoria estratificada de la Figura \@ref(fig:estrat):

```{r}
set.seed(42)
c(sort(sample(40,6,replace=FALSE)),sort(sample(41:100,9, replace =FALSE))) 
```

* La muestra aleatoria por conglomerados de la Figura \@ref(fig:clust):

```{r}
set.seed(42)
Y=sort(sample(20,3,rep=FALSE))  #Los conglomerados
sort(rep(Y,each=5)+20*(0:4))
```

|          Los conglomerados escogidos (ordenados de izquierda a derecha) han sido el `r Y[1]`, el `r Y[2]` y el `r Y[3]`.


* La muestra aleatoria polietápica de la Figura \@ref(fig:poli):

```{r}
set.seed(42)
Y=sample(20,5,replace=FALSE)
sort(sapply(Y,FUN=function(x){sample(x+20*(0:4),3,replace=FALSE)}))
```

|          Los conglomerados escogidos han sido el `r sort(Y)[1]`, el `r sort(Y)[2]`, el `r sort(Y)[3]`, el `r sort(Y)[4]` y el `r sort(Y)[5]`.
-->

## Guía rápida

* `sample(x, n)` genera una muestra aleatoria de tamaño `n` del vector `x`. Si `x` es un número natural $x$, representa el vector 1,2,...,$x$. Dispone de los dos parámetros siguientes:

    * `replace`, que igualado a `TRUE` produce muestras con reposición e igualado a `FALSE` (su valor por defecto) produce muestras sin reposición.
    * `prob`, que permite especificar las probabilidades de aparición de los diferentes elementos de `x` (por defecto, son todas la misma).

* `set.seed` permite fijar la semilla de aleatoriedad. 

* `replicate(n,expresión)` evalúa `n` veces la `expresión`, y organiza los resultados como las columnas de una matriz (o un vector, si el resultado de cada `expresión` es unidimensional).

## Ejercicios


### Test {-}


*(1)* Queremos escoger 100 estudiantes de grado de la UIB para preguntarles cuántas horas semanales estudian. Como creemos que el tipo de estudio cursado influye en este dato, clasificamos los estudiantes según el centro (facultad o escuela) en el que están matriculados, y tomaremos una muestra al azar de cada centro, por sorteo a partir de la lista de todos los matriculados en ese centro y de manera que el tamaño de la muestra de cada centro sea proporcional al número de matriculados en el mismo. ¿De qué tipo de muestreo se tratará?

(a) Muestreo aleatorio simple
(b) Muestreo aleatorio estratificado
(c) Muestreo aleatorio sin reposición
(d) Muestreo aleatorio por conglomerados
(e) Muestreo aleatorio sistemático
(f) Ninguno de los anteriores

*(2)* Con una sola  instrucción, calculad la media de una muestra aleatoria sin reposición de 15 elementos escogidos de un vector numérico llamado $X$. 

*(3)* Con una sola instrucción, extraed un *subdataframe* del *dataframe* `iris` formado por una muestra aleatoria sin reposición de 40 filas, y llamadlo `muestra`. Y antes de contestar, comprobad que funciona.

*(4)* Con una sola instrucción, calculad un vector formado por las medias de 100 muestras aleatorias sin reposición de 20 elementos cada una escogidos de un vector numérico llamado $X$ y llamadlo `medias`.

<!--
### Problemas {-}

**(1)** Considerad la tabla de datos **datacrab.txt** que encontraréis en https://raw.githubusercontent.com/AprendeR-UIB/Material/master/datacrab.txt y que contiene información sobre una muestra de  cangrejos. Cargadla en un `dataframe`.

*(a)* Definid una función de parámetros *N*, *n* y *s* que tome $N$ muestras aleatorias simples de $n$ filas de este `dataframe` usando como semilla de aleatoriedad el número $s$; a continuación, calcule las medias de los pesos de los individuos de cada una de estas muestras; y finalmente calcule la media y la desviación típica muestral del vector formado por estas  medias.  Tenéis que usar `set.seed` y `replicate` para definir la función.

*(b)* Aplicadla a $N$=100, $n$=30 y tomando como $s$ el número formado por las 5 primeras cifras de vuestro NIF o pasaporte.

*(c)* ¿Qué valores predice el Teorema Central del Límite que se deberían obtener? ¿Habéis obtenido resultados similares a los predichos por dicho teorema?

**(2)** Extraed una muestra aleatoria estratificada de 15 flores de la tabla `iris` manteniendo en la muestra las proporciones de las diferentes especies en la tabla. Procurad que el método empleado sea lo más general posible, para poderlo aplicar a cualquier `data frame`, cualquier tamaño de muestra (aproximadamente) y una clasificación definida por cualquier factor del `data frame`. 
-->


### Respuestas al test {-}


*(1)* b

*(2)* `mean(sample(X,15))`

(También sería correcto `sum(sample(X,15))/15`. Y en ambos casos también sería correcto añadiendo dentro de la función `sample` el parámetro  `replace=FALSE`, que hemos omitido porque es el valor por defecto de `replace`.)

*(3)* `muestra=iris[sample(dim(iris)[1],40),]`

(También sería correcto consultar antes el número de filas con `str` o `tail`, ver que son 150, y responder `muestra=iris[sample(150,40),]`. Hay otras respuestas correctas, no las damos para no liaros. Además, y como antes, también sería correcto añadir `replace=FALSE`.)

*(4)* `medias=replicate(100,mean(sample(X,20)))`

(¿Ya os hemos dicho que también sería correcto con `replace=FALSE`?)

<!--
### Soluciones sucintas de los problemas {-}

**(1)** Cargamos la tabla en el `dataframe` **DCR**

```{r}
library(RCurl)
datos=getURL("https://raw.githubusercontent.com/AprendeR-UIB/Material/master/datacrab.txt")
DCR=read.table(text=datos,header=TRUE)
str(DCR)
```


*(a)* Una posible función:

```{r}
M=function(N,n,s){
set.seed(s)
Muestras=replicate(N,mean(sample(DCR$weight,n,replace=TRUE)))
Res=round(c(mean(Muestras),sd(Muestras)),2)
attr(Res,"names")=c("Media","Desv. Típica")
Res
}
```

*(b)*
```{r}
M(100,30,42)
```

*(c)* El TCL predice como media la de la población (que en este caso es el vector `DCR$weight`)

```{r} 
round(mean(DCR$weight),2)
``` 

y como desviación típica la desviación típica de la población dividida por la raíz cuadrada del tamaño de las muestras

```{r} 
round((sd(DCR$weight)*sqrt(length(DCR$weight)-1)/sqrt(length(DCR$weight)))/sqrt(30),2)
``` 

**(2)** Por ejemplo

```{r}
Clust_sample=function(DF,Fact,n){
  attach(DF)
  Tamaños=round(prop.table(table(Fact))*n)
  Muestra=c()
  for(i in 1:length(levels(Fact))){
     Muestra=c(Muestra,sample(which(Fact==levels(Fact)[i]),Tamaños[i]))
  }
  detach(DF)
  # Por si la muestra ha quedado "corta" debido a redondeos
  # al calcular los tamaños de las submuestras:
  Muestra=c(Muestra,sample((1:dim(DF)[1])[-Muestra],n-length(Muestra)))
  sort(Muestra)  
}
Clust_sample(iris,Species,15)
```

Una construcción equivalente, sin usar un bucle **for**

```{r}
Clust_sample2=function(DF,Fact,n){
  attach(DF)
  M=length(levels(Fact))
  Tamaños=round(prop.table(table(Fact))*n)
  F=function(i){sample(which(Fact==levels(Fact)[i]),Tamaños[i])}
  Muestra=unlist(sapply(1:M,FUN=F))
  detach(DF)
  Muestra=c(Muestra,sample((1:dim(DF)[1])[-Muestra],n-length(Muestra)))
  sort(Muestra) 
}
Clust_sample2(iris,Species,15)
```

Por ejemplo, podríamos usar esta función para extraer una muestra aleatoria estratificada de 15 cangrejos de la tabla **datacrab.txt** usada en el problema anterior usando como estratos los valores de la variable `color.spine`.

```{r,warning=FALSE}
DCR$color.spine=as.factor(DCR$color.spine)
Clust_sample(DCR,color.spine,15)
```
-->

<!--chapter:end:02-Muestreo.Rmd-->

# Estimación puntual {#chap:estimacion}

En un estudio inferencial, una vez tomada la muestra y obtenidos los datos sobre sus miembros, el siguiente paso es inferir, es decir, deducir información sobre la población a partir de estos datos. Dicha información se puede deducir de dos formas:

* Suponiendo que conocemos el **modelo** al que se ajusta la población: es decir, suponiendo que conocemos el tipo de distribución de la variable aleatoria que modela la característica de la población en la que estamos interesados, pero desconocemos uno o varios parámetros de los que depende dicha distribución (observad que si lo sabemos todo sobre esta distribución, ya no hace falta tomar muestras para inferir algo sobre ella). Así, podemos saber (o suponer) que las longitudes de los ejemplares adultos de una cierta especie se distribuyen según una variable aleatoria normal, pero desconocer sus parámetros $\mu$ (media) y $\sigma$ (desviación típica), y usar este conocimiento para inferir información sobre dichas longitudes a partir de las de una muestra: por ejemplo, para estimar con un cierto margen de error su longitud media. Si estamos en este caso, hablaremos de **estimación paramétrica**.

* Suponiendo que desconocemos qué tipo de distribución tiene la variable aleatoria que modela la característica que nos interesa (aunque a veces necesitaremos saber algo de esta distribución; por ejemplo, si es simétrica o no). En este caso, hablaremos de **estimación no paramétrica**.

En ambos casos, existen tres vías para obtener información sobre los parámetros de la distribución (conocida o desconocida) de la variable aleatoria que nos interesa:

* **Estimación puntual**. Se trata de obtener expresiones matemáticas, llamadas **estimadores puntuales**, que aplicadas a los valores de una muestra nos dan una aproximación (el término exacto es una **estimación**) del valor de dicho parámetro para la población. A modo de ejemplo, la media aritmética de los datos $x_1,\ldots,x_n$ de una muestra aleatoria,
$$
\overline{x}=\frac{x_1+\cdots +x_n}{n},
$$
es un estimador del **valor medio** (**valor esperado**, **esperanza**) de la variable aleatoria de la que hemos extraído la muestra.

* **Estimación por intervalos de confianza**. Se trata de obtener intervalos que contengan con probabilidad alta el parámetro objeto de estudio. Trataremos este tema en la Lección \@ref(chap:IC). 

* **Contraste de hipótesis**. *Grosso modo*, se establecen dos hipótesis opuestas sobre el parámetro o, más en general, sobre la distribución de la variable aleatoria, y se contrastan para intentar decidir cuál es la verdadera. Los estudiaremos en próximas lecciones. 


En esta lección hablaremos de la estimación puntual. Para empezar, es obvio que no toda fórmula matemática sirve para estimar de manera sensata el valor de un parámetro. Por ejemplo, si queréis estimar la media de las alturas de los habitantes de una población y disponéis de una muestra aleatoria de las mismas, no tomáis la raíz cuadrada de la altura máxima en la muestra como  estimación de la altura media de la población, ¿verdad? Lo que habéis hecho toda la vida, y seguiréis haciendo en este curso, ha sido calcular la media de las alturas en la muestra y dar ese valor como estimación de la altura media poblacional. Y es lo correcto, porque la media muestral es siempre un estimador **insesgado** de la media poblacional y muy a menudo es además su estimador **máximo verosímil**, Veamos qué significan estas propiedades.

* **Insesgado**: Los valores de un estimador sobre muestras aleatorias de una población forman una variable aleatoria con una distribución de probabilidad propia, llamada genéricamente  **muestral**. Decimos entonces que un estimador es **insesgado** cuando su valor esperado coincide con el valor del parámetro poblacional que se quiere estimar. Por ejemplo, si se toman muestras aleatorias con o sin reposición, la media muestral es siempre un estimador insesgado del valor medio poblacional: su valor esperado es el valor medio poblacional.

* **Máximo verosímil**: Cada muestra aleatoria de una población tiene una probabilidad de obtenerse que no solo depende de la muestra, sino también de la distribución de probabilidad de la variable aleatoria poblacional. Si la distribución poblacional es de un tipo concreto (Bernoulli, normal, ...), esta probabilidad depende de sus parámetros. Decimos entonces que un estimador es **máximo verosímil** cuando el resultado que da sobre cada muestra aleatoria es el valor del parámetro poblacional que maximiza la probabilidad de obtenerla. Por ejemplo, si lanzamos una moneda al aire $n$ veces y calculamos la proporción de veces que obtenemos cara, esa **proporción muestral** $\widehat{p}$ es el estimador máximo verosímil de la probabilidad $p$ de obtener cara con esa moneda. Esto quiere decir que, de entre todas las distribuciones binomiales $B(n,p)$ que pueden modelar el número de caras que obtenemos al lanzar $n$ veces nuestra moneda, aquella que asigna mayor probabilidad al número de caras que hemos obtenido es la que tiene como parámetro $p$ la frecuencia relativa de caras $\widehat{p}$ que hemos observado.

Para algunas distribuciones, el método de estimación por máxima verosimilitud de sus parámetros da lugar a fórmulas cerradas más o menos sencillas, pero en otros casos nos tenemos que conformar con un valor aproximado obtenido mediante algún método numérico.


## Estimación máximo verosímil


A continuación recordamos una lista de los estimadores máximo verosímiles de los parámetros de las distribuciones más comunes a partir de una muestra aleatoria simple:

* Para la familia Bernoulli, el estimador máximo verosímil del parámetro $p$ es la proporción muestral de éxitos $\widehat{p}$. Este estimador es además insesgado.

*  Para la familia Poisson, el estimador máximo verosímil  del parámetro $\lambda$ es la media muestral $\overline{X}$. Este estimador es de nuevo  insesgado.

*  Para la familia geométrica, el estimador máximo verosímil del parámetro $p$ es ${1}/{\overline{X}}$. Este estimador es sesgado.

*   Para la familia exponencial, el estimador máximo verosímil del parámetro $\lambda$ también es ${1}/{\overline{X}}$. Este estimador también es sesgado.

*  Para la familia normal, los  estimadores máximo verosímiles de la media $\mu$, la desviación típica $\sigma$ y la varianza $\sigma^2$ son, respectivamente, la media muestral $\overline{X}$, la desviación típica "verdadera" $S_X$ y la varianza "verdadera" $S_X^2$. Además, $\overline{X}$ es un estimador insesgado de $\mu$. La varianza verdadera $S_X^2$ no es un estimador insesgado de  $\sigma^2$, pero sí que lo es la varianza muestral $\widetilde{S}^2$. Y ninguna de las dos desviaciones típicas, ni la  "verdadera" $S_X$ ni la muestral $\widetilde{S}_X$, es un estimador  insesgado de $\sigma$; si necesitáis un estimador insesgado de la desviación típica de una variable aleatoria normal a partir de una muestra aleatoria simple, lo podéis encontrar en la [correspondiente entrada de la Wikipedia](http://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation). No obstante, el beneficio de usar este estimador insesgado no suele compensar lo complicado de su cálculo.


Cuando se estima algún parámetro de una distribución a partir de una muestra, es conveniente aportar el **error típico**, o **estándar**,  como medida de la finura de la estimación. Recordemos que el **error típico de un estimador** es la desviación típica de su distribución muestral, y que el **error típico de una estimación** a partir de una muestra es la estimación del error típico del estimador usando dicha muestra. 

Veamos un ejemplo sencillo.  Supongamos que tenemos una muestra aleatoria simple de tamaño $n$ de una variable $X$ que sigue una distribución Bernoulli de probabilidad poblacional $p$ desconocida que queremos estimar. Por ejemplo, puede ser que tengamos una moneda posiblemente trucada, la hayamos lanzado 100 veces al aire y hayamos anotado los resultados (1, cara, 0, cruz), y a partir de este experimento queramos estimar la probabilidad de sacar cara con esta moneda. O que hayamos anotado para 100 individuos de una población  elegidos al azar si tienen o no una determinada enfermedad (1 significa que sí, 0 que no) y a partir de esta muestra deseemos estimar la **prevalencia** de la enfermedad en la población, es decir, la proporción real de enfermos, que coincide con la probabilidad de que un individuo elegido al azar tenga dicha enfermedad. Tomemos, para fijar ideas, la siguiente muestra de tamaño 100:

```{r}
x=c(0,1,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,0,0,0,0,1,0,0,1,0,0,0,1,0,1,0,0,0,
0,0,0,0,1,0,0,0,0,0,0,0,0,0,1,1,0,1,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,
1,1,0,0,0,0,1,0,0,0,0,0,0,0,1,0)
```

En este caso, podemos estimar $p$ mediante la proporción muestral de éxitos $\widehat{p}$, que coincide con la media muestral. El error típico de este estimador es $\sqrt{p(1-p)/n}$, y el error típico de una estimación concreta es $\sqrt{\widehat{p}(1-\widehat{p})/n}$. Por lo tanto, a mano podemos estimar $p$ y calcular el error típico de dicha estimación de la manera siguiente:

```{r}
n=length(x)  #Tamaño de la muestra
estim.p=mean(x)  #Proporción muestral
estim.p
error.tip.p=sqrt(estim.p*(1-estim.p)/n) #Error típico de la estimación
error.tip.p
```

De esta manera, estimamos que $p$=`r estim.p`  con un error típico de `r round(error.tip.p,2)`.




Con R podemos estimar un parámetro de una distribución por el método de máxima verosimilitud a partir de una muestra y además obtener el error típico de dicha estimación usando la función `fitdistr` del paquete **MASS**.  Esta función calcula los estimadores máximo verosímiles de los parámetros de la mayoría de las familias de distribuciones disponibles en R. Su  sintaxis básica es

```{r, eval=FALSE}
fitdistr(x, densfun=..., start=...)
```

donde

*  `x` es la muestra, un vector numérico.

*  El valor de `densfun` ha de ser el nombre de la familia de distribuciones; se tiene que entrar  entre comillas y puede tomar, entre otros, los valores siguientes: `"chi-squared"`, `"exponential"`, `"f"`, `"geometric"`,  `"lognormal"`,  `"normal"` y `"poisson"`.  La lista de distribuciones a las que se puede aplicar, que podéis consultar en la Ayuda de la función, no incluye la Bernoulli ni la binomial.

*  Si `fitdistr` no dispone de una fórmula cerrada para el estimador  máximo verosímil de algún parámetro, usa un algoritmo numérico para aproximarlo que requiere de un valor inicial para arrancar. Este valor (o valores) se puede especificar igualando el parámetro `start` a una `list` con cada parámetro a estimar igualado a un valor inicial.  Para algunas distribuciones, como la `"t"`,  `fitdistr` sabe tomar valores iniciales razonables, y no es necesario especificar el parámetro `start`. Pero para otras distribuciones, como por ejemplo la `"chi-squared"`, es obligatorio especificarlo. Para las distribuciones que disponen de fórmula cerrada, como la `"normal"` o la  `"poisson"`,  se tiene que omitir el parámetro `start`. 

Como no podemos usar `fitdistr` para estimar el parámetro $p$ de una Bernoulli (los autores del paquete debieron de considerar que era más fácil estimarlo directamente), vamos a usarla en otro ejemplo. Consideremos la siguiente muestra *y* de 100 valores generados con distribución de Poisson de parámetro $\lambda=10$:

```{r}
set.seed(100) 
y=rpois(100,10)
set.seed(NULL) 
y
```

Vamos a estimar el parámetro $\lambda$ de una distribución Poisson que haya generado este vector:

```{r}
library(MASS)
fitdistr(y, densfun="poisson")
```

El resultado dice que el valor estimado de  $\lambda$ es `r round(fitdistr(y, densfun="poisson")$estimate, 2)`, con un error típico estimado de   `r round(fitdistr(y, densfun="poisson")$sd, 2)`. Veámoslo directamente:  el estimador máximo verosímil de $\lambda$ es la media aritmética $\overline{X}$ y el error típico de este estimador es $\sqrt{\lambda}/\sqrt{n}$ (recordad que la desviación típica de una Poisson de parámetro $\lambda$ es $\sqrt{\lambda}$ y que el error típico de la media muestral es la desviación típica poblacional dividida por la raíz cuadrada del tamaño de la muestra), por lo que el error típico de una estimación es  $\sqrt{\overline{X}}/\sqrt{n}$.

```{r}
mean(y)
sqrt(mean(y)/length(y))
```

También podemos  estimar la media y la desviación típica de una variable normal que hubiera producido esta muestra.

```{r}
fitdistr(y, densfun="normal")
```

Observad que la estimación de la desviación típica que nos da `fitdistr` es la desviación típica "verdadera" (que es su estimador máximo verosímil) y no la muestral:

```{r}
sd(y)
sqrt((length(y)-1)/length(y))*sd(y)
```

Vamos a estimar ahora el número de grados de libertad de una t de Student que hubiera producido esta muestra.

```{r}
fitdistr(y, densfun="t")
```

¡Vaya!, aparte del número de grados de libertad, `df`, han aparecido parámetros que no esperábamos. Los parámetros `m` y `s` son los parámetros  de posición, $\mu$, y de escala, $\sigma$, respectivamente, que definen una familia más general de distribuciones  t de Student (si os interesa, consultad esta [entrada de la Wikipedia](http://en.wikipedia.org/wiki/Noncentral_t-distribution)). Las que usamos en este curso tienen $\mu=0$ y $\sigma=1$.  ¿Cómo podríamos estimar los grados de libertad de una t de Student de las nuestras? Especificando dentro de `fitdistr` los valores de los parámetros que queremos que tomen un valor concreto:  en este caso, añadiendo `m=0` y `s=1`.

```{r ,error=TRUE}
fitdistr(y, densfun="t", m=0, s=1)
```


Ahora R nos pide que demos un valor inicial al número de grados de libertad,  `df`, para poder arrancar el algoritmo numérico que usará.
Vamos a inicializarlo a 1, y de paso veremos cómo se usa este parámetro:

```{r,warning=TRUE,width=10}
fitdistr(y, densfun="t", m=0, s=1, start=list(df=1))
```


Obtenemos un número estimado de  grados de libertad de la t de Student de aproximadamente `r round(fitdistr(y, densfun="t",m=0,s=1,start=list(df=1))$estimate,2)` grados de libertad (sí, los grados de libertad de una t de Student pueden ser un número real positivo cualquiera).
Por otro lado, R nos avisa de que el resultado es poco de fiar, pero tampoco nos importa mucho, porque el objetivo era mostrar un ejemplo de cómo fijar valores de parámetros, igualándolos a dichos valores, y cómo especificar el parámetro `start`, como una `list` donde asignamos a cada parámetro un valor inicial.

El resultado de  `fitdistr` es una `list`, y por lo tanto el
 valor de cada estimador y su error típico se pueden obtener con los sufijos adecuados. En concreto, los valores estimados forman la componente `estimate` y los errores típicos la componente `sd`. Para obtenerlos directamente, basta usar los sufijos `$estimate` y `$sd`, respectivamente:
 
```{r}
fitdistr(y,"poisson")$estimate  #Estimación de lambda
fitdistr(y,"poisson")$sd   #Error típico
fitdistr(y,"normal")$estimate  #Estimaciones 
fitdistr(y,"normal")$estimate[1]  #Estimación de mu
fitdistr(y,"normal")$estimate[2]  #Estimación de sigma
```


## Guía rápida

*  `fitdistr` del paquete **MASS**, sirve para calcular los estimadores  máximo verosímiles  de los parámetros de una distribución a partir de una muestra. El resultado es una `list` que incluye los objetos `estimate` (los valores estimados) y `sd` (los errores típicos de las estimaciones). Sus parámetros principales son:

     * `densfun`: el nombre de  la familia de distribuciones, entre comillas.
     * `start`: permite fijar el valor inicial del algoritmo numérico para calcular el estimador, si la función lo requiere.



## Ejercicios


### Test {-}

*(1)* Las distribuciones de Weibull tienen dos parámetros: forma, `shape`, y escala, `scale`.  Supongamos que los datos siguientes siguen una distribución de Weibull: 2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72. Calculad el estimador máximo verosímil del parámetro de escala de esta distribución, redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado.

*(2)* Generad, con semilla de aleatoriedad igual a 42,  una secuencia aleatoria de 100 valores con distribución geométrica Ge(0.6). A continuación estimad por máxima verosimilitud el parámetro $p$ de una distribución geométrica que haya generado dicha muestra y dad como respuesta a esta pregunta *el error típico de esta estimación* redondeado a 3 cifras decimales. Tenéis que dar el resultado (sin ceros innecesarios a la derecha), no cómo lo habéis calculado.




### Respuestas al test {-}


*(1)* `r round(fitdistr(c(2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72),"weibull")$estimate[2],3)`

Nosotros lo hemos calculado con
```{r,eval=FALSE}
x=c(2.46, 2.28, 1.7, 0.62, 0.87, 2.81, 2.35, 2.08, 2.11, 1.72)
round(fitdistr(x,"weibull")$estimate,3)
```

*(2)* `r set.seed(42); round(fitdistr(rgeom(100,0.6),"geometric")$sd,3)`

Nosotros lo hemos calculado con
```{r,eval=FALSE}
set.seed(42)
x=rgeom(100,0.6)
round(fitdistr(x,"geometric")$sd,3)
```

```{r, include=FALSE}
set.seed(NULL)
```


<!--chapter:end:03-Estimacion_Puntual.Rmd-->

# Intervalos de confianza {#chap:IC}

En esta lección explicamos cómo calcular con R algunos intervalos de confianza básicos. Recordad que un **intervalo de confianza del  $q\times 100\%$** (con $q$ entre 0 y 1) para un parámetro poblacional (la media, la desviación típica, la probabilidad de éxito de una variable Bernoulli, ...) es un intervalo obtenido aplicando a una muestra aleatoria simple una fórmula que garantiza (si se cumplen una serie de condiciones sobre la distribución de la variable aleatoria poblacional que en cada caso dependen del parámetro y de la fórmula) que el  $q\times 100\%$ de las veces que la aplicáramos a una muestra aleatoria simple de la misma población, el intervalo resultante contendría el parámetro poblacional que queremos estimar. Esto es lo que significa lo de "confianza del  $q\times 100\%$": que confiamos en que nuestra muestra pertenece al $q\times 100\%$ de las muestras (aleatorias simples) en las que la fórmula acierta y da un intervalo que contiene el parámetro deseado.

Algunas de las funciones que aparecen en esta lección volverán a salir en la próxima, ya que aunque calculan intervalos de confianza, su función principal es en realidad efectuar contrastes de hipótesis.


## Intervalo de confianza para la media basado en la t de Student {#sec:ICT}

Supongamos  que queremos estimar a partir de una m.a.s. la media $\mu$ de una población que sigue una distribución normal o tomando la muestra grande (por fijar una cota, de tamaño 40 o mayor). En esta situación, si $\overline{X}$, $\widetilde{S}_{X}$ y $n$ son, respectivamente, la media muestral, la desviación típica muestral y el tamaño de la muestra, un intervalo de confianza del $q\times 100\%$ para $\mu$ es

\begin{equation}
\overline{X}\pm t_{n-1,(1+q)/2} \cdot \frac{\widetilde{S}_{X}}{\sqrt{n}}
(\#eq:form1)
\end{equation}

donde $t_{n-1,(1+q)/2}$ es el cuantil de orden $(1+q)/2$ de una variable aleatoria con distribución t de Student con $n-1$ grados de libertad. Fijaos en que $\widetilde{S}_{X}/\sqrt{n}$ es el error típico de la estimación de la media.

A la hora de calcular este intervalo de confianza, tenemos dos posibles situaciones. Una, típica de ejercicios, es cuando  de la muestra sólo conocemos su media muestral $\overline{X}$, su desviación típica muestral $\widetilde{S}_X$ y su tamaño $n$. Si los denotamos por  `x.b`, `sdm` y `n`, respectivamente, y  denotamos el nivel de confianza en tanto por uno $q$  por  `q`, podemos calcular los  extremos de este intervalo de confianza con la expresión siguiente:

```{r,eval=FALSE}
x.b+(qt((1+q)/2,n-1)*sdm/sqrt(n))*c(-1,1)
```

Ahora bien,  "en la vida real" lo usual es disponer de un vector  numérico `X` con los valores de la muestra. En este caso, podemos usar la función `t.test` de R, que, entre otra información, calcula estos intervalos de confianza para $\mu$. Si solo nos interesa el intervalo de confianza, podemos usar la sintaxis siguiente:

```{r,eval=FALSE}
t.test(X,conf.level=...)$conf.int
```

donde tenemos que igualar el parámetro  `conf.level` al nivel de confianza $q$ en tanto por uno. Si $q=0.95$, no hace falta entrarlo, porque  es su valor por defecto. 



```{example, label="lux"}
Tenemos una muestra de pesos en gramos de 28 recién nacidos con luxación severa de cadera:
  
```

```{r}
pesos=c(2466,3941,2807,3118,3175,3515,3317,3742,3062,3033,2353,3515,3260,2892,
4423,3572,2750,3459,3374,3062,3205,2608,3118,2637,3438,2722,2863,3513)
```

Vamos a suponer que nuestra muestra es aleatoria simple  y que los pesos al nacer de los bebés con esta patología siguen una distribución normal.  A partir de esta muestra, queremos calcular un intervalo de confianza del 95% para el peso medio de un recién nacido con luxación severa de cadera, y ver si contiene el peso medio de la población global de recién nacidos, que es de unos 3400 g.

Como suponemos que la variable aleatoria poblacional es normal, para calcular un intervalo de confianza del 95% para su valor medio vamos a usar la fórmula basada en la distribución t de Student, y por lo tanto la función `t.test`:

```{r}
t.test(pesos)$conf.int
```

El intervalo que obtenemos es  [`r round(t.test(pesos)$conf.int,1)`] y está completamente a la izquierda del peso medio global de 3400 g, por lo que tenemos evidencia (a un 95% de confianza) de que los niños con luxación severa de cadera pesan de media al nacer por debajo de la media global. La apostilla entre paréntesis "a un 95% de confianza" aquí significa que hemos basado esta conclusión en un intervalo obtenido con una fórmula que acierta con una probabilidad del 95%, en el sentido de que el 95% de las ocasiones que aplicamos esta fórmula a una m.a.s. de una variable aleatoria normal, produce un intervalo que contiene la media de esta variable.  

Observad que el resultado de `t.test(pesos)$conf.int` tiene un atributo, `conf.level`, que indica su nivel de confianza. En principio este atributo no molesta para nada en cálculos posteriores con los extremos de este intervalo de confianza, pero si os molesta, lo podéis quitar igualándolo a `NULL`.

```{r}
IC.lux=t.test(pesos)$conf.int
attr(IC.lux,"conf.level")=NULL
IC.lux
```

Veamos cómo podríamos haber obtenido este intervalo directamente con la fórmula \@ref(eq:form1):

```{r}
x=mean(pesos)
sdm=sd(pesos)
n=length(pesos)
q=0.95
x+(qt((1+q)/2,n-1)*sdm/sqrt(n))*c(-1,1)
```

Como podéis ver, coincide con el intervalo obtenido con la función `t.test`.

```{example, label="experiment"}
Vamos a comprobar con un experimento esto de la "confianza" de los intervalos de confianza, y en concreto de la fórmula  \@ref(eq:form1).
Vamos a generar al azar una  *Población* de 10,000,000 "individuos" con distribución normal estándard. Vamos a tomar 200 muestras aleatorias simples de tamaño 50 de esta población y calcularemos el intervalo de confianza para la media poblacional usando dicha fórmula. Finalmente, contaremos cuántos de estos intervalos de confianza contienen la media de la *Población*. Fijaremos la semilla de aleatoriedad para que el experimento sea reproducible y podáis comprobar que no hacemos trampa. En otras simulaciones habríamos obtenido resultados mejores o peores, es lo que tienen las simulaciones aleatorias. 

```


```{r}
set.seed(42)
Poblacion=rnorm(10^7) #La población
mu=mean(Poblacion)  # La media poblacional
M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras
dim(M)
```

Tenemos una matriz M de `r dim(M)[2]` columnas y `r dim(M)[1]` filas, donde cada columna es una m.a.s. de nuestra población. Vamos a aplicar a cada una de estas muestras la función `t.test` para calcular un intervalo de confianza del 95% y luego  contaremos los aciertos, es decir, cuántos de ellos contienen la media poblacional

```{r}
IC.t=function(X){t.test(X)$conf.int}
ICs=apply(M,FUN=IC.t,MARGIN=2)
Aciertos=length(which((mu>=ICs[1,]) & (mu<=ICs[2,])))
Aciertos
```

Hemos acertado `r Aciertos` veces, es decir, un `r round(100*Aciertos/200,1)`% de los intervalos obtenidos contienen la media poblacional. No hemos quedado muy lejos del 95% esperado. 

Para visualizar mejor los aciertos, vamos a dibujar los intervalos apilados en un gráfico, donde aparecerán en azul claro los que aciertan y en rojo los que no aciertan.

```{r,results="hide", fig.cap="Aciertos y errores en 200 Intervalos de confianza al 95%"}
plot(1,type="n",xlim=c(-0.8,0.8),ylim=c(0,200),xlab="Valores",ylab="Repeticiones",main="")
seg.int=function(i){
  color="light blue";
  if((mu<ICs[1,i]) | (mu>ICs[2,i])){color = "red"}
  segments(ICs[1,i],i,ICs[2,i],i,col=color,lwd=2)
  }
sapply(1:200,FUN=seg.int)
abline(v=mu,lwd=2)
```

Fijaos en que los errores no se distribuyen por igual a los dos lados, hay muchos más intervalos que dejan la media poblacional a su izquierda que a su derecha, mientras que, en teoría, tendríamos que esperar que en la mitad de los errores la media poblacional estuviera a la izquierda del intervalo calculado y en la otra mitad a la derecha. Cosas de la aleatoriedad. 



## Intervalos de confianza para la proporción poblacional

En esta sección consideramos el caso en que la población objeto de estudio sigue una distribución Bernoulli y  queremos estimar su probabilidad de éxito (o **proporción poblacional**) $p$. Para ello, tomamos una muestra aleatoria simple de tamaño $n$ y número de éxitos $x$, y, por lo tanto, de **proporción muestral** de éxitos $\widehat{p}_X=x/n$.

El **método "exacto" de Clopper-Pearson** para calcular un intervalo de confianza del $q\times  100\%$ para $p$ se basa en el hecho de que, en estas condiciones, el valor de $x$ sigue una distribución binomial $B(n,p)$. Este método se puede usar siempre, sin ninguna restricción sobre la muestra, y  consiste básicamente en encontrar los valores $p_0$ y $p_1$ tales que

$$
\sum_{k=x}^n\binom{n}{k}p_0^k(1-p_0)^{n-k}=(1-q)/2,\qquad
\displaystyle\sum_{k=0}^x\binom{n}{k}p_1^k(1-p_1)^{n-k}=(1-q)/2
$$

y dar el intervalo $[p_0,p_1]$. Para calcular este intervalo se puede usar  la función `binom.exact` del paquete **epitools**. Su sintaxis es

```{r, eval=FALSE}
binom.exact(x,n,conf.level)
```

donde `x` y `n` representan, respectivamente, el número de éxitos y el tamaño de la muestra, y `conf.level` es $q$, el nivel de confianza en tanto por uno.  El valor por defecto de  `conf.level` es 0.95.

```{example}
Supongamos que, de una muestra de 15 enfermos tratados con un cierto medicamento, solo 1 ha desarrollado taquicardia. Queremos conocer un intervalo de confianza del 95% para la proporción de enfermos tratados con este medicamento que presentan este efecto adverso.

```

Tenemos una población Bernoulli, formada por los enfermos tratados con el medicamento en cuestión, donde los éxitos son los enfermos que desarrollan taquicardia. La fracción de éstos es la fracción poblacional $p$ para la que queremos calcular el intervalo de confianza del 95%.  Para ello cargamos el paquete **epitools** y usamos `binom.exact`:

```{r}
library(epitools)
binom.exact(1,15)
```

El resultado de la función `binom.exact` es un *data frame*; el intervalo de confianza deseado está formado por los números en las columnas `lower` (extremo inferior) y `upper` (extremo superior):

```{r}
binom.exact(1,15)$lower
binom.exact(1,15)$upper
```
Hemos obtenido el intervalo de confianza [`r round(binom.exact(1,15)$lower,3)`,`r round(binom.exact(1,15)$upper,3)`]: podemos afirmar con un nivel de confianza del 95% que el porcentaje de enfermos tratados con este medicamento que presentan este efecto adverso está entre el `r round(100*binom.exact(1,15)$lower,1)`% y el `r round(100*binom.exact(1,15)$upper,1)`%.

Supongamos ahora que el tamaño $n$ de la muestra aleatoria simple es grande; de nuevo, digamos que $n\geq 40$. En esta situación, podemos usar el **Método de Wilson** para aproximar, a partir del Teorema Central del Límite, un intervalo de confianza del parámetro $p$ al nivel de confianza $q\times 100\%$, mediante la fórmula
$$
\frac{\widehat{p}_{X}+\frac{z_{(1+q)/2}^2}{2n}\pm z_{(1+q)/2}\sqrt{\frac{\widehat{p}_{X}(1-\widehat{p}_{X})}{n}+\frac{z_{(1+q)/2}^2}{4n^2}}}{1+\frac{z_{(1+q)/2}^2}{n}}
$$
donde  $z_{(1+q)/2}$ es el cuantil de orden $(1+q)/2$ de una variable aleatoria normal estándar.

Para calcular este intervalo se puede usar  la función `binom.wilson` del paquete **epitools**. Su sintaxis es

```{r, eval=FALSE}
binom.wilson(x,n,conf.level)
```

con los mismos parámetros que `binom.exact`.

```{example}
Supongamos que tratamos 45 ratones con un agente químico, y 10 de ellos desarrollan un determinado cáncer de piel. Queremos calcular un intervalo de confianza al 90% para la proporción $p$ de ratones que desarrollan este cáncer de piel al ser tratados con este agente químico.

```

Como 45 es relativamente grande, usaremos el método de Wilson. Para comparar los resultados, usaremos también el método exacto. Fijaos que, en este ejemplo, $q=0.9$.

```{r}
binom.wilson(10,45,0.9)
binom.exact(10,45,0.9)
```

Con el método de Wilson obtenemos el intervalo [`r round(binom.wilson(10,45,0.9)$lower,3)`,`r round(binom.wilson(10,45,0.9)$upper,3)`] y con el método de Clopper-Pearson, el intervalo [`r round(binom.exact(10,45,0.9)$lower,3)`,`r round(binom.exact(10,45,0.9)$upper,3)`], un poco más ancho: hay una diferencia en los extremos de alrededor de un punto porcentual. 

Supongamos finalmente que la muestra aleatoria simple es considerablemente más grande que la usada en el método de Wilson y que, además, la proporción muestral de éxitos $\widehat{p}_{X}$ está alejada de 0 y de 1. Una posible manera de formalizar estas condiciones es requerir que $n\geq 100$ y que $n\widehat{p}_{X}\geq 10$ y $n(1-\widehat{p}_{X})\geq 10$; observad que estas dos últimas condiciones son equivalentes a que tanto el número de éxitos como el número de fracasos en la muestra sean  como mínimo 10. En este caso, se puede usar la **fórmula de Laplace**, que simplifica la de Wilson (aunque, en realidad, la precede en más de 100 años):  un intervalo de confianza del parámetro $p$ al nivel de confianza $q\times 100\%$ viene dado aproximadamente por la fórmula
$$
\widehat{p}_{X}\pm z_{(1+q)/2}\sqrt{\frac{\widehat{p}_{X}
(1-\widehat{p}_{X})}{n}}
$$
Esta fórmula está implementada en la función `binom.approx` del paquete **epitools**, de uso similar al de las dos funciones anteriores.


```{example}
En una muestra aleatoria  de 500 familias con niños en edad escolar de una determinada ciudad se ha observado que 340 introducen fruta de forma diaria en la dieta de sus hijos.  A partir de este dato, queremos encontrar un intervalo de confianza del 95% para la proporción  real de familias de esta ciudad con niños en edad escolar que incorporan fruta fresca de forma diaria en la dieta de sus hijos.

```

Tenemos una población Bernoulli donde los éxitos son las familias que aportan fruta de forma diaria a la dieta de sus hijos, y la fracción de estas familias en el total de la población es la proporción poblacional $p$ para la que queremos calcular el intervalo de confianza. Como $n$ es muy grande y los números de éxitos y fracasos también lo son, podemos emplear el método de Laplace. 

```{r}
binom.approx(340,500)
```

Por lo tanto, según la fórmula de Laplace, un intervalo de confianza al  95% para la proporción poblacional es [`r round(binom.approx(340,500)$lower,3)`,`r round(binom.approx(340,500)$upper,3)`]. ¿Qué hubiéramos obtenido con los otros dos métodos?

```{r}
binom.wilson(340,500)
binom.exact(340,500)
```

Como podéis ver, los resultados son muy parecidos, con diferencias de unas pocas milésimas.


## Intervalo de confianza para la varianza de una población normal {#sec:ICvar}

Supongamos ahora que queremos estimar la varianza $\sigma^2$, o la desviación típica $\sigma$, de una población que sigue una distribución normal. Tomamos una muestra aleatoria simple de tamaño $n$, y sea  $\widetilde{S}_{X}$ su desviación típica muestral. En esta situación, un intervalo de confianza del $q\times 100\%$ para la varianza $\sigma^2$ es
$$
\left[ \frac{(n-1)\widetilde{S}_{X}^2}{\chi_{n-1,(1+q)/2}^2},\
\frac{(n-1)\widetilde{S}_{X}^2}{\chi_{n-1,(1-q)/2}^2}\right],
$$
donde $\chi_{n-1,(1-q)/2}^2$  y $\chi_{n-1,(1+q)/2}^2$ son, respectivamente, los cuantiles de orden $(1-q)/2$ y $(1+q)/2$ de una variable aleatoria que sigue una distribución $\chi^2$ con
$n-1$ grados de libertad. 

Si conocéis la varianza muestral $\widetilde{S}_{X}^2$, que denotaremos por `varm`, podéis calcular este intervalo de confianza para la varianza con la fórmula siguiente (donde `q`indica el nivel de confianza en tanto por uno $q$):

```{r,eval=FALSE}
c((n-1)*varm/qchisq((1+q)/2,n-1),(n-1)*varm/qchisq((1-q)/2,n-1))
```


Si, en cambio, disponéis de la muestra, podéis calcular  este intervalo de confianza con la función `varTest` del paquete **EnvStats**. La sintaxis es similar a la usada con `t.test`:

```{r,eval=FALSE}
varTest(X,conf.level)$conf.int
```

donde `X` es el vector que contiene la muestra y `conf.level` el nivel de confianza, que por defecto es igual a 0.95.


```{example}
Un índice de calidad de un reactivo químico es el tiempo que tarda en actuar.   Se supone que la distribución de este tiempo de actuación del reactivo es aproximadamente normal.  Se realizaron 30  pruebas independientes, que forman una muestra aleatoria simple, en las que se midió el tiempo de actuación del reactivo.  Los tiempos obtenidos fueron

```

```{r}
reactivo = c(12,13,13,14,14,14,15,15,16,17,17,18,18,19,19,25,25,26,27,30,33,34,35,
40,40,51,51,58,59,83)
```

Queremos usar estos datos para calcular un intervalo de confianza del 95% para la desviación típica de este tiempo de actuación.

El siguiente código calcula un intervalo de confianza al 95% para la varianza a partir del vector `reactivo`:

```{r}
library(EnvStats)
varTest(reactivo)$conf.int
```

Este intervalo de confianza es para la varianza. Como la desviación típica es la raíz cuadrada de la varianza, para obtener un intervalo  de confianza al 95% para la desviación típica, tenemos que tomar la raíz cuadrada de este intervalo para la varianza:

```{r}
sqrt(varTest(reactivo)$conf.int)
```

Por lo tanto un intervalo  de confianza del 95% para la desviación típica poblacional es [`r round(sqrt(varTest(reactivo)$conf.int),2)`]. De nuevo, si os molesta, podéis eliminar el atributo `conf.level` igualándolo a `NULL`.


## Bootstrap

Cuando no tiene sentido usar un método paramétrico como los explicados en las secciones anteriores para calcular un intervalo de confianza porque no se satisfacen las condiciones teóricas que garantizan que el intervalo obtenido contiene el 95% de las veces el parámetro poblacional deseado, podemos recurrir a un método no paramétrico. El más utilizado es el **bootstrap**, que básicamente consiste en:

1. **Remuestrear** la muestra: tomar muchas muestras aleatorias simples de la muestra de la que disponemos, cada una de ellas del mismo tamaño que la muestra original (pero simples, es decir, con reposición).

2. Calcular el estimador sobre cada una de estas submuestras.

3. Organizar los resultados en un vector.

4. Usar este vector para calcular un intervalo de confianza. 

La manera más sencilla de llevar a cabo el cálculo final del intervalo de confianza es el llamado **método de los percentiles**, en el que se toman como extremos del intervalo de confianza del $q\times 100\%$ los cuantiles de orden  $(1-q)/2$ y $(1+q)/2$ del vector de estimadores, pero hay mucho otros métodos; encontraréis algunos en [esta entrada de la Wikipedia](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Deriving_confidence_intervals_from_the_bootstrap_distribution).

```{example}
Volvamos a la muestra de pesos del Ejemplo \@ref(exm:lux), pero supongamos ahora que la variable aleatoria poblacional de la que la hemos extraído no es normal (o que no queremos suponer que lo sea). Vamos a usar el método  *bootstrap* de los percentiles para calcular un intervalo de confianza del 95% para el peso medio poblacional.

```

Para ello, vamos a general 1000 muestras aleatorias simples de la muestra, todas ellas del mismo tamaño que la muestra, calcularemos la media de cada una de estas muestras simples, construiremos un vector con estas medias muestrales, y daremos como extremos del intervalo de confianza los cuantiles de orden 0.025 y 0.975 del vector así obtenido.

```{r,include=FALSE}
options(digits=6)
```


```{r}
set.seed(42)
n=length(pesos)
X=replicate(1000,mean(sample(pesos,n,replace=TRUE)))
IC.boot=c(quantile(X,0.025),quantile(X,0.975))
round(IC.boot,1)
```

El intervalo obtenido en este caso es [`r round(IC.boot,1)`]; como se trata de un método basado en una simulación aleatoria, seguramente con otra semilla de aleatoriedad daría un intervalo diferente. Para comparar, recordad que el intervalo de confianza obtenido con la fórmula basada en la t de Student ha sido [`r round(t.test(pesos)$conf.int,1)`].

El paquete **boot** dispone de la función `boot` para llevar a cabo simulaciones *bootstrap.* Aplicando luego la función `boot.ci` al resultado de la función `boot` obtenemos diversos intervalos de confianza basados en el enfoque *bootstrap*.  La sintaxis básica de la función `boot` es

```{r, eval=FALSE}
boot(X,estadístico,R)
```

donde:

* `X` es el vector que forma la muestra de la que disponemos

* `R` es el número de muestras que queremos extraer de la muestra original

* El `estadístico` es la función que calcula el estadístico deseado de la submuestra, y tiene que tener dos parámetros: el primero representa la muestra original `X` y el segundo representa el vector de índices de una m.a.s. de `X`. Por ejemplo, si vamos a usar la función `boot` para efectuar una simulación *bootstrap* de medias muestrales, podemos tomar como  `estadístico` la función:

```{r}
media.boot=function(X,índices){mean(X[índices])}
```

Por otro lado, el nivel de confianza se especifica en la función `boot.ci` mediante el parámetro `conf` (no `conf.level`, como hasta ahora), cuyo valor por defecto es, eso sí, el de siempre: 0.95.

A modo de ejemplo, vamos a usar las funciones del paquete **boot** para calcular un intervalo de confianza del 95% para la media de la variable aleatoria que ha producido el vector `pesos`. 

```{r}
library(boot)
set.seed(42)
simulacion=boot(pesos,media.boot,1000)
```

El resultado `simulacion` de esta última instrucción es una `list` que incluye, en su componente `t`,  el vector de 1000 medias muestrales obtenido mediante la simulación; sus 10 primeros valores son:

```{r}
simulacion$t[1:10]
```

Calculemos ahora el intervalo de confianza deseado:

```{r, warning=TRUE}
boot.ci(simulacion)
```

Obtenemos cuatro intervalos de confianza para $\mu$, calculados con cuatro métodos a partir de la simulación realizada (y un aviso de que no ha podido calcular un quinto intervalo). Cada uno de estos intervalos es un objeto de una `list` y por lo tanto se puede obtener con el sufijo adecuado, que podréis deducir del resultado de aplicar `str` al resultado de una función `boot.ci`. El intervalo `Percentile` es el calculado con el método de los percentiles que hemos explicado antes, y se obtiene con el sufijo `$percent[4:5]` (no ha dado lo mismo que antes, pese a usar la misma semilla de aleatoriedad, porque el procedimiento interno usado por la función `boot` para remuestrear el vector `pesos` ha sido diferente). No vamos a entrar en detalle sobre los métodos que usa para calcular el resto de intervalos, en realidad todos tienen ventajas e inconvenientes.

```{example}
¿Realmente funciona el enfoque *bootstrap*? Vamos a retomar el experimento realizado en el Ejemplo \@ref(exm:experiment), donde construimos una matriz M cuyas columnas son 200 muestras aleatorias simples de tamaño 50 de una población que sigue una distribución normal estándard.

```

En dicho ejemplo calculamos para cada una de estas muestras el intervalo de confianza del 95% para la media poblacional usando la  fórmula \@ref(eq:form1), que es la recomendada por la teoría en este caso.  De los 200 intervalos calculados, `r Aciertos` contuvieron la media poblacional, lo que representa un `r round(100*Aciertos/200,1)`% de aciertos. 
Ahora vamos a calcular para cada una de estas muestras un intervalo de confianza del 95% por el método *bootstrap* de los percentiles y compararemos las tasas de aciertos. Aunque se puede comprobar fácilmente que no es el caso, para mayor seguridad vamos a volver a generar en las mismas condiciones las 200 muestras de la población, no sea que a lo largo de la lección hayamos modificado inadvertidamente el contenido de la matriz `M` (y así de paso fijamos la semilla de aleatoriedad).


```{r}
set.seed(42)
Poblacion=rnorm(10^7) #La población
mu=mean(Poblacion)  # La media poblacional
M=replicate(200, sample(Poblacion,50,replace=TRUE)) # Las muestras
IC.b=function(X){boot.ci(boot(X,media.boot,1000))$percent[4:5]}
ICs.bootstrap=apply(M,FUN=IC.b,MARGIN=2)
Aciertos.bootstrap=length(which((mu>=ICs.bootstrap[1,]) & (mu<=ICs.bootstrap[2,])))
Aciertos.bootstrap
```

Con el *bootstrap*, hemos acertado en `r Aciertos.bootstrap` ocasiones, lo que supone un `r round(100*Aciertos.bootstrap/200,1)`% de aciertos. 



## Guía rápida

* `t.test(X, conf.level=...)$conf.int` calcula el intervalo de confianza del `conf.level`$\times 100\%$ para la media poblacional usando la fórmula basada en la t de Student aplicada a la muestra `X`.

* `binom.exact(x,n,conf.level=...)` del paquete **epitools**, calcula el intervalo de confianza del `conf.level`$\times 100\%$ para la proporción poblacional aplicando el método de Clopper-Pearson  a una muestra de tamaño `n` con `x` éxitos.


* `binom.wilson(x,n,conf.level=...)` del paquete **epitools**, calcula el intervalo de confianza del `conf.level`$\times 100\%$ para la proporción poblacional aplicando el método de Wilson  a una muestra de tamaño `n` con `x` éxitos.

* `binom.approx(x,n,conf.level=...)` del paquete **epitools**, calcula el intervalo de confianza del `conf.level`$\times 100\%$ para la proporción poblacional aplicando la fórmula de Laplace  a una muestra de tamaño `n` con `x` éxitos.

* `varTest(X,conf.level=...)$conf.int` del paquete **EnvStats**, calcula el intervalo de confianza del `conf.level`$\times 100\%$ para la varianza poblacional usando la fórmula basada en la ji cuadrado aplicada a la muestra `X`.

* `boot(X,E,R)` del paquete **boot**, lleva a cabo una simulación *bootstrap*, tomando `R` submuestras del vector `X` y calculando sobre ellas el estadístico representado por la función `E`.

* `boot.ci` del paquete **boot**, aplicado al resultado de una función `boot`, calcula diversos intervalos de confianza a partir del resultado de la simulación efectuada con `boot`. El nivel de confianza se especifica con el parámetro `conf`. 

## Ejercicios 


### Modelo de test {-}

*(1)* Tomad la muestra de todas las longitudes de pétalos de flores *Iris setosa* contenida en la tabla de datos `iris` y usadla para calcular un intervalo de confianza del 95% para el valor medio de las longitudes de pétalos de esta especie de flores usando la fórmula basada en la t de Student. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha).

*(2)* Tenemos una población de media $\mu$ desconocida. Tomamos una muestra aleatoria simple de tamaño 80 y obtenemos una media muestral de 6.2 y una desviación típica muestral de 1.2. Usad estos datos y la fórmula del intervalo de confianza para la media basado en la t de Student para calcular un intervalo de confianza al 95% para $\mu$. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 2 cifras decimales (sin ceros innecesarios a la derecha).


*(3)* Tenemos una población Bernoulli de proporción poblacional $p$ desconocida. Tomamos una muestra aleatoria simple de 80 individuos y obtenemos una proporción muestral de 35% de éxitos. Calculad un intervalo de confianza para $p$ a un nivel de confianza del 95% usando el método de Wilson. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha).

*(4)* Tomad la muestra de todas las longitudes de pétalos de flores *Iris setosa* contenida en la tabla de datos `iris` y usadla para calcular un intervalo de confianza del 95% para la varianza de las longitudes de pétalos de  esta especie de flores. Tenéis que dar el extremo inferior y el extremo superior, en este orden, separados por una coma (sin paréntesis u otros delimitadores, ni espacios en blanco) y redondeados a 3 cifras decimales (sin ceros innecesarios a la derecha).





### Respuestas al test {-}



```{r, echo=FALSE}
#1
X=iris[iris$Species=="setosa","Petal.Length"]
IC1=t.test(X)$conf.int
#2
x=6.2
n=80
sdm=1.2
conf.level=0.95
IC2=x+(qt((1+conf.level)/2,n-1)*sdm/sqrt(n))*c(-1,1)
#3
IC3=binom.wilson(0.35*80,80)
#4
IC4=varTest(X)$conf.int
```



*(1)*  `r round(IC1[1],2)`,`r round(IC1[2],2)`

Nosotros lo hemos calculado con
```{r}
round(t.test(iris[iris$Species=="setosa","Petal.Length"])$conf.int,2)
```


*(2)* `r round(IC2[1],3)`,`r round(IC2[2],3)`

Nosotros lo hemos calculado con
```{r}
x=6.2
n=80
sdm=1.2
conf.level=0.95
round(x+(qt((1+conf.level)/2,n-1)*sdm/sqrt(n))*c(-1,1),3)
```


*(3)* `r round(IC3$lower,3)`,`r round(IC3$upper,3)`

Nosotros lo hemos calculado con
```{r}
round(binom.wilson(0.35*80,80),3)
```


*(4)* `r round(IC4[1],3)`,`r round(IC4[2],3)`

Nosotros lo hemos calculado con
```{r}
round(varTest(iris[iris$Species=="setosa","Petal.Length"])$conf.int,3)
```




<!--chapter:end:04-Intervalos_de_confianza.Rmd-->

# Contrastes de hipótesis {#chap:contrastes}

En esta lección explicamos algunas instrucciones de R que permiten llevar a cabo contrastes de hipótesis sobre parámetros poblacionales. Antes de empezar, repasemos el vocabulario básico  relacionado con los contrastes de hipótesis:

* **Hipótesis alternativa**, $H_1$: Aquella de la que buscamos evidencia en nuestro estudio.

* **Hipótesis nula**, $H_0$: La hipótesis que estamos dispuestos a aceptar si no encontramos evidencia suficiente de la hipótesis alternativa. Suele plantearse en términos de "no hay diferencia".

* **Contraste bilateral**: Contraste en el que la hipótesis alternativa viene definida por un $\neq$: que un parámetro sea *diferente* de un valor dado, que un parámetro sobre una población sea *diferente* del mismo parámetro sobre otra población, ...

* **Contraste unilateral**: Contraste en el que la hipótesis alternativa viene definida por un $>$ o un $<$: que un parámetro sea *mayor* que un valor dado, que un parámetro sobre una población sea *menor* que el mismo parámetro sobre otra población, ...


* **Error de tipo I**, o **Falso positivo**: Concluir que la hipótesis alternativa es verdadera cuando en realidad es falsa.

* **Nivel de significación**, $\alpha$: La probabilidad de cometer un error de tipo I.

* **Nivel de confianza**, $1-\alpha$: La probabilidad de *no* cometer un error de tipo I.

* **Error de tipo II**, o **Falso negativo**: Concluir que la hipótesis alternativa es falsa cuando en realidad es verdadera.

* **Potencia**, $1-\beta$: La probabilidad de *no* cometer un error de tipo II.

* **Estadístico de contraste**: El valor que se calcula a partir de la muestra obtenida en el estudio y que se usará para tomar la decisión en el contraste planteado.

* **p-valor**: La probabilidad de que, si la hipótesis nula es verdadera, el estadístico de contraste tome un valor tan o más extremo *en el sentido de la hipótesis alternativa* que el obtenido en el estudio

* **Intervalo de confianza** al nivel de confianza $1-\alpha$: Un intervalo en el que el parámetro poblacional que contrastamos tiene probabilidad $1-\alpha$ de pertenecer en el sentido de los intervalos de confianza de la Lección \@ref(chap:IC): es decir, porque se ha obtenido con un procedimiento que produce intervalos que, en el $(1-\alpha)\times 100\%$ de las ocasiones que lo aplicamos a muestras aleatorias simples, contiene el parámetro poblacional. Está formado por los valores del parámetro poblacional que, si fueran los que contrastáramos en nuestro contraste, producirían un p-valor como mínimo $\alpha$. Los intervalos de confianza de los contrastes bilaterales coinciden con los definidos en la Lección \@ref(chap:IC).

* **Regla de rechazo**: Rechazamos la hipótesis nula en favor de la alternativa con un nivel de significación $\alpha$ dado cuando se da alguna de las dos condiciones siguientes (que son equivalentes, es decir, se dan las dos o ninguna):

     * El p-valor és menor que el nivel de significación.
     * El valor contrastado del parámetro poblacional no pertenece al intervalo de confianza del nivel $1-\alpha$.     

```{example,label="monedabase"}
Tenemos una moneda y creemos que está trucada a favor de Cara, es decir, que al lanzarla al aire produce más caras que cruces. Para intentar decidir si esto es cierto o no, lanzamos la moneda al aire 20 veces y obtenemos 15 caras. 

```

Llamemos $p$ a la probabilidad de obtener cara al lanzar al aire esta moneda. Entonces:

* La hipótesis alternativa, de la que buscamos evidencia, es que la moneda está trucada a favor de cara: $H_1: p>0.5$.

* La hipótesis nula, que aceptaremos por defecto si no encontramos evidencia de que la alternativa sea verdadera, es que la moneda no está trucada: $H_0: p=0.5$.

* Cometeríamos un error de tipo I si la moneda fuera honrada y nosotros concluyéramos que está trucada. 

* Cometeríamos un error de tipo II si la moneda estuviera trucada y nosotros concluyéramos que no lo está. 

* El estadístico de contraste en este experimento es simplemente el número de caras en un serie de lanzamientos de la moneda.

* El p-valor es la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda si fuera verdad que $p=0.5$. Como en este caso el número de caras seguiría una distribución binomial $B(20,0.5)$, podemos calcular fácilmente esta probabilidad:

```{r}
1-pbinom(14,20,0.5)
```

|        Por lo tanto, el p-valor es `r round(1-pbinom(14,20,0.5),3)`.

* El intervalo de confianza del 95% de este contraste está formado por los valores de $p$ para los que la probabilidad de obtener 15 o más caras al lanzar al aire 20 veces la moneda es mayor o igual que el 5%, y es [`r round(binom.test(15,20,p=0.5,alternative="greater")$conf.int,4)`].

Y bueno, tras todo este vocabulario, ¿cuál sería la conclusión? El p-valor obtenido significa que si la moneda no estuviera trucada, la probabilidad de obtener el número de caras que hemos obtenido o más es muy pequeña, lo que hace difícil de creer que la moneda no esté trucada. En particular, si trabajamos con un nivel de significación del 5%, como el p-valor es más pequeño que 0.05, rechazamos la hipótesis nula. Equivalentemente, como el intervalo de confianza del 95% para la $p$ está completamente a la derecha del valor que contrastamos, 0.5, con este nivel de confianza hemos de concluir que $p>0.5$. 

En resumen, aceptando una probabilidad de error de tipo I (de concluir que una moneda honrada está trucada) del 5%, rechazamos la hipótesis nula e inferimos que la moneda está trucada a favor de cara. 


```{r, echo=FALSE,fig.cap="\"Hipótesis Nula\" (http://imgs.xkcd.com/comics/null_hypothesis.png (CC-BY-NC 2.5))"}
library(linguisticsdown)
include_graphics2("http://imgs.xkcd.com/comics/null_hypothesis.png")
```


## Contrastes para medias

### El test t {-}

El **test t** para contrastar una o dos medias basado en la t de Student  está implementado en la función `t.test`. Este test usa  diferentes estadísticos según que el contraste sea de una media o de dos; en este último caso, según se usen muestras emparejadas o independientes; y en este último caso, según las poblaciones tengan varianzas iguales o diferentes. Aunque este test solo es exacto (en el sentido de que da la conclusión con el nivel de significación requerido) cuando las poblaciones involucradas siguen  distribuciones normales, el Teorema Central del Límite garantiza que también da resultados aproximadamente correctos cuando las muestras son grandes, aunque las poblaciones no sean normales, por lo que en esta situación también se recomienda su uso. Así pues, aunque en la práctica el test t se use como test "de talla única" para contrastar una o dos medias en cualquier situación,  hay que tener claro que su resultado es fiable tan solo:

* cuando las variables poblacionales involucradas son (aproximadamente) normales, o

* cuando todas las muestras usadas son grandes.


Al final de esta sección explicamos las funciones asociadas a algunos contrastes no paramétricos que pueden usarse cuando estas condiciones no se cumplen.

La sintaxis básica de la función `t.test` es

```{r,eval=FALSE}
t.test(x, y, mu=..., alternative=..., conf.level=..., paired=..., var.equal=...)
```

donde:

* `x` es el vector de datos que forma la muestra que analizamos.

*  `y` es un vector opcional; si lo entramos, R entiende que estamos realizando un contraste de  dos medias, con hipótesis nula la igualdad de estas medias.

*  Podemos sustituir los vectores `x` e `y` por una fórmula `variable1~variable2` que indique que separamos la variable numérica `variable1` en dos vectores definidos por los niveles de un factor `variable2` de dos niveles (o de otra variable asimilable a un factor de dos niveles, como por ejemplo una variable numérica que solo tome dos valores diferentes). Con esta construcción,  R tomará estos vectores en el orden natural de los niveles de `variable2`: `x` será el vector correspondiente al primer nivel e `y` el correspondiente al segundo. Hay que tener esto en cuenta a la hora de especificar la hipótesis alternativa si es unilateral. Si las dos variables de la fórmula son columnas de un *dataframe*, se puede usar el parámetro `data=...` para indicarlo. 

*  Solamente tenemos que especificar el parámetro `mu` si hemos entrado una sola muestra, y en este caso lo hemos de igualar al valor $\mu_0$ que queremos contrastar, de manera que la hipótesis nula será $H_0: \mu=\mu_0$.

*  El parámetro `alternative` puede tomar tres valores: `"two.sided"`, para contrastes bilaterales, y `"less"`  y `"greater"`, para contrastes unilaterales. En esta función, y en todas las  que explicamos en esta lección, su valor por defecto, que no hace falta especificar, es `"two.sided"`. El significado de estos valores depende  del tipo de test que efectuemos:

     * Si el test es de una sola muestra,  `"two.sided"` representa la hipótesis alternativa $H_1: \mu\neq \mu_0$, `"less"` corresponde a  $H_1: \mu< \mu_0$, y `"greater"` corresponde a  $H_1: \mu> \mu_0$. 

     * Si hemos entrado dos muestras y llamamos $\mu_x$ y $\mu_y$ a las medias de las poblaciones de las que hemos extraído las muestras $x$ e $y$, respectivamente, entonces
`"two.sided"` representa la hipótesis alternativa  $H_1: \mu_x
\neq \mu_y$; `"less"` indica que la hipótesis alternativa es $H_1: \mu_x< \mu_y$; y `"greater"`, que la hipótesis alternativa es $H_1: \mu_x> \mu_y$. 

*  El valor del parámetro `conf.level` es el nivel de confianza $1-\alpha$. En esta función, y en todas las  que explicamos en esta lección, su valor por defecto, que no es necesario especificar, es 0.95, que corresponde a un nivel de confianza del 95%, es decir,  a un nivel de significación $\alpha=0.05$.

*  El parámetro `paired` solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias. En este caso, con `paired=TRUE` indicamos que las muestras son emparejadas, y con `paired=FALSE` (que es su valor por defecto) que  son independientes. Si se trata de muestras emparejadas, los vectores `x` e `y` tienen que tener la misma longitud, naturalmente.

*  El parámetro `var.equal` solo lo tenemos que especificar si llevamos a cabo un contraste de dos medias usando muestras independientes, y en este caso sirve para indicar si queremos considerar las dos varianzas poblacionales iguales (igualándolo a TRUE) o diferentes (igualándolo a FALSE, que es su valor por defecto).

La función `t.test` tiene otro parámetro que queremos destacar, que es común a la mayoría de las funciones de estadística inferencial y análisis de datos. Se trata del  parámetro `na.action`, que sirve para especificar qué queremos hacer con los valores NA. Su valor por defecto es `na.omit`, que elimina las entradas  NA de los vectores (o los pares que contengan algún  NA, en el caso de muestras emparejadas). Por ahora, esta opción por defecto es la adecuada, por lo que no hace falta usar este parámetro, pero conviene saber que hay alternativas. Las más útiles son: `na.fail`, que hace que la ejecución pare si hay algún NA en los vectores, y  `na.pass`, que no hace  nada con los NA y permite que las operaciones internas de la función sigan su curso y los manejen como les corresponda.

 

Veamos varios ejemplos de uso de esta función.


```{example, label="norm1bis"}
Consideremos el siguiente vector  de longitud 25:
  
```

```{r}
x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04,3.74,3.24,3.92,2.38,2.82,2.2,
    2.42,2.82,2.84,4.22,3.64,1.77,3.44,1.53)
```

Supongamos que esta muestra ha sido extraída de una población normal. Postulamos que el valor medio $\mu$ de la población no es 2. Para confirmarlo, vamos realizar el contraste
$$
\left\{\begin{array}{l}
H_{0}:\mu=2\\
H_{1}:\mu\neq 2
\end{array}\right.
$$
con nivel de significación $\alpha=0.05$:

```{r}
t.test(x, mu=2, alternative="two.sided", conf.level=0.95)
```

```{r, include=FALSE}
TT1=t.test(x, mu=2, alternative="two.sided", conf.level=0.95)
```


(Como los parámetros `alternative="two.sided"` y `conf.level=0.95` eran los que toma R por defecto, en realidad no hacía falta especificarlos.)
Observad la información que obtenemos con esta instrucción:


*  Información sobre la muestra $x$: su media muestral (`mean of x`) $\overline{x}$, que vale `r TT1$estimate`.

*  La hipótesis alternativa (`alternative hypothesis`), en este caso `true mean is not equal to 2`: la media verdadera, o poblacional, $\mu$ es diferente de 2.

*  El valor `t` que toma el estadístico de contraste,  $T=\frac{\overline{X}-\mu_0}{\widetilde{S}_X/\sqrt{n}}$, sobre la muestra, en este caso `r round(TT1$statistic,3)`, y los grados de libertad `df` (*degrees of freedom*) de su distribución t de Student cuando la hipótesis nula es verdadera, `` df =`r TT1$parameter```.

*  El p-valor (`p-value`) de nuestro test, en este caso `p-value = 4.232e-06`, es decir, $`r round(TT1$p.value,9)`$.

*  Un intervalo de confianza del $(1-\alpha)\times 100\%$ (en nuestro caso, `95 percent confidence interval`) para la $\mu$: en este ejemplo, [`r TT1$conf.int`].



Lo único que no nos dice directamente es si tenemos que rechazar o no la hipótesis nula, pero esto lo deducimos del p-valor: como es más pequeño que el nivel de significación (de hecho, es *muy* pequeño), podemos  rechazar la hipótesis nula, $\mu=2$, en favor de la alternativa, $\mu \neq 2$. Es decir, hay evidencia estadísticamente significativa de que $\mu \neq 2$. Otra manera de decidir si rechazamos o no la hipótesis nula es mirar si el valor que contrastamos pertenece al intervalo de confianza del contraste. Puesto que $2 \notin [`r TT1$conf.int`]$, podemos rechazar la hipótesis nula en favor de la alternativa y concluir que $\mu\neq 2$.


Hagamos ahora el  test cambiando la hipótesis alternativa por $H_{1}:\mu< 3$, es decir,

$$
\left\{\begin{array}{l}
H_{0}:\mu=3\\
H_{1}:\mu< 3
\end{array}\right.
$$
y tomando como nivel de significación $\alpha=0.1$:

```{r}
t.test(x, mu=3, alternative="less", conf.level=0.9)
```

```{r, include=FALSE}
TT2=t.test(x, mu=3, alternative="less", conf.level=0.9)
```


En este caso, el p-valor es `r round(TT2$p.value,3)`, por lo que podemos rechazar la hipótesis nula con un nivel de significación del 10% y concluir, con este nivel de significación (es decir, asumiendo esta probabilidad de equivocarnos), que $\mu<3$; pero fijaos en que con un nivel de significación del 5% no podríamos rechazar la hipótesis nula. 
El intervalo de confianza del 90% es ahora $(-\infty,`r round(TT2$conf.int[2],3)`]$ (`Inf` representa $\infty$). Que no contenga el 3 (aunque por muy poco) también indica que podemos rechazar la hipótesis nula $\mu=3$ en favor de la alternativa $\mu< 3$ con este nivel de confianza.



El p-valor y el intervalo de confianza se pueden obtener directamente, añadiendo a la instrucción `t.test` los sufijos `$p.value` o `$conf.int`, respectivamente. Esperamos que recordéis que en la lección anterior ya usábamos la construcción `t.test(...)$conf.int` para calcular un intervalo de confianza para la media usando la fórmula basada en la t de Student. Es lo correcto, puesto que el intervalo de confianza de un test t bilateral es el que explicamos entonces y no depende para nada de la hipótesis nula (por eso no especificábamos la $\mu$, y dejábamos que la función tomase su valor por defecto, 0). Pero ahora podemos calcular dos intervalos de confianza más, correspondientes a los dos tipos de contrastes unilaterales. En ellos toda la "probabilidad de equivocarnos" se concentra a un lado del intervalo de confianza, en lugar de repartirse por igual a ambos lados.


```{r}
t.test(x, mu=2)$p.value
t.test(x, mu=2)$conf.int
t.test(x, mu=2)$conf.int[1]
t.test(x, mu=2)$conf.int[2]
t.test(x, mu=3, alternative="less", conf.level=0.9)$conf.int
```


Podéis consultar los sufijos necesarios para obtener las otras componentes del resultado en la Ayuda de la función.

```{example, label="colesterol"}
Queremos contrastar si el valor medio del nivel de colesterol en una población es de 220 mg/dl o no, a un nivel de significación del 5%. Es decir, si llamamos $\mu$ a la media de la variable aleatoria "Nivel de colesterol de un individuo de esta población, en mg/dl", queremos realizar el contraste bilateral
$$
\left\{\begin{array}{l}
H_{0}:\mu=220\\
H_{1}:\mu \neq 220
\end{array}\right.
$$
  
```

Para ello, hemos tomado una muestra del nivel de colesterol en plasma de 9 individuos de la población. Los datos obtenidos, en mg/dl, son los siguientes:

```{r}
colesterol=c(203,229,215,220,223,233,208,228,209)
```

Suponemos que el nivel de colesterol en plasma sigue una ley normal y que
por lo tanto  nos podemos fiar del resultado de un test t:
```{r}
t.test(colesterol, mu=220)
```
El p-valor es `r round(t.test(colesterol, mu=220)$p.value,3)`, muy grande y en particular superior a 0.05, por lo tanto no podemos rechazar la hipótesis nula  de que el valor medio sea 220 mg/dl. Además, el intervalo de confianza del 95% del contraste es [`r round(t.test(colesterol, mu=220)$conf.int,2)`], y contiene holgadamente el valor 220.



Más adelante en esta misma sección discutiremos qué podemos hacer si el nivel de colesterol en plasma  no sigue una ley aproximadamente normal, en cuyo caso el resultado de este test t no sirve para nada.


```{example, label="iris"}
Recordad el *dataframe* `iris`, que recoge datos de las flores de 50 ejemplares de cada una de tres especies de iris.

```

```{r}
str(iris)
```

Queremos estudiar si la longitud media $\mu_v$ de los sépalos de las  *Iris virginica* es mayor que la longitud media $\mu_s$ de los sépalos de las *Iris setosa* usando las muestras contenidas en esta tabla de datos. Para ello realizamos el contraste

$$
\left\{\begin{array}{l}
H_{0}:\mu_s=\mu_v\\
H_{1}:\mu_s< \mu_v
\end{array}\right.
$$
En este caso, se trata de un contraste de dos muestras independientes, y como las muestras son grandes, podemos usar con garantías un test t. 

Ahora bien, recordad que el test t concreto que hay que usar depende de si las varianzas de las dos variables cuyas medias comparamos son iguales o diferentes. Como no sabemos nada de las varianzas de las longitudes de los sépalos de estas dos especies, y no nos supone apenas esfuerzo realizar los tests, llevaremos a cabo el contraste en los dos casos: varianzas iguales y varianzas diferentes.^[Se sabe que si las dos muestras provienen de poblaciones normales y son del mismo tamaño, el test t tiende a dar la misma conclusión tanto si se supone que las dos varianzas son iguales como si se supone que son diferentes: véase C. A. Markowski y E. P. Markowski, "Conditions for the Effectiveness of a Preliminary Test of Variance," *The American Statistician* 44  (1990), pp. 322-326. Pero no sabemos si estas longitudes siguen distribuciones normales o no, y que "tienda a dar" la misma conclusión no significa que en un ejemplo concreto con p-valores cercanos al nivel de significación no pueda dar conclusiones diferentes.] Más adelante, en el Ejemplo \@ref(exm:variris), explicamos cómo contrastar si estas dos varianzas son iguales o diferentes.

```{r}
S=iris[iris$Species=="setosa",]$Sepal.Length
V=iris[iris$Species=="virginica",]$Sepal.Length
```

El test suponiendo que las dos varianzas son iguales:

```{r}
t.test(S, V, alternative="less", var.equal=TRUE) 
```

El test suponiendo que las dos varianzas son diferentes:

```{r}
t.test(S, V, alternative="less", var.equal=FALSE) 
```


```{r,include=FALSE}
TT3=t.test(S, V, alternative="less", var.equal=FALSE)$conf.int 
```


En los dos casos el p-valor es  prácticamente 0 y por lo tanto podemos rechazar la hipótesis nula en favor de la alternativa: tenemos evidencia estadísticamente muy significativa de que, en promedio, las flores de la especie setosa tienen sépalos más cortos que las de la especie virginica. El intervalo de confianza del 95% para la diferencia de medias $\mu_s-\mu_v$ en este contraste es  en ambos casos $(-\infty, `r round(TT3[2],2)`]$ y no contiene el 0, que sería el valor de esta diferencia si la hipótesis nula $\mu_s=\mu_v$ fuera verdad.

```{example, label="sueño"}
En un experimento clásico de la primera década del siglo XX, Student quiso comparar el efecto somnífero de dos compuestos químicos, la hiosciamina y la hioscina. La hipótesis a contrastar era que la hioscina es más efectiva que la hiosciamina. Para ello, tomó 10 sujetos y midió su promedio de horas de sueño durante períodos de entre 3 y 9 días bajo tres tratamientos: en condiciones normales, tomando antes de acostarse 0.6 mg de hiosciamina y tomando antes de acostarse 0.6 mg de  hioscina. A continuación, calculó para cada sujeto y cada compuesto la diferencia  "promedio de horas de sueño tomando el compuesto menos promedio de horas de sueño en condiciones  normales". Las diferencias que obtuvo fueron las siguientes (las podéis encontrar en el *dataframe* `sleep` de la instalación básica de R, aunque no lo vamos a usar):

  
  
```


```{r, echo=FALSE}
Sujeto=c(1,2,3,4,5,6,7,8,9,10)
Hiosciamina=c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0.0,2.0)
Hioscina=c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4)
df_student=data.frame(Sujeto,Hiosciamina,Hioscina)
kable(df_student)
```


Una manera de comparar el efecto en las horas de sueño de estos compuestos es comparando las medias de estas diferencias de promedios de horas de sueño: una diferencia media mayor significa que, de media, el compuesto "ha añadido" más horas de sueño al promedio normal. Digamos $\mu_1$ a la media de  las diferencias individuales del promedio de horas de sueño tomando hiosciamina menos el promedio en condiciones normales y $\mu_2$ a la media de  las diferencias individuales del promedio de horas de sueño tomando hioscina menos el promedio en condiciones normales. Tomaremos como hipótesis nula $H_0: \mu_1= \mu_2$ (ambos compuestos tienen el mismo efecto medio sobre las horas de sueño de los individuos) e hipótesis alternativa $H_1: \mu_1<\mu_2$ (la hioscina aumenta más las horas de sueño que la hiosciamina). Si podemos rechazar la hipótesis nula en favor de la alternativa, concluiremos que la hioscina tiene un mayor efecto somnífero que la hiosciamina. 



Observad que se trata de un contraste de dos muestras emparejadas, porque los datos refieren a los mismos 10 pacientes. Vamos a suponer que las diferencias medias en horas de sueño en ambos casos siguen leyes normales  (Student así lo hizo) y que, por lo tanto, el resultado de un test t es fiable.

```{r}
Hiosciamina=c(0.7,-1.6,-0.2,-1.2,-0.1,3.4,3.7,0.8,0,2.0)
Hioscina=c(1.9,0.8,1.1,0.1,-0.1,4.4,5.5,1.6,4.6,3.4)
t.test(Hiosciamina, Hioscina, alternative="less", paired=TRUE)
```

```{r,include=FALSE}
TT4=t.test(Hiosciamina, Hioscina, alternative="less", paired=TRUE)
```


El p-valor es `r round(TT4$p.value,3)`, mucho menor que 0.05, y por lo tanto podemos rechazar la hipótesis nula con un nivel de significación del 5%. Observad también que el intervalo de confianza del 95% para la diferencia de medias $\mu_1-\mu_2$ es $(-\infty,`r round(TT4$conf.int[2],3)`]$ y está totalmente a la izquierda del 0. La conclusión es, pues, que efectivamente la hioscina tiene un mayor efecto somnífero que la hiosciamina y que con un 95% de confianza podemos afirmar que añade, de media, como mínimo 52 minutos (`r round(TT4$conf.int[2],3)` horas) diarios más de sueño.

```{example, label="fumar"}
Veamos un ejemplo de aplicación de `t.test` a una fórmula. Queremos contrastar si es cierto que fumar durante el embarazo está asociado a un peso menor del recién nacido. Si llamamos $\mu_n$ y $\mu_f$ al peso medio de un recién nacido de madre no fumadora y fumadora,  respectivamente, el contraste que queremos realizar es
$$
\left\{\begin{array}{l}
H_{0}:\mu_n=\mu_f\\
H_{1}:\mu_n> \mu_f
\end{array}\right.
$$
Vamos a usar los datos incluidos en la tabla `birthwt` incluida en el paquete **MASS**, que recoge información sobre una muestra de madres y sus hijos.

```

```{r}
library(MASS)
str(birthwt)
```
En la Ayuda de `birthwt` nos enteramos de que la variable `smoke` indica si la madre ha fumado durante el embarazo  (1) o no (0), y que la variable `bwt` da el peso del recién nacido en gramos. Lo primero que haremos será mirar si las muestras de madres fumadoras y no fumadoras contenidas en esta tabla son lo suficientemente grandes como para que el resultado del test t sea fiable.

```{r}
table(birthwt$smoke)
```

Vemos que sí, que ambas son suficientemente grandes.

Para entrar en la instrucción `t.test` los vectores de pesos de hijos de fumadoras y no fumadoras, usaremos la fórmula `bwt~smoke` especificando que `data=birthwt`. Fijaos en que los valores de `smoke` son 0 y 1, y que R los considera ordenados en este orden (basta ver el resultado de la función `table` anterior). Por consiguiente, `bwt~smoke` representa, en este orden, el vector de pesos de recién nacidos de madres no fumadoras (`smoke=0`)   y el vector de pesos de recién nacidos de madres fumadoras (`smoke=1`). Como la hipótesis alternativa es $\mu_n>\mu_f$, deberemos especificar en la función `t.test` que `alternative="greater"`.

Es un contraste de muestras independientes, y por lo tanto el procedimiento para llevarlo a cabo depende de la igualdad o no de las varianzas de los pesos de los recién nacidos en los dos  grupos de madres. Como en el Ejemplo \@ref(exm:iris), vamos a llevar a cabo el test t suponiendo que estas varianzas son iguales y que son diferentes, y cruzaremos los dedos para que la conclusión sea la misma. Otra posibilidad es contrastar antes la igualdad de las varianzas.

```{r}
t.test(bwt~smoke, data=birthwt, alternative="greater", paired=FALSE, var.equal=TRUE)
t.test(bwt~smoke, data=birthwt, alternative="greater", paired=FALSE, var.equal=FALSE)
```

En ambos casos hemos obtenido un p-valor un orden de magnitud  inferior a 0.05, lo que nos permite concluir que, en efecto, los hijos de las madres no fumadoras pesan más al nacer que los de las fumadoras.

En vez de especificar los vectores de pesos con `bwt~smoke,data=birthwt`, hubiéramos podido usar `birthwt$bwt~birthwt$smoke`. Por ejemplo:

```{r}
t.test(birthwt$bwt~birthwt$smoke, alternative="greater", paired=FALSE, var.equal=TRUE)
```



### Tests no paramétricos {-}


Cuando comparamos dos medias, o una media con un valor, usando un test t sobre muestras pequeñas, suponemos que las variables poblacionales que han producido las muestras son normales. En la Lección \@ref(chap:bondad) estudiaremos los contrastes que nos permiten aceptar o rechazar que una muestra provenga de una variable aleatoria con una distribución concreta, pero en estos momentos ya tendría que ser claro que nos podemos encontrar con  conjuntos de datos para los cuales el supuesto de normalidad de la variable poblacional no esté justificado: por ejemplo, porque sean datos cuantitativos discretos o porque la variable sea claramente muy asimétrica (por citar una, la duración del embarazo, con una clara cola a la izquierda: hay un número no despreciable de partos prematuros, pero ningún embarazo dura 11 meses). En las situaciones en las que no estamos seguros de que las variables poblacionales satisfagan aproximadamente las hipótesis de los teoremas que nos garantizan la fiabilidad de las conclusiones de un contraste, por ejemplo de un test t, una salida razonable es usar un **test  no paramétrico** alternativo. 

En el caso de los contrastes de medias, los tests no paramétricos para comparar medias en realidad lo que comparan son las  medianas. Los más populares son los siguientes:


*  El **test de signos**, que permite contrastar si la mediana de una variable aleatoria cualquiera (incluso ordinal) es un valor dado $M_0$ estudiando la distribución de los signos de las diferencias entre este valor y los de una muestra (si la mediana fuera $M_0$, los  números de diferencias positivas y negativas en muestras aleatorias seguirían distribuciones binomiales con $p=0.5$). En R está implementado en la función `SIGN.test` del paquete **BSDA**. Su sintaxis es similar a la de `t.test` para una muestra, cambiando el parámetro `mu`, que en `t.test` sirve para especificar el valor de la media que contrastamos, por `md`, que en `SIGN.test` sirve para especificar el valor de la **mediana** que contrastamos. Esta función también se puede aplicar a dos muestras emparejadas: en este caso, la hipótesis nula del contraste que realiza es que "la mediana de las diferencias de las dos variables es 0".

*  El **test de Wilcoxon** para comparar la media de una variable continua simétrica con un valor dado o las medias de dos variables continuas cuya diferencia sea simétrica por medio de muestras emparejadas. Más en general, se puede usar para comparar la mediana de una variable continua con un valor dado o para comparar la mediana de la diferencia de dos variables continuas (medidas sobre muestras emparejadas) con 0. Observad que cuando las variables en juego son simétricas, las medianas coinciden con las medias y el contraste de medianas es también un contraste de medias. En R está implementado en la función `wilcox.test`  y su sintaxis es la misma que la de `t.test` para una muestra o para dos muestras emparejadas (en este último caso, hay que especificar `paired=TRUE`).


*  El **test de Mann-Whitney** para comparar las medianas de dos variables aleatorias por medio de muestras independientes. En R también está implementado en la función `wilcox.test`  y su sintaxis es la misma que la de `t.test` para dos muestras independientes (especificando `paired=FALSE`), salvo que aquí no hay que especificar si las varianzas son iguales o diferentes, puesto que esto no se usa en este test.



```{example, label="wilcox-colesterol"}
Si los niveles de colesterol no siguen una distribución normal, el test t realizado en el Ejemplo \@ref(exm:colesterol) no sirve para nada. Una posibilidad es entonces no contrastar si el nivel medio de colesterol es 220, sino si el nivel *mediano* es 220. Para ello vamos realizar un test de signos. Los parámetros `alternative="two.sided"` y `conf.level=0.95` son los que usa la función `SIGN.test` por defecto, así que no haría falta especificarlos; los incluimos para que los veáis. 

```

```{r}
library(BSDA)
SIGN.test(colesterol, md=220, alternative="two.sided", conf.level=0.95)
```

```{r, include=FALSE}
TT5=SIGN.test(colesterol, md=220, alternative="two.sided", conf.level=0.95)$Confidence.Intervals[2,2:3]
TT6=wilcox.test(colesterol, mu=220, alternative="two.sided",conf.level=0.95)
```


Observad que la salida de la función es muy similar a la de `t.test` (salvo por los últimos intervalos de confianza, que no vamos a explicar). El p-valor ha dado directamente 1 y el intervalo de confianza al 95% para la mediana ha dado [`r round(TT5,1)`]: por lo tanto, no podemos rechazar que la mediana del nivel de colesterol en la población de la que hemos extraído la muestra sea 220.

También podríamos usar el test de Wilcoxon para realizar este contraste de una mediana:

```{r, warning=TRUE}
wilcox.test(colesterol, mu=220, alternative="two.sided",conf.level=0.95)
```

El p-valor es `r round(TT6$p.value,3)`, la conclusión es la misma. El mensaje de advertencia nos avisa de que la muestra ha contenido valores iguales al valor de la mediana contrastado, por lo que el p-valor obtenido no es exacto. Solo os tenéis que preocupar de un mensaje como este si el p-valor fuera muy cercano al nivel de significación deseado, que no es el caso. 



```{example}
Si las diferencias en promedios de horas de sueño no siguen distribuciones normales, el test t realizado en el Ejemplo \@ref(exm:sueño) no sirve para nada. En este caso, vamos a usar un test de Wilcoxon para muestras emparejadas. Este test en realidad contrastará la *hipótesis nula* de que
si para cada individuo calculamos la diferencia entre el aumento promedio de horas de sueño cuando toma hiosciamina y el aumento promedio tomando hioscina, la **mediana** de la variable aleatoria que define estas diferencias es 0, y como hipótesis alternativa que esta mediana es menor que 0 (y que por lo tanto más de la mitad de las veces es negativa, es decir, que a más de la mitad de la población la hioscina le añade más tiempo promedio de sueño que la hiosciamina). Si las variables "aumento de horas de sueño" en juego son simétricas, estas medianas coinciden con las correspondientes medias y llevamos a cabo el contraste del Ejemplo \@ref(exm:sueño). Si no son simétricas, igualmente estamos contrastando si la hioscina es más efectiva que la hiosciamina, solo que planteándolo de otra manera.

```


```{r, warning=TRUE}
wilcox.test(Hiosciamina, Hioscina, alternative="less", paired=TRUE)
```

En este caso R nos avisa de nuevo de que el p-valor no es exacto, pero esto no afecta a la conclusión dado que el p-valor es muy pequeño: rechazamos la hipótesis nula en favor de la alternativa y también concluimos con este test no paramétrico que la hioscina tiene un mayor efecto somnífero que la hiosciamina.


```{example}
Nos preguntamos si los hijos de madres de 20 años pesan lo mismo al nacer que los de madres de 30 años, o no. Querríamos responder esta pregunta planteándola como un contraste bilateral de los pesos medios de los hijos de madres de 20 y 30 años y usando la muestra recogida en la tabla de datos `birthwt` del paquete **MASS**, que contiene la variable `age` con la edad de las madres.

```

```{r}
hijos.20=birthwt[birthwt$age==20,"bwt"]
hijos.30=birthwt[birthwt$age==30,"bwt"]
c(length(hijos.20),length(hijos.30))
```

Las muestras no son lo suficientemente grandes como para usar un test t si no estamos seguros de que las variables poblacionales sean normales. Como las muestras son independientes, vamos a usar un test de Mann-Whitney para comparar los pesos medianos. Es decir, en vez de traducir
"los hijos de madres de 20 años pesan lo mismo al nacer que los de madres de 30 años" en términos de igualdad de pesos medios, lo traducimos en términos de igualdad de pesos medianos.  (En realidad, la hipótesis nula de este test es que "Es igual de probable que un hijo de madre de 20 años pese más que un hijo de madre de 30 años que al revés", pero no vamos a entrar en este nivel de precisión. Es otra manera de decir que no hay tendencia a que unos pesen más que los otros.)


```{r, warning=TRUE}
wilcox.test(hijos.20, hijos.30, alternative="two.sided",paired=FALSE)
```

El p-valor es `r round(wilcox.test(hijos.20, hijos.30, alternative="two.sided",paired=FALSE)$p.value,2)`, por lo que no podemos rechazar que las medianas de los pesos al nacer de los hijos de madres de 20 años y de 30 sean iguales.



## Contrastes para varianzas


El **test $\chi^2$** para comparar la varianza $\sigma^2$ (o la desviación típica $\sigma$) de una población normal con un valor dado $\sigma_0^2$ (o $\sigma_0$) usa el estadístico
$$
\frac{(n-1)\widetilde{S}_X^2}{\sigma_0^2}
$$
que, si la hipótesis nula $\sigma^2=\sigma_0^2$ es verdadera, sigue una distribución $\chi^2_{n-1}$, de ahí su nombre. Dicho test está convenientemente implementado en la función  `sigma.test` del paquete **TeachingDemos**. Su sintaxis es la misma que la de la función `t.test` para una muestra, substituyendo el parámetro `mu` de `t.test` por el parámetro  `sigma` (para especificar el valor de la desviación típica que contrastamos,  $\sigma_0$) o `sigmasq` (por "sigma al cuadrado", para especificar el valor de la varianza que contrastamos,  $\sigma_0^2$). Como siempre, los valores por defecto de `alternative` y `conf.level` son `"two.sided"` y 0.95, respectivamente. La salida de la función es también similar a la de `t.test`. Veamos un ejemplo.

```{example}
Se ha realizado un experimento para estudiar el tiempo $X$ (en minutos) que tarda un lagarto del desierto en llegar a los 45^o^ partiendo de su temperatura normal mientras está a la sombra. Los tiempos obtenidos (en minutos) en una muestra aleatoria de lagartos fueron los siguientes:

```

```{r}
TL45=c(10.1,12.5,12.2,10.2,12.8,12.1,11.2,11.4,10.7,14.9,13.9,13.3)
```

Supongamos que estos tiempos siguen una ley normal. ¿Aporta este experimento evidencia de que la desviación típica $\sigma$ de $X$ es inferior a 1.5 minutos?
Para responder esta pregunta, hemos de realizar el contraste
$$
\left\{\begin{array}{l}
H_{0}:\sigma= 1.5 \\
H_{1}:\sigma< 1.5
\end{array}\right.
$$
Para ello, usaremos la función  `sigma.test` aplicada a esta muestra y a `sigma=1.5`:
```{r}
library(TeachingDemos)
sigma.test(TL45, sigma=1.5, alternative="less")
```
El p-valor que obtenemos es `r round(sigma.test(TL45, sigma=1.5, alternative="less")$p.value,3)`, muy grande, por lo que no tenemos evidencia que nos permita concluir que $\sigma<1.5$.

**¡Atención!** El intervalo de confianza que da la función `sigma.test` es siempre para la varianza, aunque le entréis el valor de la desviación típica. Así que si queréis un intervalo de confianza para la desviación típica, tenéis que tomar la raíz cuadrada del que os da `sigma.test`:

```{r}
sqrt(sigma.test(TL45, sigma=1.5, alternative="less")$conf.int)
```


El test $\chi^2$ no se usa mucho en la práctica. En parte, porque realmente es poco interesante ya que suele ser difícil conjeturar la desviación típica a contrastar, y en parte porque su validez depende fuertemente de la hipótesis de que la variable aleatoria poblacional sea normal. En cambio, el contraste de las desviaciones típicas de dos poblaciones sí que es muy utilizado. Por ejemplo, en un contraste de dos medias usando un test t sobre dos muestras independientes, nos puede interesar conocer *a priori* si las varianzas poblacionales son iguales o diferentes, en lugar de realizar el test bajo ambas suposiciones. Si no las conocemos, ¿cómo podemos saber cuál es el caso? Si las dos variables poblacionales son normales, podemos contrastar la igualdad de las varianzas con el **test F**, basado en el estadístico
$$
\frac{\widetilde{S}_{X_1}^2} {\widetilde{S}_{X_2}^2}
$$
que, si las dos poblaciones son normales y tienen la misma varianza, sigue una distribución F de Fisher-Snedecor. Por desgracia, este test es también muy sensible a la no normalidad de las poblaciones objeto de estudio: a la que una de ellas se aleja un poco de la normalidad, el test deja de dar resultados fiables.^[Véanse: E. S. Pearson, "The analysis of variance in cases of non-normal variation," *Biometrika* 23 (1931), pp. 114-133; G. E. P. Box, "Non-normality and tests on variances," *Biometrika*  40  (1953), pp. 318-335.]

La función para efectuar este test es `var.test`  y su sintaxis básica es la misma que la de `t.test` para dos muestras:
```{r,eval=FALSE}
var.test(x, y, alternative=..., conf.level=...)
```
donde  `x` e `y` son los dos vectores de datos, que se pueden especificar mediante una fórmula como en el caso de `t.test`, y el parámetro `alternative` puede tomar los tres mismos valores que en los tests anteriores: su valor por defecto es, como siempre, `"two.sided"`, que es el que nos permite contrastar si las varianzas son iguales o diferentes.

```{example, label="variris"}
Suponiendo que las longitudes de los sépalos de las flores de las diferentes especies de iris siguen leyes normales, ¿hubiéramos podido considerar *a priori* iguales  las varianzas de las dos muestras en el Ejemplo \@ref(exm:iris)? Veamos:
  
  
```

```{r}
S=iris[iris$Species=="setosa",]$Sepal.Length
V=iris[iris$Species=="virginica",]$Sepal.Length
var.test(S,V)
```

El p-valor es $`r round(var.test(S,V)$p.value,6)`$, muy pequeño. Por lo tanto, podemos rechazar la hipótesis nula de que las dos varianzas son iguales, en favor de la hipótesis alternativa de que las dos varianzas son diferentes. Así, pues, bastaba realizar solo el `t.test` con `var.equal=FALSE`.  

Puede ser conveniente remarcar aquí que el intervalo de confianza obtenido con `var.test` es para *el cociente de varianzas poblacionales*  $\sigma^2_x/\sigma^2_y$, no para su diferencia. Por lo tanto, para contrastar si las varianzas son iguales o diferentes, hay que mirar si el 1 pertenece o no al intervalo obtenido. En este ejemplo, el intervalo de confianza al 95% ha sido [`r round(var.test(S,V)$conf.int,3)`] y no contiene el 1, lo que confirma la evidencia de que las varianzas son diferentes.


  
```{example}
Queremos contrastar si los gatos adultos macho pesan más que los gatos adultos hembra.  Para ello usaremos los datos recogidos en el *dataframe* `cats` del paquete **MASS**, que contiene información sobre el peso de una muestra de gatos adultos, separados por su sexo.

```

```{r}
str(cats)
table(cats$Sex)
```
Consultando la Ayuda de `cats` nos enteramos de que la variable `Bwt` contiene el peso de cada gato en kg, y la variable `Sex` contiene el sexo de cada gato: F para hembra (*female*) y M para macho (*male*). Como vemos en la tabla de frecuencias, los números de ejemplares de cada sexo son grandes.

Así pues, si llamamos $\mu_m$ al peso medio de un gato macho adulto y 
$\mu_h$ al peso medio de un gato hembra adulto, el contraste que vamos a realizar es
$$
\left\{\begin{array}{l}
H_{0}:\mu_m=\mu_h\\
H_{1}:\mu_m>\mu_h
\end{array}\right.
$$
y para ello antes vamos a contrastar si las varianzas de ambas poblaciones son iguales o diferentes, para luego poder aplicar la función `t.test` con el valor de `var.equal` adecuado. 
Vamos a suponer que los pesos en ambos sexos siguen leyes normales. Para que el contraste de las varianzas sea fiable es necesario que esta suposición sea cierta; para el de los pesos medios, no, ya que ambas muestras son grandes.
El contraste de la igualdad de varianzas es el siguiente:
```{r}
var.test(Bwt~Sex, data=cats)
```

El p-valor es $`r round(var.test(Bwt~Sex, data=cats)$p.value,5)`$, y por lo tanto podemos rechazar la hipótesis nula de que las varianzas son iguales y concluir que son diferentes. Así que en el test t las consideraremos diferentes. 

Recordemos ahora que la hipótesis alternativa que queremos contrastar es $H_{1}:\mu_m>\mu_h$. En el factor `cats$Sex`, la F (hembra) va antes que la M (macho), y, por tanto,
si entramos los vectores de pesos mediante `Bwt~Sex,data=cats`, el primer vector corresponderá a las gatas y el segundo a los gatos. Así pues,  la hipótesis alternativa que tenemos que especificar es que la media del primer vector es inferior a la del segundo vector: `alternative="less"`. 
```{r}
t.test(Bwt~Sex, data=cats, alternative="less",var.equal=FALSE)
```
Como el p-valor es prácticamente 0, podemos concluir que, efectivamente, de media, los gatos adultos pesan más que las gatas  adultas.


Hemos insistido en que el test F solo es válido si las dos poblaciones cuyas varianzas comparamos son normales. ¿Qué podemos hacer si dudamos de su normalidad? Usar un test no paramétrico que no presuponga esta hipótesis. Hay diversos tests no paramétricos para realizar contrastes bilaterales de dos varianzas. Aquí os recomendamos el **test de Fligner-Killeen**, implementado en la función `fligner.test`. Se aplica o bien a una `list` formada por las dos muestras, o bien a una fórmula que separe un vector numérico en dos muestras por medio de un factor de dos niveles.

```{example}
Si queremos contrastar si las varianzas de las longitudes de los sépalos de las flores iris setosa y virginica son iguales o no sin presuponer que siguen leyes normales, podemos usar el test de Fligner-Killeen de la manera siguiente:
  
  
```


```{r}
fligner.test(list(S,V))
```
El p-valor es `r round(fligner.test(list(S,V))$p.value,4)`, por lo que podemos concluir que las varianzas son diferentes.

```{example}
Si queremos contrastar si las varianzas de los pesos de los gatos y las gatas adultos son iguales o no sin presuponer que dichos pesos tienen distribuciones normales, podemos usar el test de Fligner-Killeen de la manera siguiente:
  
  
```


```{r}
fligner.test(Bwt~Sex, data=cats)
```
El p-valor es $`r round(fligner.test(Bwt~Sex, data=cats)$p.value,5)`$, por lo que podemos concluir que las varianzas son diferentes.




## Contrastes para proporciones



Cuando tenemos que efectuar un contraste sobre una probabilidad de éxito $p$ de una variable Bernoulli,  podemos emplear el **test binomial exacto**. Este test se basa en que, si la hipótesis nula $H_0: p=p_0$ es verdadera, los números de éxitos en muestras aleatorias simples de tamaño $n$ de la variable poblacional, que será de tipo $Be(p_0)$, siguen una ley binomial $B(n,p_0)$. Este test está implementado en la función `binom.test`, cuya sintaxis es

```{r, eval=FALSE}
binom.test(x, n, p=..., alternative=..., conf.level=...)
```
donde

*  `x` y  `n`  son números naturales: el número de éxitos y el tamaño de la muestra.

*  `p` es la probabilidad de éxito que queremos contrastar.

*  El significado de `alternative` y `conf.level`, y sus posibles valores, son los usuales.

Fijaos en particular que `binom.test` no se aplica directamente al vector de una muestra, sino a su número de éxitos y a su longitud. Si la muestra es un vector binario `X`, el número de éxitos será `sum(X)` y la longitud `length(X)`.

```{remark}
El intervalo de confianza para la $p$ que da `binom.test` en un contraste bilateral es el de Clopper-Pearson.

```

```{example}
Recordemos el Ejemplo \@ref(exm:monedabase), donde, 
en una serie de 20 lanzamientos de una moneda, había obtenido 15 caras. ¿Podemos sospechar que la moneda está trucada a favor de cara? Como comentamos en ese ejemplo, si llamamos $p$ a la probabilidad de obtener cara con esta moneda, el contraste que queremos realizar es 
$$
\left\{\begin{array}{l}
H_{0}:p=0.5\\
H_{1}:p> 0.5
\end{array}\right.
$$
Usaremos la función `binom.test`:



```

```{r}
binom.test(15,20, p=0.5, alternative="greater")
```
El p-valor del test es `r round(binom.test(15,20, p=0.5, alternative="greater")$p.value,4)`  y el intervalo de confianza que nos da este test  para la $p$ es [`r round(binom.test(15,20, p=0.5, alternative="greater")$conf.int,4)`]. Ambos valores coinciden con los dados en el ejemplo original



Cuando la muestra es grande, pongamos de 40 o más sujetos, podemos usar también el **test aproximado**, basado en la aproximación de la distribución de la proporción muestral por medio de una normal dada por el Teorema Central del Límite. En R está implementado en la función  `prop.test`, que además también sirve  para contrastar dos proporciones por medio de muestras independientes grandes. Su sintaxis es

```{r, eval=FALSE}
prop.test(x, n, p =..., alternative=..., conf.level=...)
```

donde:

*  `x` puede ser dos cosas:

      *  Un número natural: en este caso, R entiende que es el número de éxitos en una muestra.
      *  Un vector de dos números naturales: en este caso, R entiende que es un contraste de dos proporciones y que estos son los números de éxitos en las muestras.

*  Cuando trabajamos con una sola muestra, `n` es su tamaño. Cuando estamos trabajando con dos muestras, `n` es el vector de dos entradas de sus tamaños. 

*  Cuando trabajamos con una sola muestra, `p` es la proporción poblacional que contrastamos. En el caso de un contraste de dos muestras, no hay que especificarlo.

*  El significado de `alternative` y `conf.level`, y sus posibles valores, son los usuales.

Veamos algunos ejemplos más.

```{example}
Queremos contrastar si la proporción de estudiantes zurdos en la UIB es diferente del 10%, el porcentaje estimado de zurdos en España. Es decir, si llamamos $p$ a la proporción de estudiantes zurdos en la UIB, queremos realizar el contraste
$$
\left\{
\begin{array}{l}
H_0:p=0.1\\
H_1:p\neq 0.1
\end{array}
\right.
$$
Para ello, tomamos una muestra de 50 estudiantes de la UIB encuestados al azar y resulta que 3 son zurdos. Vamos a suponer que forman una muestra aleatoria simple. Como la muestra es grande ($n=50$) usaremos la función `prop.test`.

  
```



```{r}
prop.test(3, 50, p=0.1)
```

El p-valor obtenido en el test es `r round(prop.test(3, 50, p=0.1)$p.value,2)`, muy  superior a 0.05.  Por lo tanto, no podemos rechazar que un 10% de los estudiantes de la UIB sean zurdos. El intervalo de confianza del 95% para $p$ que hemos obtenido es [`r round(prop.test(3, 50, p=0.1)$conf.int,3)`].

La conclusión usando el test binomial hubiera sido la misma:
```{r}
binom.test(3, 50, p=0.1)
```

Ya que estamos, comprobemos que el intervalo de confianza del 95% obtenido con `binom.test` es efectivamente el de Clopper-Pearson:

```{r}
library(epitools)
binom.exact(3,50)
```

```{remark}
El intervalo de confianza que se obtiene con `prop.test` en un contraste bilateral es el  de Wilson modificado mediante una **corrección de continuidad**, un ajuste que se recomienda realizar cuando una distribución discreta (en este caso, una binomial) se aproxima mediante una distribución continua. En concreto, la fórmula que utiliza `prop.test` es la que se explica en [esta entrada de la Wikipedia](https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval_with_continuity_correction). Podéis indicar que R no efectue esta corrección de continuidad con el parámetro `correct=FALSE`: 



```


```{r}
prop.test(3, 50, p=0.1, correct=FALSE)
```

(Observad que sin el `correct=FALSE`, el encabezamiento del resultado del `prop.test` es `1-sample proportions test with continuity correction`, mientras que con `correct=FALSE` es `1-sample proportions test without continuity correction`). Comprobemos que el intervalo de confianza del 95% que hemos obtenido ahora sí que es el de Wilson:


```{r}
binom.wilson(3, 50)
```


```{example, label=trampas1}
Una empresa que fabrica trampas para cucarachas ha producido una nueva versión de su trampa más popular y afirma que la nueva trampa mata más cucarachas que la vieja. Hemos llevado a cabo un experimento para comprobarlo. Hemos situado dos trampas en dos habitaciones. En cada habitación hemos soltado 60 cucarachas. La versión vieja de  la trampa  ha matado 40 y la nueva, 48. ¿Es suficiente evidencia de que la nueva trampa es más efectiva que la vieja?
  
  
```

Digamos $p_v$ y $p_n$ a las proporciones de cucarachas que matan la trampa vieja y la trampa nueva, respectivamente. La hipótesis nula será que las trampas de los dos tipos son igual de efectivas, $H_0:p_v=p_n$, y la hipótesis alternativa que las trampas nuevas  son más efectivas que las viejas, $H_1:p_v<p_n$. Los tamaños de las muestras nos permiten usar la función `prop.test`.



```{r}
prop.test(c(40,48),c(60,60),alternative="less")
```

El p-valor es `r round(prop.test(c(40,48),c(60,60),alternative="less")$p.value,3)`, y el intervalo de confianza que nos da el test, [`r round(prop.test(c(40,48),c(60,60),alternative="less")$conf.int,3)`], es para la diferencia de proporciones $p_v-p_n$ y contiene el 0, aunque por poco. En resumen,  a un nivel de significación  de 0.05 no encontramos evidencia de que la trampa nueva sea mejor que la vieja, pero el resultado no es del todo concluyente y convendría llevar a cabo otro experimento con más cucarachas para aumentar la potencia (cf. Ejemplo \@ref(exm:trampas1bis) en la próxima sección).


La función `prop.test` solo sirve para contrastar dos proporciones cuando las dos muestras son independientes y grandes. Un test que se puede usar siempre para contrastar dos proporciones usando muestras independientes es el **test exacto de Fisher**, que usa una distribución hipergeométrica.


Supongamos que evaluamos una característica dicotómica (es decir, que solo puede tomar dos valores  y por tanto define distribuciones de Bernoulli) sobre dos poblaciones y tomamos dos muestras independientes, una de cada población. Resumimos los resultados en una tabla como la que sigue:



$$
\begin{array}{r|c}
 & \quad\mbox{Población}\quad \\
\mbox{Característica} &\quad 1 \qquad 2\quad \\\hline
 \mbox{Sí} &\quad a  \qquad b\quad \\
 \mbox{No} &\quad c  \qquad d\quad
\end{array}
$$

Llamemos $p_{1}$ a la proporción de individuos con la característica bajo estudio en la población 1 y $p_{2}$ a su proporción en la población 2. Queremos contrastar la hipótesis nula $H_{0}:p_1=p_2$ contra alguna hipótesis alternativa. Por ejemplo, en el experimento de las trampas para cucarachas, las poblaciones vendrían definidas por el tipo de trampa, y la característica que tendríamos en cuenta sería si la cucaracha ha muerto o no, lo que nos daría la tabla siguiente:

$$
\begin{array}{r|c}
 & \qquad\mbox{Trampas}\quad \\
&\quad \mbox{Viejas}\qquad \mbox{Nuevas}\\\hline
 \mbox{Muertas} &\qquad 40  \qquad\qquad 48\quad \\
 \mbox{Vivas} &\qquad 20  \qquad\qquad 12\quad
 \end{array}
$$



El test exacto de Fisher está implementado en la función `fisher.test`. Su sintaxis es 

```{r, eval=FALSE}
fisher.test(x, alternative=..., conf.level=...)
```
donde

*  `x` es la matriz  $\left(\begin{array}{cc} a & b\\ c & d\end{array}\right)$,  en la que los números de éxitos van en la primera fila y los de fracasos en la segunda, y las poblaciones se ordenan por columnas.

*  El significado de `alternative` y `conf.level`, y sus posibles valores, son los usuales.

Así, en el ejemplo de las trampas para cucarachas, entraríamos:
```{r}
Datos=rbind(c(40,48),c(20,12))
Datos
fisher.test(Datos, alternative="less")
```
y obtenemos de nuevo un p-valor cercano a `r round(fisher.test(Datos, alternative="less")$p.value,3)`. 

Hay que ir con cuidado con la interpretación del intervalo de confianza que da esta función: no es ni para la diferencia de las proporciones ni para su cociente, sino para su **odds ratio**: el cociente
$$
\Big({\frac{p_v}{1-p_v}}\Big)\Big/\Big({\frac{p_n}{1-p_n}}\Big).
$$  

Recordad que si la probabilidad de un suceso $A$ es $P(A)$, sus **odds** son el cociente
$$
\mbox{Odds}(A)=\frac{P(A)}{1-P(A)}
$$
que mide cuántas veces es más probable $A$ que su contrario. Las *odds* son una función creciente de la probabilidad, y por lo tanto
$$
\mbox{Odds}(A)<\mbox{Odds}(B)\Longleftrightarrow P(A)<P(B).
$$
Esto permite comparar *odds* en vez de probabilidades, con la misma conclusión. Por ejemplo, en nuestro caso, como el intervalo de confianza para la *odds ratio* va de 0 a `r round(fisher.test(Datos, alternative="less")$conf.int[2],3)`, en particular contiene el 1, por lo que no podemos rechazar que
$$
\Big({\frac{p_v}{1-p_v}}\Big)\Big/\Big({\frac{p_n}{1-p_n}}\Big)=1,
$$
es decir, no podemos rechazar que 
$$
\frac{p_v}{1-p_v}=\frac{p_n}{1-p_n}
$$
y esto es equivalente a $p_v=p_n$. Si, por ejemplo, el intervalo de confianza hubiera ido de 0 a 0.8, entonces la conclusión a este nivel de confianza hubiera sido que
$$
\Big({\frac{p_v}{1-p_v}}\Big)\Big/\Big({\frac{p_n}{1-p_n}}\Big)<1
$$
es decir,  que 
$$
\frac{p_v}{1-p_v}<\frac{p_n}{1-p_n}
$$
y esto es equivalente a $p_v<p_n$.




```{example}
Para determinar si el Síndrome de Muerte Súbita del Recién Nacido (*SIDS*, por sus siglas en inglés) tiene algún componente genético, se estudiaron parejas de gemelos y mellizos en las que se dio algún caso de SIDS. Sean $p_1$ la proporción de casos con exactamente una muerte por SIDS entre las parejas de gemelos con algún caso de SIDS, y $p_2$ la proporción de casos con exactamente una muerte por SIDS entre las parejas de mellizos con algún caso de SIDS. La hipótesis de trabajo es que si el SIDS tiene componente genético, será más probable que un gemelo de un muerto por SIDS también lo sufra que si solo es mellizo, y por lo tanto que en las parejas de gemelos ha de ser más raro que haya exactamente un caso de SIDS que en las parejas de mellizos. Es decir, que $p_1<p_2$. Así pues, queremos realizar el contraste
$$
\left\{\begin{array}{l}
H_0:p_1=p_2\\
H_1:p_1< p_2
\end{array}\right.
$$
```

En un estudio se obtuvieron los datos siguientes:

$$
\begin{array}{r|c}
 & \ \mbox{Tipo de gemelos}\  \\
\mbox{Casos de SIDS} &\ \mbox{Gemelos}\qquad \mbox{Mellizos}\\\hline
\mbox{Uno} & \quad\  23 \qquad\quad\quad\quad \ 35\quad  \\
 \mbox{Dos} & \quad\  1  \quad\quad\qquad\quad \ \hphantom{3}  2 \quad
 \end{array}
$$

Vamos a realizar el contraste. Observad que damos la tabla de manera que $p_1$ es la proporción de parejas con un solo caso de SIDS entre las de la población 1 (gemelos), y $p_{2}$ es la proporción de parejas con un solo caso de SIDS entre las de la población 2 (mellizos). Por tanto hemos de aplicar `fisher.test` a esta matriz y $p_1<p_2$ corresponderá a `alternative="less"`.
```{r}
Datos=rbind(c(23,35),c(1,2))
Datos
fisher.test(Datos, alternative="less")
```
El p-valor es `r round(fisher.test(Datos, alternative="less")$p.value, 3)`, muy grande, por lo que no obtenemos evidencia de componente genético en el SIDS.


Supongamos ahora que queremos comparar dos proporciones usando muestras emparejadas. Por ejemplo, supongamos que evaluamos dos características dicotómicas sobre una misma muestra de $n$ sujetos. Resumimos los resultados obtenidos en la tabla siguiente:

$$
\begin{array}{r|c}
 & \ \mbox{Característica 1}\  \\
\mbox{Característica 2} &\ \ \, \mbox{Sí}\qquad \mbox{No}\\\hline
 \mbox{Sí} & \quad\ \  a \qquad \ \ \, b\quad  \\
 \mbox{No} & \quad\ \   c  \qquad \ \ \, d\quad
 \end{array}
$$


donde $a+b+c+d=n$. Esta tabla quiere decir, naturalmente, que $a$ sujetos de la muestra tuvieron la característica 1 y la característica 2, que $b$ sujetos de la muestra tuvieron la característica 2 pero no tuvieron la característica 1, etc.

Vamos a llamar $p_{1}$ a la proporción poblacional de individuos con la característica 1, y $p_{2}$ a la proporción poblacional de individuos con la característica 2. Queremos contrastar la hipótesis nula $H_{0}:p_1=p_2$ contra alguna hipótesis alternativa. En este caso, no pueden usarse las funciones `prop.test` o  `fisher.test`. Tenemos dos soluciones posibles.

La primera nos permite realizar el contraste bilateral
$$
\left\{\begin{array}{l}
H_{0}:p_1=p_2\\
H_{1}:p_1\neq p_2
\end{array}\right.
$$
cuando $n$ es grande (digamos que $n\geq 100$) y el número $b+c$ de **casos discordantes** (en los que una característica da Sí y la otra da No) es razonablemente grande, pongamos $\geq 20$. En esta situación podemos usar el  **test de McNemar**, que se lleva a cabo en R con la instrucción `mcnemar.test`. Su sintaxis básica es

```{r, eval=FALSE}
mcnemar.test(X)
```

donde `X` es la matriz $\left(\begin{array}{cc}
a & b\\ c& d
\end{array}\right)$
que corresponde a la tabla anterior. 



```{example, label="asma1"}
Para comparar la efectividad de dos tratamientos del asma, se escogieron 200 pacientes con asma severo, y a cada uno se le trató durante un mes con el tratamiento A o el tratamiento B, decidiéndose cada tratamiento al azar; tras esta fase de tratamiento, se les dejó sin tratamiento durante un mes, y a continuación a cada uno se le trató durante un mes con el otro tratamiento (B si antes había recibido A, A si antes había recibido B). 
Se anotó si durante cada periodo de tratamiento cada enfermo visitó o no el servicio de urgencias por dificultades respiratorias. Los resultados del experimento se resumen en la tabla siguiente ("Sí" significa que sí que acudió a urgencias por dificultades respiratorias):
  
  
```

  
$$
\begin{array}{r|c}
 & \ \mbox{Tratamiento A}\  \\
\mbox{Trat. B} &\quad \ \mbox{ Sí}\qquad\quad \mbox{No}\quad \\\hline
 \mbox{Sí} & \quad \  71 \qquad\quad    48\quad  \\
 \mbox{No} & \quad \   30  \qquad\quad   51\quad
 \end{array}
$$



Queremos determinar si hay diferencia en la efectividad de los dos tratamientos. Para ello, entramos la tabla anterior en una matriz y le aplicamos la función `mcnemar.test`:

```{r}
Datos=matrix(c(71,48,30,51),nrow=2,byrow=TRUE)
Datos
mcnemar.test(Datos)
```
El p-valor del test es `r round(mcnemar.test(Datos)$p.value,3)`, ligeramente superior a 0.05, por lo tanto no permite concluir a un nivel de significación del 5%  que haya evidencia de que la efectividad de los dos tratamientos sea diferente, pero sería conveniente llevar a cabo un estudio más amplio.


Otra posibilidad para realizar un contraste de dos proporciones usando muestras emparejadas, que no requiere de ninguna hipótesis sobre los tamaños de las muestras,  es usar de manera adecuada la función `binom.test`. Para explicar este método, consideremos la tabla siguiente, donde ahora damos las probabilidades poblacionales de las cuatro combinaciones de resultados:
$$
\begin{array}{r|c}
 & \ \mbox{Característica 1}\  \\
\mbox{Característica 2} &\quad \ \!\mbox{Sí}\qquad\quad\, \mbox{No}\quad \\\hline
 \mbox{Sí} & \quad \  p_{11}  \qquad\quad p_{01}\quad  \\
 \mbox{No} & \quad \  p_{10} \qquad\quad  p_{00}\quad
 \end{array}
$$


De esta manera $p_1=p_{11}+p_{10}$ y $p_2=p_{11}+p_{01}$. Entonces,  $p_1=p_2$ es equivalente a $p_{10}=p_{01}$ y cualquier hipótesis alternativa se traduce en la misma desigualdad, pero para $p_{10}$ y $p_{01}$: $p_1\neq p_2$ es equivalente a $p_{10}\neq p_{01}$; $p_1< p_2$ es equivalente a $p_{10}< p_{01}$; y $p_1> p_2$ es equivalente a $p_{10}> p_{01}$. Por lo tanto podemos traducir el contraste sobre $p_1$ y $p_2$ al mismo contraste sobre $p_{10}$ y $p_{01}$. La gracia ahora está en que  si la hipótesis nula $p_{10}=p_{01}$ es cierta, entonces, en el total de casos discordantes, el número de sujetos en los que la característica 1 da Sí y la característica 2 da No sigue una ley binomial con $p=0.5$. Por lo tanto, podemos efectuar el contraste usando un test binomial exacto tomando como muestra los casos discordantes de nuestra muestra, de tamaño $b+c$, como éxitos los sujetos  que han dado Sí en la característica 1  y No en la característica 2, de tamaño $c$, con proporción a contrastar $p=0.5$ y con hipótesis alternativa la que corresponda. La ventaja de este test es que su validez no requiere de ninguna hipótesis sobre los tamaños de las muestras.  El inconveniente es que el intervalo de confianza que nos dará será para $p_{10}/(p_{10}+p_{01})$, y no permite obtener un intervalo de confianza para la diferencia o el cociente de las probabilidades $p_1$ y $p_2$ de interés.

```{example}
Usemos el test binomial para llevar a cabo el contraste bilateral del Ejemplo \@ref(exm:asma1). Habíamos obtenido 30+48=78 casos discordantes, de los que 48 eran casos en los que el tratamiento A había dado Sí y el tratamiento B había dado No.

```

```{r}
binom.test(48, 78, p=0.5)
```
Obtenemos de nuevo un p-valor en la zona de penumbra, ligeramente superior a 0.05. 


```{example}
Para determinar si un test casero de VIH basado en un frotis bucal da más positivos (que seguramente serán falsos positivos) que el test de VIH de referencia, basado en una analítica de sangre que detecta la presencia del virus, se tomó una muestra aleatoria de 241 individuos en situación de riesgo,  y a todos se les realizaron ambos tests. Los resultados se resumen en la tabla siguiente:
  
```

$$
\begin{array}{r|c}
 & \ \mbox{Test estándar}\  \\
\mbox{Test casero}   &\quad \ \mbox{ Positivo}\qquad\quad \quad \mbox{Negativo}\quad \\\hline
 \mbox{Positivo} & \quad\ \  72  \qquad\qquad\qquad\    10 \quad  \\
 \mbox{Negativo} & \quad\quad   2 \qquad\qquad\quad\quad \  157  \quad
 \end{array}
$$



Si llamamos $p_{c}$ a la probabilidad de que el test casero dé positivo y 
$p_{e}$ a la probabilidad de que el test estándar dé positivo, queremos realizar el contraste
$$
\left\{\begin{array}{l}
H_{0}:p_{e}=p_{c}\\
H_{1}:p_{e}< p_{c}
\end{array}\right.
$$

El número de casos discordantes es pequeño (10+2=12) y además el test es unilateral, así que usaremos el test binomial. Como queremos realizar un contraste unilateral, hay que pensar en cómo traducir la hipótesis alternativa en términos de una hipótesis sobre la probabilidad $p=p_{10}/(p_{10}+p_{01})$ de que un caso discordante tenga la característica 1 (la de las columnas). Veamos, $p_{e}< p_{c}$ significa que la característica de las columnas es menos probable que la de las filas, por tanto se ha de traducir en que la probabilidad de tener la característica de las columnas y no la de las filas es más pequeña que la probabilidad de tener la característica de las filas y no la de las columnas, es decir, en que $p<0.5$: hemos de usar `alternative=less`. 


```{r}
binom.test(2, 12, alternative="less", p=0.5)
```

Obtenemos evidencia significativa de que, efectivamente, el test casero da positivo con mayor frecuencia que el de referencia. 


## Cálculo de la potencia de un contraste 

Recordemos que la **potencia** de un contraste de hipótesis es la probabilidad de no cometer un **error de tipo II**, es decir, la probabilidad de aceptar la hipótesis alternativa  si es verdadera. Usualmente,  la probabilidad de cometer un error de tipo II se denota por $\beta$, y por lo tanto la potencia es $1-\beta$.  

La potencia de un contraste está relacionada con lo que se llama la **magnitud del efecto** (*effect size*). En un contraste, el **efecto** es la diferencia entre el valor estimado del parámetro a partir de la muestra usada y el valor que se da a dicho parámetro como hipótesis nula: por ejemplo, en el contraste de una media, la diferencia entre la media muestral $\overline{x}$ y el valor contrastado $\mu_0$; o, en el contraste de dos medias, la diferencia entre las dos medias muestrales. Se rechaza entonces la hipótesis nula si el efecto observado es tan grande  que es muy improbable cuando la hipótesis nula es verdadera. Pero recordad que, en realidad, no se tiene en cuenta si el efecto observado ha sido grande o no por si mismo, solo si es estadísticamente significativo, es decir, si es improbable cuando  la hipótesis nula es verdadera. Entonces, sin entrar en detalle, digamos que la **magnitud del efecto** es una medida estadística específica del tamaño del efecto observado respecto de su valor esperado si la hipótesis nula es verdadera. La fórmula para calcular la magnitud del efecto depende del contraste y del estadístico usado. 

Para cada tipo de test se han consensuado  unos valores de la magnitud del efecto considerados como "pequeño", "medio" y "grande". Estos valores se obtienen con R con la función `cohen.ES` del paquete **pwr**. Su sintaxis básica es
```{r,eval=FALSE}
cohen.ES(test=..., size=...)
```
donde:

* el parámetro `test` sirve para indicar el tipo de test: por ejemplo, `test="t"` para un test t usando `t.test`, o `test="p"` para un test aproximado de proporciones usando `prop.test`;

* el parámetro `size` sirve para indicar la magnitud esperada:  `"small"`, `"medium"` o  `"large"`.

A modo de ejemplo, la siguiente instrucción nos da la magnitud de efecto que se considera pequeña  en un test t:

```{r}
library(pwr)
cohen.ES(test="t",size="small")
```
De manera similar, para saber la magnitud de efecto que se considera media en un  test aproximado de proporciones podemos usar instrucción siguiente:

```{r}
cohen.ES(test="p",size="medium")
```

Si se desea solo el valor de la magnitud del efecto, para poderlo entrar en otras funciones, se obtiene con el sufijo `$effect.size`:

```{r}
cohen.ES(test="p",size="medium")$effect.size
```



Así pues, en un contraste de hipótesis intervienen cuatro cantidades fundamentales:
el **tamaño** de la muestra, $n$; el **nivel de significación**, $\alpha$; la **potencia**, $1-\beta$; y la **magnitud del efecto**.
El tamaño de la muestra y el nivel de significación están bajo el control del investigador; sin embargo, la potencia del contraste y la magnitud del efecto afectan al contraste de forma más indirecta y su control escapa al investigador. Por ejemplo, si incrementamos el tamaño de la muestra, la potencia aumenta, pero el aumento preciso depende de la magnitud del efecto esperada.
De hecho, las cuatro cantidades anteriores no son independientes, sino que, a partir de tres cualesquiera de ellas,  se puede calcular la cuarta. 
Las funciones del paquete **pwr** permiten realizar estos cálculos  para los contrastes de medias y proporciones.
 
Las funciones de dicho paquete que por ahora nos interesan en este sentido son las siguientes:

*  `pwr.t.test`, para utilizar en tests t  de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras   independientes del mismo tamaño.

*  `pwr.t2n.test`, para utilizar en tests t  de dos medias  usando muestras independientes de distinto tamaño.

*  `pwr.p.test`, para utilizar en contrastes aproximados de una proporción. 

*  `pwr.2p.test`, para utilizar en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño.

*  `pwr.2p2n.test`, para utilizar en contrastes aproximados de dos proporciones  usando  muestras de distinto tamaño.


Estas funciones  tienen los parámetros básicos siguientes:

*  `n`: el tamaño de la muestra (o de las muestras cuando son del mismo tamaño).

*  `n1` y `n2`: los tamaños de las dos muestras en `pwr.2p2n.test` y `pwr.t2n.test`.

*  `d` (en las dos primeras) o `h` (en las tres últimas): la magnitud del efecto.

*  `sig.level`:  el nivel de significación.

*  `power`:  la potencia.

*  `type` (en la primera): el tipo de muestras usado, siendo sus posibles valores  `"one.sample"` (para contrastes de una muestra), `"two.sample"` (para contrastes de dos muestras independientes), o `"paired"` (para contrastes de dos muestras emparejadas).

*  `alternative`: el tipo de hipótesis alternativa, con sus valores usuales.

Si, en una cualquiera de estas funciones se especifican todos los parámetros `n` (o  `n1` y `n2`), `d` (o `h`), `sig.level` y `power` menos uno, la función da el valor del  parámetro que falta.

Veamos algunos ejemplos de uso.





```{example, label="norm1bispot"}
Queremos calcular la potencia del contraste llevado a cabo en el Ejemplo  \@ref(exm:norm1bis). Se trataba de un contraste bilateral de una media usando un test t, por lo que utilizaremos la función  `pwr.t.test`. Los parámetros que le entraremos son:

```  

*  `n`, el tamaño de la muestra; en este ejemplo, $n=25$.

* `d`, la magnitud del efecto.  Para tests t de una media e hipótesis nula $H_0: \mu = \mu_0$, la magnitud del efecto se calcula con la fórmula
$$
d=\frac{|\overline{x}-\mu_0|}{\widetilde{s}_x}.
$$
En nuestro ejemplo, $d=\frac{|2.8048-2|}{0.68064}= 1.1824$.

*  `sig.level`,  el nivel de significación; en este ejemplo, $\alpha=0.05$.

Además como es un contraste bliateral de una media, especificaremos `type="one.sample"` y `alternative="two.sided"` (esto último en realidad no hace falta: como siempre, este es su valor por defecto).
```{r}
x=c(2.2,2.66,2.74,3.41,2.46,2.96,3.34,2.16,2.46,2.71,2.04,3.74,3.24,
  3.92,2.38,2.82,2.2,2.42,2.82,2.84,4.22,3.64,1.77,3.44,1.53)
mag.ef=abs(mean(x)-2)/sd(x) #Magnitud del efecto
pwr.t.test(n=25, d=mag.ef, sig.level=0.05, type="one.sample", alternative="two.sided")
```
Obtenemos que la potencia del test es prácticamente 1.

Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño mínimo de una muestra para tener un nivel de significación del 5% y potencia del 99%, suponiendo *a priori* que la magnitud del efecto esperado va a ser grande (y que por lo tanto detectar que la hipótesis alternativa es verdadera va a ser fácil),  primero calcularíamos cuánto vale una magnitud del efecto grande:

```{r}
cohen.ES(test="t",size="large")$effect.size
```

y a continuación la usaríamos en la función `pwr.t.test`:

```{r}
pwr.t.test(d=0.8, sig.level=0.05, power=0.99, type="one.sample")
```
Bastarían `r ceiling(pwr.t.test(d=0.8, sig.level=0.05, power=0.99, type="one.sample")$n)` observaciones para tener la potencia deseada. Si en cambio esperáramos una magnitud del efecto pequeña:

```{r}
pwr.t.test(d=cohen.ES(test="t",size="small")$effect.size,
           sig.level=0.05, power=0.99, type="one.sample")
```

En este caso necesitaríamos `r ceiling(pwr.t.test(d=cohen.ES(test="t",size="small")$effect.size,sig.level=0.05, power=0.99, type="one.sample")$n)` observaciones. 

Podemos obtener solo una de las componentes del resultado de una de estas funciones añadiéndole el sufijo adecuado. Por ejemplo, la potencia se obtiene con el sufijo `$power` y  el valor de $n$ con el sufijo `$n`:

```{r}
pwr.t.test(n=25, d=mag.ef, sig.level=0.05, type="one.sample", alternative="two.sided")$power
pwr.t.test(d=0.8, sig.level=0.05, power=0.99, type="one.sample")$n
```



```{example,label="trampas1bis"}
Vamos a  calcular la potencia del contraste


```

$$
\left\{
\begin{array}{l} 
 H_0:p_v=p_n\\
 H_1:p_v<p_n
 \end{array}
 \right.
$$
 
del Ejemplo \@ref(exm:trampas1). En este caso, usamos la función `pwr.2p.test`, ya que usamos dos muestras del mismo tamaño, y le entramos los parámetros siguientes:


*  `n`, el tamaño de las muestras; en este ejemplo, $n=60$.

*  `h`, la magnitud del efecto. Para calcularla,^[Por si a alguien le interesa, la fórmula para esta magnitud del efecto es
$$
h=2\left(\arcsin\big(\sqrt{\widehat{p}_1}\,\big)-\arcsin\big(\sqrt{\widehat{p}_2}\,\big)\right),
$$
siendo $\widehat{p}_1$ y $\widehat{p}_2$ las proporciones muestrales de éxitos de las dos muestras.] usamos la función `ES.h` del mismo paquete **pwr** y que se aplica a las proporciones muestrales de éxitos: en este ejemplo,  $\widehat{p}_v=0.67$ y $\widehat{p}_n =0.8$ y la magnitud del efecto vale:
```{r}
 ES.h(0.67,0.8)
```

*  `sig.level`, el nivel de significación, 0.05.

Como solo nos interesa la potencia, añadiremos al `pwr.2p.test` el sufijo `$power`:

```{r}
pwr.2p.test(h=ES.h(0.67,0.8), n=60, sig.level=0.05,alternative="less")$power
```
Hemos obtenido una potencia de, aproximadamente, un `r round(100*pwr.2p.test(h=ES.h(0.67,0.8), n=60, sig.level=0.05,alternative="less")$power)`%.

Si estuviéramos diseñando el experimento y quisiéramos calcular el tamaño de las muestras necesario para tener una potencia del 90%  al nivel de significación del 5% y esperando una magnitud del efecto pequeña (porque esperamos una mejora con las nuevas trampas, pero solo pequeña), entraríamos:

```{r}
cohen.ES(test="p",size="small")$effect.size
pwr.2p.test(h=-0.2, sig.level=0.05, power=0.9,alternative="less")$n
```
Tendríamos que usar dos muestras de `r ceiling(pwr.2p.test(h=-0.2, sig.level=0.05, power=0.9, alternative="less")$n)` cucarachas cada una. Observad que en `pwr.2p.test` hemos entrado en `h` la magnitud del efecto en negativo: esto es debido a que usamos `alternative="less"` y por lo tanto esperamos que la primera proporción sea menor que la segunda.


```{example,label="fumarbis"}
En el contraste

```
$$
\left\{\begin{array}{l}
H_{0}:\mu_n=\mu_f\\
H_{1}:\mu_n> \mu_f
\end{array}\right.
$$
del Ejemplo \@ref(exm:fumar),  ¿qué tamaño de la muestra de mujeres fumadoras tendríamos que tomar si usáramos una muestra de 100 no fumadoras, quisiéramos una potencia del 90% y un nivel de significación del 5% y esperáramos una magnitud del efecto media?


Como es un contraste de dos medias independientes y los tamaños de las muestras pueden ser diferentes, usaremos la función `pwr.t2n.test`. Entraremos como `n1` el tamaño de la muestra de fumadoras y le pediremos que nos dé solo el valor de `n2`, el tamaño de la "otra" muestra:

```{r}
pwr.t2n.test(n1=100, d=cohen.ES(test="t",size="medium")$effect.size,  
             sig.level=0.05, power=0.9, alternative="greater")$n2
```
Bastaría estudiar `r ceiling(pwr.t2n.test(n1=100, d=0.5,  sig.level=0.05, power=0.9, alternative="greater")$n2)` madres fumadoras.



## Guía rápida

Excepto en las que decimos lo contrario, todas las funciones para realizar contrastes que damos a continuación admiten los parámetros `alternative`, que sirve para especificar el tipo de contraste (unilateral en un sentido u otro o bilateral), y  `conf.level`, que sirve para indicar el nivel de confianza $1-\alpha$.  Sus valores por defecto son contraste bilateral y nivel de confianza 0.95.

*  `t.test` realiza tests t para contrastar una o dos medias (tanto usando muestras independientes como emparejadas). Aparte de `alternative` y  `conf.level`, sus parámetros principales son:
   
     *  `mu` para especificar el valor de la media que queremos contrastar en un test de una media.

     *  `paired` para indicar si en un contraste de dos medias usamos muestras independientes o emparejadas.

     *  `var.equal` para indicar en un contraste de dos medias usando muestras independientes si las varianzas poblacionales son iguales o diferentes.


*  `SIGN.test` del paquete **BSDA**,  realiza un test de signos para contrastar una mediana. Dispone del parámetro  `md`  para entrar la mediana a contrastar.

*  `wilcox.test`, para realizar tests de Wilcoxon y de Mann-Whitney para contrastar una o dos  medianas (tanto usando muestras independientes como emparejadas). Sus parámetros son los mismos que los de `t.test` (salvo `var.equal`, que en estos tests no tiene sentido).

*  `sigma.test`, para realizar tests $\chi^2$ para contrastar una varianza (o una desviación típica). Dispone de los parámetros `sigma` y `sigmasq` para indicar, respectivamente, la desviación típica o la varianza a contrastar.

*  `var.test`, para realizar tests F para contrastar dos varianzas (o dos desviaciones típicas). 

*  `fligner.test`, para realizar tests no paramétricos de Fligner-Killeen para contrastar dos varianzas (o dos desviaciones típicas). No dispone de los parámetros `alternative` (solo sirve para contrastes bilaterales) ni `conf.level` (no calcula intervalos de confianza).


*  `binom.test`, para realizar tests binomiales exactos para contrastar una proporción. Dispone del parámetro  `p` para indicar la proporción a contrastar.

*  `prop.test`, para realizar tests aproximados para contrastar  una proporción o dos proporciones de poblaciones usando muestras independientes.  También dispone del parámetro  `p` para indicar la proporción a contrastar en un contraste de una proporción.

*  `fisher.test`, para realizar tests exactos de Fisher para contrastar dos proporciones usando muestras independientes. 

*  `mcnemar.test`, para realizar tests bilaterales de McNemar para contrastar dos proporciones usando muestras emparejadas. No dispone de los parámetros `alternative`  ni `conf.level`.

* `cohen.ES` del paquete **pwr**, da los valores aceptados por convenio como "pequeño", "mediano" y "grande" para diferentes tests.

*  `pwr.t.test` del paquete **pwr**,  relaciona el tamaño de la(s) muestra(s), el nivel de significación, la potencia y la magnitud del efecto (en el sentido de que si se entran tres de estos valores se obtiene el cuarto) en tests t  de una media, de dos medias usando muestras emparejadas o de dos medias usando muestras  independientes del mismo tamaño. Sus parámetros, son
     * `n`: el tamaño de la muestra o de las muestras.
     *  `sig.level`:  el nivel de significación.
     * `power`:  la potencia.
     * `d`: la magnitud del efecto
     *  `type`: el tipo de muestras (una muestra, dos muestras emparejadas, dos muestras independientes).
     *  `alternative`: el tipo de hipótesis alternativa.

     

*  `pwr.t2n.test`del paquete **pwr**,  relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en tests t   de dos medias  usando muestras independientes de distinto tamaño. Sus parámetros  son
     * `n1` y `n2`: los tamaños de las dos muestras.
     *  `sig.level`,   `power`,  `d` y `alternative` como en `pwr.t.test`.


*  `pwr.p.test` del paquete **pwr**,  relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de una proporción. Sus parámetros  son
    * `n`,  `sig.level`, `power` y `alternative` como en  `pwr.t.test`.
    * `h`: la magnitud del efecto


*  `pwr.2p.test` del paquete **pwr**,  relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en contrastes aproximados de dos proporciones usando muestras independientes del mismo tamaño. Sus parámetros  son los mismos que los de  `pwr.p.test`.

*  `pwr.2p2n.test`, del paquete **pwr**,  relaciona los tamaños de muestras, el nivel de significación, la potencia y la magnitud del efecto en  contrastes aproximados de dos proporciones  usando  muestras de distinto tamaño. Sus parámetros  son
     * `n1` y `n2`: los tamaños de las dos muestras.
     * `sig.level`, `power`, `h` y `alternative` como en  `pwr.p.test`.



## Ejercicios

### Modelo de test {-}

*(1)*  Tenemos una m.a.s. de una población normal $X\sim N(\mu,\sigma)$ formada por los números 2,5,3,5,6,6,7,2. Usando la función `t.test`, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste $H_0: \mu=4$ contra $H_1:\mu \neq 4$ y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco.


*(2)*  Tenemos dos muestras de poblaciones normales, $X_1\sim N(\mu_1,\sigma_1)$ y $X_2\sim N(\mu_2,\sigma_2)$. Sean $x_1=(2,5,3,5,6,6,7,2)$ y $x_2=(3,2,5,4,2,2,4,5,1,6,2)$ muestras aleatorias simples de $X_1$ y $X_2$, respectivamente. Usando la función `t.test`, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste $H_0: \mu_1=\mu_2$ contra $H_1:\mu_1>\mu_2$ suponiendo que las varianzas son diferentes y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.1. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco.

*(3)*   Tenemos dos muestras de poblaciones normales, $X_1\sim N(\mu_1,\sigma_1)$ y $X_2\sim N(\mu_2,\sigma_2)$. Sean $x_1=(2,5,3,5,6,6,7,2)$ y $x_2=(3,2,10,9,2,2,4,5,1,10,2)$ muestras aleatorias simples de $X_1$ y $X_2$, respectivamente. Usando la función `var.test`, calculad los extremos inferior y superior de un intervalo de confianza del 95% para $\sigma_1^2/\sigma_2^2$ (redondeados a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, sin acento, o NO) si en el contraste $H_0: \sigma_1=\sigma_2$ contra $H_1:\sigma_1 \neq \sigma_2$ podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las tres respuestas en este orden, separadas por un único espacio en blanco.

*(4)*  Tenemos dos variables aleatorias de Bernoulli de proporciones poblacionales $p_1$ y $p_2$, respectivamente. En una muestra de 100 observaciones de la primera hemos obtenido 20 éxitos, y en una muestra de 150 observaciones de la segunda, hemos obtenido 40 éxitos. Usando la función `prop.test`, calculad el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del contraste $H_0: p_1=p_2$ contra $H_1:p_1<p_2$ y decid (contestando SI, sin acento, o NO) si podemos rechazar la hipótesis nula en favor de la alternativa a un nivel de significación de 0.05. Tenéis que dar las dos respuestas en este orden, separadas por un único espacio en blanco.

<!--
### Ejercicios {-}

**(1)** Para satisfacer las necesidades respiratorias de los peces de agua caliente, el contenido de oxígeno disuelto debe presentar un promedio de
6.5 partes por millón (ppm), con una desviación típica no mayor de 1.2 ppm.
Cuando la temperatura del agua crece, el oxígeno disuelto disminuye, y esto causa la asfixia del pez. 

Se realizó un estudio sobre los efectos del calor en verano en el contenido de oxígeno disuelto en un gran lago. Después de un período particularmente caluroso, se tomaron muestras de agua en $35$ lugares aleatoriamente seleccionados
en el lago, y se determinó el contenido de oxígeno disuelto. Los resultados (en ppm) fueron los siguientes:
```{r}
O2=c(9.1,6.8,7.0,7.5,8.7,3.2,5.4,8.1,4.4,5.1,6.2,6.9,6.9,4.3,
    8.0,5.3,6.2,6.4,7.8,5.8,6.9,7.7,5.2,5.8,6.3,5.9,8.5,7.5,8.9,
    5.6,6.6,5.3,5.7,6.9,6.6)
```
Suponemos que estos contenidos de oxígeno siguen una distribución normal.

*(a)*  ¿Hay evidencia de que el contenido medio de oxígeno en el lago sea inferior al nivel aceptable de $6.5$ ppm?
*(b)*  ¿Hay evidencia de que la desviación típica del contenido  de oxígeno en el lago sea superior a $1.2$ ppm?

**(2)** Los angiogramas son la técnica estándar para diagnosticar un ictus, pero tienen un ligero riesgo de mortalidad (inferior al 1%). Algunos investigadores han propuesto usar una prueba PET para diagnosticar el ictus de manera no invasiva. Sobre 64 pacientes ingresados en urgencias con síntomas de ictus se usaron ambas técnicas de diagnóstico. Los resultados obtenidos se resumen en la tabla siguiente:
$$
\begin{array}{r|c}
 & \quad\mbox{Angiograma}\quad \\
\mbox{PET} &\quad \mbox{Positivo} \qquad \mbox{Negativo}\quad \\\hline
 \mbox{Positivo} &\quad 32  \qquad\qquad\ \ 8\quad \\
 \mbox{Negativo} &\quad\quad 3   \qquad\qquad\ \   21\quad\ 
\end{array}
$$

Contrastad si ambas técnicas de diagnóstico tienen la misma probabilidad de dar positivo.
-->



### Respuestas al test {-}

```{r, include=FALSE}
##1
x=c(2,5,3,5,6,6,7,2)
Res1=round(t.test(x,mu=4,alternative="two.sided")$p.value,3)
##2
x1=c(2,5,3,5,6,6,7,2)
x2=c(3,2,5,4,2,2,4,5,1,6,2)
Res2=round(t.test(x1,x2,conf.level=0.9,
                 alternative="greater")$p.value,3)
##3
x1=c(2,5,3,5,6,6,7,2)
x2=c(3,2,10,9,2,2,4,5,1,10,2)
Res3=round(var.test(x1,x2)$conf.int,3)
#4
Res4=round(prop.test(c(20,40),c(100,150),alternative="less")$p.value,3)
```


*(1)*   `r Res1` NO

Nosotros lo hemos resuelto con
```{r}
x=c(2,5,3,5,6,6,7,2)
round(t.test(x,mu=4)$p.value,3)
```

*(2)*  `r Res2` SI

Nosotros lo hemos resuelto con
```{r}
x1=c(2,5,3,5,6,6,7,2)
x2=c(3,2,5,4,2,2,4,5,1,6,2)
round(t.test(x1,x2,alternative="greater")$p.value,3)
```


*(3)*   `r Res3[1]` `r Res3[2]` NO

Nosotros lo hemos resuelto con
```{r}
x1=c(2,5,3,5,6,6,7,2)
x2=c(3,2,10,9,2,2,4,5,1,10,2)
round(var.test(x1,x2)$conf.int,3)
```


*(4)*  `r Res4` NO

Nosotros lo hemos resuelto con
```{r}
round(prop.test(c(20,40),c(100,150),alternative="less")$p.value,3)
```



<!--
### Respuestas sucintas a los ejercicios {-}

**(1)** *(a)* No:
```{r}
O2=c(9.1,6.8,7.0,7.5,8.7,3.2,5.4,8.1,4.4,5.1,6.2,6.9,6.9,4.3,
    8.0,5.3,6.2,6.4,7.8,5.8,6.9,7.7,5.2,5.8,6.3,5.9,8.5,7.5,8.9,
    5.6,6.6,5.3,5.7,6.9,6.6)
t.test(O2,mu=6.5,alternative="less")
```

*(b)* Tampoco:
```{r}
library(TeachingDemos)
sigma.test(O2, sigma=1.2, alternative="greater")
```

**(2)** Nop:
```{r}
binom.test(3,11, p=0.5)
```
-->

<!--chapter:end:05-ContrastesI.Rmd-->

# Contrastes de bondad de ajuste {#chap:bondad}

Una de las condiciones  habituales que requerimos sobre una muestra, por ejemplo, al razonar sobre la distribución  de sus estadísticos o al realizar contrastes de hipótesis, es que la población de la que la hemos extraído siga una determinada distribución. En la Lección \@ref(chap:agrup) de la primera parte del curso comprobábamos gráficamente el ajuste de una muestra a una distribución normal mediante  histogramas y dibujando las curvas de densidad muestral y de densidad de la normal. En esta lección presentamos algunas instrucciones que implementan contrastes de **bondad de ajuste** (*goodness of fit*), técnicas cuantitativas que permiten decidir si los datos de una muestra "se ajustan" a una determinada distribución de probabilidad, es decir, si la variable aleatoria que los ha generado sigue o no esta distribución de probabilidad.

Los contrastes de bondad de ajuste tienen el mismo significado que los explicados en la Lección \@ref(chap:contrastes). Se contrasta una hipótesis nula 

* $H_0$:  La variable aleatoria poblacional tiene distribución $X$
     
contra la hipótesis alternativa 

* $H_1$:  La variable aleatoria poblacional *no* tiene distribución $X$
      
Como siempre, para llevar a cabo el contraste tomamos una muestra aleatoria de la población. Entonces, obtenemos evidencia significativa de que la población no tiene distribución $X$ cuando es muy raro obtener nuestra muestra si la población tiene esta distribución. En este caso,  rechazamos la hipótesis nula en favor de la alternativa. Si, en cambio, no es del todo inverosímil que la muestra se haya generado con la distribución $X$, aceptamos la hipótesis nula "por defecto" y concluimos que la población sí que tiene esta distribución.  Pero que aceptemos la hipótesis nula no nos da evidencia de que la población tenga distribución $X$: simplemente nos dice que no encontramos motivos para rechazarlo. Naturalmente, a efectos prácticos, si aceptamos la hipótesis nula de que la población tiene la distribución $X$, actuaremos como si creyéramos que efectivamente esta hipótesis es verdadera, por ejemplo a la hora de decidir que fórmulas usar para calcular un intervalo de confianza o para efectuar un contraste de hipótesis. Pero no estaremos seguros de que podamos emplear estas fórmulas sobre nuestra muestra, simplemente no tendremos motivos para dejar de creerlo.

Los pasos habituales para contrastar la bondad del ajuste de una muestra a una distribución son los siguientes:

1. Fijar la familia de distribuciones teóricas a la que queremos ajustar los
datos. Esta familia estará parametrizada por uno o varios parámetros. Recordemos los ejemplos más comunes:

    * Si la familia es la Bernoulli, el parámetro es $p$: la probabilidad poblacional de éxito.

    * Si la familia es la Poisson, el parámetro es $\lambda$: la esperanza.

    * Si la familia es la binomial, los parámetros son $n$  y $p$: el tamaño de las muestras y la probabilidad de éxito, respectivamente.

    * Si la familia es la normal, los parámetros son $\mu$  y $\sigma$: la esperanza  y la desviación típica, respectivamente.

    * Si la familia es la $\chi^2$, el parámetro es el número de grados de libertad.

    * Si la familia es la t de Student, el parámetro es de nuevo  el número de grados de libertad.

    * Otras familias de distribuciones tienen parámetros de **localización** (*location*), **escala** (*scale*) o **forma** (*shape*), por lo  que no nos ha de extrañar si R nos pide que asignemos parámetros con estos nombres. 


2.  Si el diseño del experimento no fija sus valores, tendremos que estimar  a partir de la muestra los valores de los parámetros que mejor se ajusten a nuestros datos.  Ya hemos tratado la estimación de parámetros en la Lección \@ref(chap:estimacion).

3. Determinar qué tipo de contraste vamos a utilizar. En esta lección veremos dos tipos básicos de contrastes generales:

    * El **test $\chi^2$ de Pearson**. Este test es válido tanto para variables discretas como para continuas, pero solo se puede aplicar a conjuntos grandes de datos (por fijar una cota concreta, de 30 o más elementos). Además, si el **espacio muestral**, es decir, el conjunto de resultados posibles, es infinito, es necesario agrupar estos resultados en un número finito de clases.

    * El **test de Kolgomorov-Smirnov**. Este test solo es válido para variables continuas, y compara la función de distribución acumulada muestral con la teórica. No requiere que la muestra sea grande, pero en cambio, en principio, no admite que los datos de la muestra se puedan repetir.^[A las repeticiones se las suele llamar empates, *ties* en inglés, porque la función   de distribución acumulada muestral ordena los datos y las repeticiones producen empates en las posiciones de valores sucesivos.] Por desgracia, las repeticiones suelen ser habituales si la muestra es grande y la precisión de los datos es baja o la variabilidad de la población muestreada es pequeña.

    * Aparte, determinados tipos de  distribuciones tienen sus contrastes de bondad de ajustes específicos. Este es el caso especialmente de la normal, para la que explicaremos algunos tests que permiten contrastar si una muestra proviene de *alguna* distribución normal.

4.  Realizar el contraste y redactar las conclusiones. Es conveniente apoyar los
resultados del contraste con gráficos. En esta lección explicaremos los **gráficos cuantil-cuantil**, o **Q-Q-plots**, que  sirven para visualizar el ajuste de unos datos a una distribución conocida y son una buena alternativa a los histogramas con curvas de densidad.


## Pruebas gráficas: Q-Q-plots

Para comparar la distribución de una muestra con una distribución  poblacional teórica se pueden realizar diversas pruebas gráficas. En la Lección  \@ref(chap:agrup) de la primera parte del curso usábamos para ello histogramas con densidades estimadas y teóricas. En esta sección explicamos otro tipo de gráficos que pueden usarse con el mismo fin, los **gráficos cuantil-cuantil**, o, para abreviar, **Q-Q-plots**. Estos gráficos comparan  los cuantiles observados de la muestra con los cuantiles teóricos de la distribución teórica. 


```{r,include=FALSE}
library(car)
```


```{r, echo=FALSE, label=qqp1,fig.cap="Q-Q-plot básico de la muestra del Ejemplo \\@ref(exm:qqp-1) contra una t de Student con 4 grados de libertad."}
muestra=c(0.27,0.81,-0.73,-0.96,1.33,0.91,-1.70,0.24,-0.19,
  0.29,1.41,0.13,-0.06,-0.85,-0.59,-3.62,-1.02,2.36,0.34,-0.31,
  0.81,-0.88,0.27,0.52,1.05,0.20,0.76,0.25,-1.43,3.71,-0.78,
  0.39,-1.01,1.53,-0.72,1.22,0.56,-1.17,-0.65,-0.33,-0.07,0.31,
  -0.74,0.36,-1.72,-1.21,-0.05,-1.17,0.28,1.30,0.89,1.45,0.13,
  -1.12,3.13,-1.21,-0.90,-0.31,-1.05,0.89,-1.06,0.21,-0.50,
  -0.36,-0.29,-0.19,-1.71,0.09,0.21,0.55,-1.42,0.19,-0.62,2.46,
  -0.17,-0.63,0.77,0.94,0.55,0.35,-4.47,1.71,0.07,-0.57,-1.43,
  -0.85,1.06,0.82,0.19,-1.08,0.30,-0.87,0.77,1.23,-0.04,0.66,
  -0.87,-0.86,-1.06,0.10)
qqPlot(muestra, distribution="t", df=4, envelope=FALSE,
  xlab="Cuantiles de t", ylab="Cuantiles de la muestra",
  line="none", pch=20, grid=FALSE, id=FALSE)
abline(0,1, col="red", lwd=1.5)
```


La Figura \@ref(fig:qqp1) muestra un Q-Q-plot.  Cada punto corresponde a un cuantil: *grosso modo*, hay un punto para cada $k/n$-cuantil, siendo $n$ la longitud de la muestra y $k=1,\ldots,n$. Para cada uno de estos cuantiles, el punto  correspondiente tiene  abscisa el  cuantil de la distribución teórica (en este caso, una t de Student con 4 grados de libertad) y ordenada el cuantil de la muestra. Por lo tanto, si el ajuste es bueno, para cada $k/n$, el cuantil muestral y el cuantil teórico han de ser parecidos, de manera que  los puntos del gráfico (les llamaremos **Q-Q-puntos**, para abreviar) han de estar cerca de la diagonal $y=x$, que hemos añadido al gráfico. En general, se considera que un Q-Q-plot muestra un buen ajuste cuando no se observa una tendencia marcada de desviación respecto de la diagonal.  Sin embargo, a menudo los Q-Q-plots son difíciles de interpretar, y es conveniente combinarlos con algún contraste de bondad de ajuste.

Hay varias maneras de producir Q-Q-plots con R. Aquí solo explicaremos una: la función `qqPlot` del paquete **car**. Su sintaxis básica es
```{r, eval=FALSE}
qqPlot(x, distribution=...,  parámetros, id=FALSE, ...)
```
donde:

* `x` es el vector con la muestra.

* El parámetro `distribution` se ha de igualar al nombre de la familia de distribuciones  entre comillas, y puede tomar como valor cualquier familia de distribuciones de la que R sepa calcular la densidad y los cuantiles: esto incluye  las distribuciones que hemos estudiado hasta el momento: `"norm"`, `"binom"`, `"poisson"`, `"t"`, etc.

*  A continuación, se tienen que entrar los parámetros de la distribución, igualando su nombre habitual (`mean` para la media, `sd` para la desviación típica, `df` para los grados de libertad, etc.)  a su valor. En algunos casos, si no se especifican los parámetros, `qqPlot` toma sus valores por defecto: por ejemplo, si queremos realizar un Q-Q-plot contra una normal y no especificamos los valores de la media y la desviación típica de la distribución teórica, `qqPlot` los toma iguales a 0 y 1, respectivamente.

* Por defecto, el gráfico obtenido con la función `qqPlot` identifica los dos Q-Q-puntos con ordenadas más extremas. Para omitirlos, usad el parámetro `id=FALSE`.

Otros parámetros a tener en cuenta:

* `qqPlot` añade por defecto una rejilla al gráfico, que podéis eliminar con `grid=FALSE`.


*  `qqPlot` añade por defecto una línea recta que une los Q-Q-puntos correspondientes al  primer y tercer cuartil: se la llama **recta cuartil-cuartil**. Un buen ajuste de los Q-Q-puntos a esta recta significa que la muestra se ajusta a la distribución teórica, pero posiblemente con parámetros diferentes a los especificados. Os recomendamos mantenerla, pero si queréis eliminarla por ejemplo para substituirla por la diagonal $y=x$, podéis usar el parámetro `line="none"`.

* `qqPlot` también añade dos curvas discontinuas que abrazan una "región de confianza del 95%" para el Q-Q-plot. Sin entrar en detalles, esta región contendría todos los Q-Q-puntos en un 95% de las ocasiones que tomáramos una muestra de la distribución teórica del mismo tamaño que la nuestra. Por lo tanto, si todos los Q-Q-puntos caen dentro de esta franja, no hay evidencia para rechazar que la muestra provenga de la distribución teórica. Esta franja de confianza es muy útil para interpretar el Q-Q-plot, pero la podéis eliminar con `envelope=FALSE`.

* Se pueden usar los parámetros usuales de `plot` para poner nombres a los ejes, título, modificar el estilo de los puntos, etc., y otros parámetros específicos para modificar el aspecto del gráfico.  Por ejemplo, `col.lines` sirve para especificar el color de las líneas  que añade. Consultad la Ayuda de la función.


```{example, label="qqp-1"}
Consideremos la siguiente muestra:


```

```{r}
muestra=c(0.27,0.81,-0.73,-0.96,1.33,0.91,-1.70,0.24,-0.19,0.29,1.41,0.13,-0.06,
  -0.85,-0.59,-3.62,-1.02,2.36,0.34,-0.31,0.81,-0.88,0.27,0.52,1.05,0.20,0.76,0.25,
  -1.43,3.71,-0.78,0.39,-1.01,1.53,-0.72,1.22,0.56,-1.17,-0.65,-0.33,-0.07,0.31,
  -0.74,0.36,-1.72,-1.21,-0.05,-1.17,0.28,1.30,0.89,1.45,0.13,-1.12,3.13,-1.21,
  -0.90,-0.31,-1.05,0.89,-1.06,0.21,-0.50,-0.36,-0.29,-0.19,-1.71,0.09,0.21,0.55,
  -1.42,0.19,-0.62,2.46,-0.17,-0.63,0.77,0.94,0.55,0.35,-4.47,1.71,0.07,-0.57,
  -1.43,-0.85,1.06,0.82,0.19,-1.08,0.30,-0.87,0.77,1.23,-0.04,0.66,-0.87,-0.86,
  -1.06,0.10)
```


Queremos comprobar gráficamente si sigue una distribución t de Student de 4 grados de libertad. Vamos a usar la función `qqPlot` con sus parámetros por defecto:
```{r,fig.cap="Q-Q-plot de la muestra del Ejemplo \\@ref(exm:qqp-1) contra una t de Student con 4 grados de libertad producido por defecto con qqPlot."}
library(car)
qqPlot(muestra, distribution="t", df=4, id=FALSE)
```

Como todos los Q-Q-puntos están dentro de la región de confianza del 95%, podemos aceptar que la muestra proviene de una t de Student.


El Q-Q-plot básico de la Figura \@ref(fig:qqp1) se ha obtenido con el código siguiente:

```{r, eval=FALSE}
qqPlot(muestra, distribution="t", df=4, envelope=FALSE, xlab="Cuantiles de t",
   ylab="Cuantiles de la muestra", line="none", pch=20, grid=FALSE, id=FALSE)
abline(0,1, col="red", lwd=1.5)
```



Veamos otro ejemplo.

```{example, label=irisqq}
Consideremos el *data frame* `iris` que contiene información sobre medidas relacionadas con las flores de una muestra de iris de tres especies. Vamos a producir un Q-Q-plot que ilustre si las longitudes de los sépalos de las plantas iris recogidas en esta tabla de datos  siguen una distribución normal. A un Q-Q-plot que compara una muestra con una distribución normal se le suele llamar, para abreviar, un **normal-plot**. 

```

En primer lugar, estimamos los parámetros máximo verosímiles de la distribución normal que podría haber generado nuestra muestra:

```{r}
library(MASS)
iris.sl=iris$Sepal.Length
mu=fitdistr(iris.sl,"normal")$estimate[1]
sigma=fitdistr(iris.sl,"normal")$estimate[2]
round(c(mu,sigma),3)
```

y ahora generamos el Q-Q-plot usando estos parámetros

```{r,fig.cap="Normal-plot de longitudes de sépalos de flores iris.",label=qqplotiris}
qqPlot(iris.sl, distribution="norm", mean=mu, sd=sigma,xlab="Cuantiles de la normal",id=FALSE,
   ylab="Cuantiles de la muestra",main="")
```

Vemos cómo los primeros puntos salen de la región de confianza del 95%. Esto significa que los datos están más desplazados hacia la izquierda de la media que lo que se esperaría en una muestra aleatoria de una normal. El *boxplot* de la Figura \@ref(fig:bplotiris) muestra este desplazamiento. 

```{r,fig.cap="Boxplot de longitudes de sépalos de flores iris.",label=bplotiris}
boxplot(iris.sl)
```


Interpretamos el Q-Q-plot anterior como evidencia de que estas longitudes no siguen una distribución normal. Más adelante usaremos tests de normalidad específicos para contrastar la normalidad de estos datos.

## El test $\chi^2$ de Pearson

El test $\chi^2$ de Pearson contrasta si una muestra ha sido generada o no con una cierta distribución, cuantificando si sus valores aparecen con una frecuencia cercana a la que sería de esperar si la muestra  siguiera esa distribución. Esto se lleva a cabo por medio del estadístico de contraste

$$
X^2=\sum_{i=1}^k\frac{(\mbox{frec. observada}_i-\mbox{frec. esperada}_i)^2}{\mbox{frec. esperada}_i}
$$
donde *k* es el número de clases e *i* es el índice de las clases, de manera que "frec. observada~*i*~" y "frec. esperada~*i*~" denotan, respectivamente, la frecuencia observada  de la clase *i*-ésima y  su frecuencia esperada bajo la distribución que contrastamos. Si se satisfacen una serie de condiciones, este estadístico sigue aproximadamente una ley $\chi^2$ con un número de grados de libertad igual al número de clases menos uno y menos el número de parámetros de la distribución teórica que hayamos estimado. Las condiciones que se han de satisfacer son: 

* La muestra ha de ser grande, digamos que de tamaño como mínimo 30; 

*  Si los posibles valores son infinitos, hay que agruparlos en un número finito *k* de clases que cubran todos los posibles valores (recordad que en la Lección \@ref(chap:agrup) de la primera parte del curso ya explicamos cómo agrupar variables aleatorias continuas con la función `cut`); 

* Las frecuencias esperadas de las clases en las que hemos agrupado el espacio muestral han de ser todas, o al menos una gran mayoría, mayores o iguales que 5.


La instrucción básica en R para realizar un test $\chi^2$ es `chisq.test`. Su sintaxis básica es
```{r,eval=FALSE}
chisq.test(x, p=..., rescale.p=..., simulate.p.value=...)
```
donde:

* `x` es el vector (o la tabla, calculada con `table`) de frecuencias absolutas observadas de las clases en la muestra. 


* `p` es el vector de probabilidades teóricas de las clases para la distribución que queremos contrastar. Si no lo especificamos, se entiende que la probabilidad es la misma para todas las clases. Obviamente, estas probabilidades se tienen que especificar en el mismo orden que las frecuencias de `x` y, como son las probabilidades de todos los resultados posibles, en principio tienen que sumar 1; esta condición se puede relajar con el siguiente parámetro.

* `rescale.p` es un parámetro lógico que, si se iguala a `TRUE`, indica que  los valores de `p` no son probabilidades, sino solo proporcionales a las probabilidades; esto hace que R tome como probabilidades teóricas los valores de `p`  partidos por su suma, para que sumen 1.  Por defecto vale `FALSE`, es decir, se supone que el vector que se entra como  `p`  son probabilidades y por lo tanto debe sumar 1, y si esto no pasa se genera un mensaje de error indicándolo. Igualarlo a `TRUE` puede ser útil, porque nos permite especificar las probabilidades mediante  las frecuencias esperadas o mediante porcentajes. Pero también es peligroso, porque si nos hemos equivocado y hemos entrado un vector en `p` que no corresponda a una probabilidad, R no nos avisará.

*  `simulate.p.value` es un  parámetro lógico que indica a la función si debe
optar por una simulación para el cálculo  del p-valor del contraste. Por
defecto vale `FALSE`, en cuyo caso este p-valor no se simula sino que se calcula mediante la distribución $\chi^2$ correspondiente.  Si se especifica como `TRUE`, R realiza una serie  de replicaciones aleatorias de la situación teórica: por defecto, 2000, pero su número se puede especificar mediante el parámetro `B`. Es decir,  genera un conjunto de vectores aleatorios de frecuencias con la distribución que queremos contrastar,  cada uno de suma total la de `x`.  A continuación, calcula la proporción de estas repeticiones en las que el estadístico de contraste es  mayor o igual que el obtenido para `x`, y éste es el  p-valor que da. Cuando no se satisfacen las condiciones para que $X^2$ siga aproximadamente una distribución $\chi^2$, estimar el p-valor mediante simulaciones es  una buena alternativa. 


Veamos un primer ejemplo sencillo.

```{example, label=dado}
Tenemos un dado, y queremos contrastar si está equilibrado o trucado. Lo hemos lanzado  40 veces y hemos obtenido los resultados siguientes:

  
```

```{r,echo=FALSE}
df_temp=data.frame(Resultados=1:6, Frecuencias=c(8,4,6,3,7,12))
knitr::kable(df_temp)
```

Si el dado está equilibrado, la probabilidad de cada resultado es 1/6 y por lo tanto la frecuencia esperada de cada resultado es 40/6=`r round(40/6,3)`. Como la muestra tiene más de 30 elementos y las frecuencias esperadas son todas mayores que 5, podemos realizar de manera segura un test $\chi^2$. Por lo tanto, entraremos estas frecuencias en un vector y le aplicaremos la función `chisq.test`. Como contrastamos si todas las clases tienen la misma probabilidad, no hace falta especificar el valor del parámetro `p`.

```{r}
freqs=c(8,4,6,3,7,12)
chisq.test(freqs)
```

Observemos la estructura del resultado de un `chisq.test`. Nos da el valor del estadístico $X^2$ (`X-squared`), el p-valor del contraste (`p-value`), y los grados de libertad de la distribución $\chi^2$ que ha usado para calcularlo (`df`). En este caso, el p-valor es `r round(chisq.test(freqs)$p.value,3)`, y por lo tanto no podemos rechazar que el dado esté equilibrado. Queremos remarcar que, como R no sabe si hemos estimado o no parámetros, el número de grados de libertad que usa  `chisq.test` es simplemente el número de clases menos 1. Si no hemos estimado parámetros para calcular las probabilidades teóricas, ya va bien, pero si lo hemos hecho y por lo tanto el número de grados de libertad no es el adecuado, tendremos que calcular el p-valor correcto a partir del valor del estadístico. Veremos varios ejemplos más adelante.

El resultado de un `chisq.test` es una `list`, de la que podemos extraer directamente la información que deseemos con los sufijos adecuados. En concreto, podemos obtener el valor del estadístico $X^2$ con el sufijo `$statistic`, los grados de libertad con el sufijo  `$parameter` y el p-valor con el sufijo `$p.value`.
```{r}
chisq.test(freqs)$statistic
chisq.test(freqs)$parameter
chisq.test(freqs)$p.value
```

Imaginemos ahora que, en vez de lanzar el dado 40 veces, lo lanzamos 20 veces, y obtenemos los resultados siguientes:

```{r,echo=FALSE}
df_temp=data.frame(Resultados=1:6, Frecuencias=c(4,2,3,2,3,6))
knitr::kable(df_temp)
```

¿Hay evidencia de que el dado esté trucado? Ahora la muestra no es grande y las frecuencias esperadas son todas 20/6=`r round(20/6,3)`, menores que 5. Por tanto, el p-valor del test $\chi^2$ que se obtiene usando una distribución $\chi^2_5$ no tiene por qué tener ningún significado. En una situación como ésta es cuando conviene usar el parámetro `simulate.p.value`. Vamos a pedir a R que simule 5000 veces el experimento de lanzar 20 veces un dado equilibrado, y que calcule como p-valor la proporción de simulaciones en las que el estadístico $X^2$ haya dado un valor  mayor o igual que el que se obtiene con nuestra muestra. 

```{r, include=FALSE}
set.seed(100)
```


```{r}
freqs2=c(4,2,3,2,3,6)
chisq.test(freqs2, simulate.p.value=TRUE, B=5000)
```

Resulta que en un `r set.seed(100); freqs2=c(4,2,3,2,3,6); round(100*chisq.test(freqs2, simulate.p.value=TRUE, B=5000)$p.value,1)`% de las simulaciones el valor de $X^2$ ha sido mayor o igual que el de nuestra muestra, `r chisq.test(freqs2, simulate.p.value=TRUE, B=5000)$statistic`. Por lo tanto, nuestra muestra entra dentro de lo normal para un dado equilibrado, por lo que no hay evidencia  de que el dado esté trucado.

Como este p-valor se basa en simulaciones, en cada aplicación del test el p-valor puede dar resultados  diferentes, pero en general la conclusión es robusta si se toma un número suficiente de simulaciones.
```{r}
chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value
chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value
chisq.test(freqs2,simulate.p.value=TRUE,B=5000)$p.value
```
Como vemos, los p-valores son todos similares. 

Por curiosidad, ¿qué p-valor da el test $\chi^2$ usando la distribución de $\chi^2_5$?
```{r,warning=TRUE}
chisq.test(freqs2)$p.value
```

El p-valor no es muy diferente y la conclusión en este caso sería la misma, pero fijaos en el mensaje de advertencia: para la muestra dada, la aproximación de la distribución de $X^2$ mediante una $\chi^2$ no tiene por qué ser correcta.



```{example}
Vamos a estudiar las frecuencias de los nucleótidos en una cadena de ADN, y contrastar si aparecen los cuatro con la misma probabilidad o no. En este caso, el espacio muestral son los cuatro nucleótidos: adenina (A), citosina (C), guanina (G) y timina (T). Identificaremos una cadena de ADN con un vector de letras  `a`, `c`, `g` y `t`.  Si llamamos $p_a$, $p_c$, $p_g$ y $p_t$ a las probabilidades de aparición de estas  letras, el contraste que queremos realizar es 


```
 
$$
\left\{\begin{array}{l} 
H_0 : p_a=p_c=p_g=p_t=0.25\\
H_1: \mbox{Algunos nucleótidos son más probables que otros}
\end{array}
\right.
$$
Vamos a analizar una cadena de ADN "de verdad", extraída de la base de datos
*[GenBank](\url{http://www.ncbi.nlm.nih.gov/genbank/})*. Para ello,
utilizaremos el paquete **ape**, que incorpora una función  `read.GenBank` que permite leer secuencias de genes incluidas en esta base de datos y convertirlas en vectores de letras  `a`, `c`, `g` y `t`. En concreto, si la aplicamos al número de acceso (*accession number*) de una secuencia (entrado entre comillas, ya que es una palabra) y usamos el parámetro `as.character=TRUE`, nos  devuelve dicha secuencia como un vector de letras junto con otra información sobre la secuencia. 

En este ejemplo,  nos vamos a interesar  por el gen que codifica la mioglobina humana, que es una proteína relativamente pequeña constituida por una sola cadena polipeptídica de 153 aminoácidos. Su número de acceso  es  AH002877.2. El  código siguiente lee este gen y lo guarda en un objeto.

```{r}
library(ape)
myoglobin=read.GenBank("AH002877.2", as.character=TRUE)
```

Consultemos cómo es el objeto donde hemos guardado esta secuencia:

```{r}
str(myoglobin)
```

Vemos que se trata una `list` formada por una sola componente, el vector de bases, y un atributo. Vamos a extraer el vector, para poder trabajar con él. La manera más sencilla es añadiendo a la  `list` el sufijo `[[1]]`:

```{r}
myoglobin=myoglobin[[1]]
myoglobin[1:10]
```

Nos preguntamos si en esta secuencia las cuatro bases aparecen de manera equiprobable. Para responder esta pregunta, calculamos las frecuencias de las letras con la función `table` y aplicamos el test $\chi^2$ a los resultados.  

```{r}
table(myoglobin)
```

Aparecen valores `n`, que corresponden a bases no resueltas. Vamos borrarlas de la secuencia:

```{r}
myoglobin=myoglobin[myoglobin!="n"]
table(myoglobin)
```

Ahora ya estamos en condiciones de llevar a cabo el contraste deseado con la función `chisq.test`. Puesto que miramos si todos los resultados aparecen con la misma probabilidad, no hace falta especificar el vector `p` de probabilidades.

```{r}
chisq.test(table(myoglobin))
```

El p-valor es prácticamente 0, podemos rechazar que las cuatro bases aparezcan con la misma probabilidad: las diferencias entre las frecuencias de los cuatro aminoácidos son lo suficientemente grandes como para hacer inverosímil que se hayan generado con la misma probabilidad. 


```{example}
Siguiendo con el ejemplo anterior, vamos a contrastar ahora  si las bases siguen una distribución en la que A y G aparecen un 25% de veces más que C y T. Usaremos `p=c(1.25,1,1.25,1)` para especificar estas proporciones.


```

```{r,warning=TRUE,error=TRUE,message=TRUE}
chisq.test(table(myoglobin),p=c(1.25,1,1.25,1))
```

¡Vaya! Nos habíamos olvidado de especificar  `rescale.p=TRUE`, para poder entrar como `p` un vector proporcional a las probabilidades.

```{r}
chisq.test(table(myoglobin),p=c(1.25,1,1.25,1),rescale.p=TRUE)$p.value
```

De nuevo, tenemos que  rechazar la hipótesis nula.

```{example}
Vamos a realizar otro experimento con la cadena del gen de la mioglobina. Este gen consta de tres exones. El primero corresponde al número de acceso M10090.1. Vamos a comparar si la frecuencia de bases en este exón es  similar a la de la cadena total, que hará de distribución teórica.


```

Para ello, leemos el exón y lo guardamos en una cadena
```{r}
exon1=read.GenBank("M10090.1", as.character=TRUE)[[1]]
```

Calculemos la tabla de frecuencias relativas de las bases en la cadena completa
```{r}
probs.tot=prop.table(table(myoglobin))
round(probs.tot,3)
```

Vamos a usar esta tabla como parámetro `p` de la función `chisq.test`:

```{r}
chisq.test(table(exon1),p=probs.tot)$p.value
```

El p-valor es muy pequeño, y por lo tanto podemos rechazar que en este exón las bases aparezcan con la misma probabilidad que en la cadena total. 


```{example}
Ahora vamos a llevar a cabo el experimento siguiente. Queremos contrastar si la aparición de pares "cg" en la mioglobina humana es aleatoria, en el sentido de que se debe simplemente a las apariciones al azar de sus dos bases, o si por el contrario hay algún otro mecanismo que los produce.

```

Para ello, tomaremos la secuencia completa de la mioglobina humana, que tenemos almacenada en `myoglobin`, y repetiremos 100 veces el proceso siguiente: extraemos una muestra aleatoria simple de 20 posiciones  de la secuencia y contamos cuántas de ellas contienen el par de bases "cg". Luego contrastaremos si la muestra así obtenida proviene de una distribución binomial con la probabilidad de aparición de "cg" como si las dos bases fueran independientes.

Fijamos la semilla de aleatoriedad para que se pueda reproducir el experimento.^[Lo confesamos, hemos elegido la semilla de aleatoriedad no porque seamos fans de [Daniel Bernoulli](https://es.wikipedia.org/wiki/Daniel_Bernoulli) sino  para que la muestra obtenida solo contenga ceros, unos y doses, lo que, como veréis, motivará una pequeña discusión sobre qué clases tomar.] El código, que luego explicamos, y el resultado son los siguientes:
  
```{r}
cg.in.sample=function(x,S){#x un vector, S vector de índices
  length(which(x[S]=="c" & x[S+1]=="g"))
}
set.seed(1700)  
muestra=replicate(100, cg.in.sample(myoglobin, 
            sample(1:(length(myoglobin)-1), 20,replace=TRUE)))
muestra
```
La función `cg.in.sample` que hemos definido toma un vector `x` y un vector de índices `S` y cuenta el número de índices `s` de `S` en los que `x[s]` es `c` y `x[s+1]` es `g`. Entonces, con el `replicate`, hemos repetido 100 veces el proceso de extraer una muestra aleatoria simple  de 20 índices de `myoglobin` (excluyendo el último, para que sean posiciones donde empieza un par de letras) y aplicar la función `cg.in.sample` a `myoglobin` y a este vector de índices.

Queremos determinar si esta muestra sigue una distribución binomial. En concreto, vamos a plantear tres casos de esta pregunta:

* Con probabilidad de aparición de la pareja "cg" 0.25·0.25=`r 0.25*0.25`, que correspondería al hecho de que las dos bases aparecieran de manera equiprobable e independiente.

* Con probabilidad de aparición de la pareja "cg" el producto de  las frecuencias relativas de las bases en la secuencia global, que correspondería al hecho de que las dos bases aparecieran de manera independiente, pero no equiprobable sino con sus probabilidades dentro de la secuencia de la mioglobina. 

* Estimando el valor "real" de $p$, lo que correspondería al hecho de que su probabilidad de aparición no tuviera nada que ver con las probabilidades individuales de sus dos bases.


Empezamos con el primer caso. Calculemos las frecuencias con las que aparecen los diferentes resultados en la muestra:

```{r}
table(muestra)
```

Y las frecuencias esperadas con las que deberían aparecer si siguieran una distribución B(20,`r 0.25*0.25`):

```{r}
round(dbinom(0:20,20,0.0625)*100,2)
```

Las frecuencias esperadas a partir de 4 son inferiores a 5 (recordad que la primera frecuencia corresponde al 0), y además su suma no llega a 5. Por lo tanto, vamos a agrupar en una sola clase los resultados mayores o iguales que 3. La nueva tabla de frecuencias esperadas es:

```{r}
round(c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625))*100,2)
```

Ahora tenemos dos opciones: o bien tomamos como resultados posibles "0", "1", "2" y "3 o más", en cuyo caso contaríamos que hemos observado 0 veces este último resultado en nuestra muestra, o bien tomamos como resultados posibles "0", "1",  y "2 o más", que se corresponde con los valores observados. Como norma general, es recomendable usar el mayor número de clases posible. Por consiguiente, vamos a optar por la primera estrategia: 4 clases y no 3. Por lo tanto, hay que añadir a la tabla de frecuencias de la muestra un 0 en la columna correspondiente a 3 o más observaciones.

```{r}
freq.obs=c(table(muestra),0)
prob.teor=c(dbinom(0:2,20,0.0625),1-pbinom(2,20,0.0625))
chisq.test(freq.obs,p=prob.teor)$p.value
```
El p-valor es prácticamente 0, por lo que podemos concluir que la muestra no sigue una distribución B(20,0.0625): las apariciones de los pares "cg" no se explican por la aparición de sus dos bases de manera independiente y equiprobable.


¿Hubiera variado la conclusión si hubiéramos optado por solo considerar tres clases?
```{r}
freq.obs=table(muestra)
prob.teor=c(dbinom(0:1,20,0.0625),1-pbinom(1,20,0.0625))
chisq.test(freq.obs,p=prob.teor)$p.value
```
El p-valor es prácticamente el mismo, la conclusión es la misma.




Pasemos al segundo caso de nuestro problema. Vamos a calcular la frecuencia relativa de "c" y "g" en la secuencia completa de la mioglobina humana y tomaremos como probabilidad $p$ el producto de ambas frecuencias relativas.

```{r}
prop.table(table(myoglobin))
p=prod(prop.table(table(myoglobin))[2:3])
p
```
Calculemos ahora las frecuencias esperadas tomando esta $p$:

```{r}
round(dbinom(0:20,20,p)*100,2)
```

Vamos a tener que agrupar de nuevo en una sola clase los resultados mayores o iguales que 3.
```{r}
freq.obs=c(table(muestra),0)
prob.teor=c(dbinom(0:2,20,p),1-pbinom(2,20,p))
chisq.test(freq.obs,p=prob.teor)$p.value
```
Obtenemos de nuevo un valor prácticamente 0: las apariciones de los pares "cg" no se explican por la aparición de sus dos bases de manera independiente.



Finalmente, vamos a estimar el parámetro $p$. La binomial es otra de las distribuciones no cubiertas por `fitdistr`, por lo que tendremos que apelar a lo que sabemos de teoría para hacerlo. 
Como el valor esperado de una variable aleatoria $X\sim B(n,p)$ es $np$,  estimaremos  $p$ mediante $\overline{X}/n$. De hecho, éste es el estimador máximo verosímil de $p$ cuando $n$ es conocida.
```{r}
p.estim=mean(muestra)/20
p.estim
```
Repetimos el proceso: calculemos las frecuencias teóricas
```{r}
round(dbinom(0:20,20,p.estim)*100,2)
```
En este caso, hemos  de agrupar  los resultados en "0", "1" y "2 o más", para que las frecuencias teóricas sean mayores que 5. 
Coincide con los diferentes valores observados en la muestra.
```{r}
freq.obs=table(muestra)
prob.teor=c(dbinom(0:1,20,p.estim),1-pbinom(1,20,p.estim))
chisq.test(freq.obs,p=prob.teor)
```
¡Cuidado! Este p-valor no es el correcto. Hemos estimado un parámetro, pero R no lo sabe. Por lo tanto tenemos que bajar en 1 los grados de libertad y calcular el p-valor a mano, mediante
$$
  P(\chi_1^2\geq  X^2)=1-P(\chi_1^2\leq  `r 
  round(chisq.test(freq.obs,p=prob.teor)$statistic,3)`)
$$

```{r}
1-pchisq(chisq.test(freq.obs,p=prob.teor)$statistic,1)
```
El p-valor es  grande. Por lo tanto, no podemos rechazar la hipótesis nula de que
las apariciones de "cg" en muestras aleatorias de 20 posiciones sigan una ley binomial de parámetro $p=0.019$. Que es, por otro lado, lo que debería pasar si las "cg" estuvieran repartidas de manera aleatoria en la secuencia original, por lo que no podemos rechazar esto último.

La conclusión es, por lo tanto, que aceptamos que las "cg" aparecen distribuidas de manera aleatoria en la secuencia de la mioglobina humana, pero tenemos evidencia estadísticamente significativa de que las "c" y las "g" que las forman no aparecen de manera independiente.


## El test $\chi^2$ para distribuciones continuas

El procedimiento de contraste de bondad de ajuste mediante el test $\chi^2$ para variables continuas tiene la particularidad de que es necesario un paso preliminar que consiste en definir los intervalos de clase para los que realizaremos el conteo de las frecuencias observadas. El proceso es similar al que estudiamos en la Lección \@ref(chap:agrup) de la primera parte del curso para dibujar histogramas. Por lo tanto, necesitaremos definir unos intervalos de clase para el conteo de frecuencias absolutas observadas,  y con las funciones `cut` y `table` obtendremos las frecuencias observadas de estas clases en la muestra de la variable continua.

Para obtener los intervalos podemos seguir dos estrategias razonables:  reutilizar  los generados por la función `hist`, o dividir el rango de la variable en un número prefijado $k$ de intervalos  de amplitud fija. Vamos a ver en detalle un ejemplo de cada tipo.

```{example,label=testnormiris1}
Vamos a contrastar si las longitudes de los sépalos de las plantas iris recogidas en la tabla de datos `iris` siguen una distribución normal. Recordaréis que el Q-Q-plot de estas longitudes que mostrábamos en la Figura \@ref(fig:qqplotiris) mostraba evidencia de que no la siguen.


```

Primero vamos a estimar de nuevo la media y la desviación típica de la distribución de estas longitudes.
```{r}
iris.sl=iris$Sepal.Length
mu=fitdistr(iris.sl,"normal")$estimate[1]
sigma=fitdistr(iris.sl,"normal")$estimate[2]
round(c(mu,sigma),3)
```

En este ejemplo, usaremos los intervalos en los que la función `hist` agrupa por defecto estos datos.

```{r}
h=hist(iris.sl, plot=FALSE)
h$breaks
```

Ahora se nos presenta el problema de que los intervalos que definen estos puntos de corte  (*breaks*) no cubren toda la recta real, que es el  espacio muestral de una variable aleatoria normal. Así que tenemos que reemplazar los extremos de este vector de `breaks` por los límites del  espacio muestral de la variable, que en este caso son $-\infty$ e $\infty$.

```{r}
breaks2=h$breaks
breaks2[1]=-Inf    #Cambiamos el primer elemento por -Infinito
breaks2[length(breaks2)]=Inf   #Cambiamos el último elemento por Infinito
breaks2
```

Ahora podemos calcular las frecuencias de la muestra en los intervalos definidos por estos puntos de corte:
```{r}
freq.obs=table(cut(iris.sl,breaks=breaks2))
freq.obs
```
Ahora calcularemos las probabilidades teóricas. Para cada intervalo $(x,y]$ en los que hemos cortado la recta real, tenemos que calcular $P(x < X\leq y)= P(X\leq y)-P( X\leq x)$, para lo que usaremos expresiones de la forma `pnorm(y,mu,sigma)-pnorm(x,mu,sigma)`.


Definimos dos vectores que nos den los extremos izquierdo y derecho de cada intervalo.
```{r}
extremo.izq=breaks2[-length(breaks2)]
extremo.der=breaks2[-1]
extremo.izq
extremo.der
```
Ahora podemos calcular las probabilidades teóricas y las frecuencias esperadas de todos los intervalos de golpe. La probabilidades teóricas son:
```{r}
probs.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma) 
probs.teor
```

Las frecuencias esperadas son:

```{r}
freq.esp=probs.teor*length(iris.sl)
round(freq.esp,3)
```

La frecuencia esperada de la última clase es inferior a 5, así que vamos a fundirla con la penúltima y así la clase resultante tendrá una frecuencia esperada superior a 5. 

```{r}
k=length(probs.teor)
probs.teor2=c(probs.teor[1:(k-2)],sum(probs.teor[(k-1):k])) #Nuevas probabilidades teóricas
freq.obs2=c(freq.obs[1:(k-2)],sum(freq.obs[(k-1):k])) #Nuevas frecuencias observadas
chisq.test(freq.obs2,p=probs.teor2)
```
Recordemos que el p-valor obtenido no es el correcto: como hemos estimado dos parámetros, lo tenemos que  calcular con una $\chi^2$ con 4 grados de libertad (dos menos de los que ha usado `chisq.test`):
```{r}
test.iris=chisq.test(freq.obs2,p=probs.teor2)
1-pchisq(test.iris$statistic,test.iris$parameter-2)
```
El p-valor es inferior a 0.05, por tanto obtenemos evidencia de que la muestra no proviene de una población normal, es decir, de que las longitudes de los sépalos de las flores iris no siguen una ley normal.


```{example}
Vamos a repetir el estudio del ejemplo anterior, pero ahora calculando a mano los intervalos.
En general, el número de intervalos debe ser suficiente para cubrir toda la forma de la
distribución, pero tampoco conviene que haya muchos para evitar frecuencias esperadas  pequeñas que obliguen a agrupar intervalos.  Para una distribución normal  se recomienda tomar entre 5 y 15 intervalos. Otra posibilidad es decidir el número de intervalos con alguna de las reglas explicadas en la Lección \@ref(chap:agrup) de la primera parte del curso.


```


En nuestro ejemplo, vamos a usar 10 intervalos. Para calcularlos, tomamos el máximo y el mínimo de las observaciones, los restamos y dividimos por el número de intervalos (y, si fuera necesario, redondearíamos adecuadamente).
```{r}
Ampl=(max(iris.sl)-min(iris.sl))/10
Ampl
```
Los extremos de los intervalos en los que dividimos la muestra forman la secuencia que empieza en el mínimo y va sumando la amplitud hasta definir los $k=10$ intervalos. Luego hay que adecuar los dos extremos para que cubran el dominio de la densidad de la distribución teórica, en nuestro caso toda la recta real.
```{r}
breaks=min(iris.sl)+Ampl*(0:10)
breaks
breaks2=breaks
breaks2[1]=-Inf
breaks2[length(breaks2)]=Inf
breaks2
```
Calculemos, como en el ejemplo anterior, las frecuencias observadas, las probabilidades teóricas y las frecuencias esperadas.
```{r}
frec.obs=table(cut(iris.sl,breaks=breaks2))
frec.obs
extremo.izq=breaks2[-length(breaks2)]
extremo.der=breaks2[-1]
prob.teor=pnorm(extremo.der,mu,sigma)-pnorm(extremo.izq,mu,sigma)
frec.esp=round(prob.teor*length(iris.sl),2)
frec.esp
```
Agruparemos las frecuencias de los dos últimos intervalos y aplicaremos el test $\chi^2$ con el número adecuado de grados de libertad:
```{r}
frec.obs2=c(frec.obs[1:8], sum(frec.obs[9:10]))
prob.teor2=c(prob.teor[1:8], sum(prob.teor[9:10]))
test.iris.2=chisq.test(frec.obs2,p=prob.teor2)
1-pchisq(test.iris.2$statistic, test.iris.2$parameter-2)
```
El p-valor es de nuevo inferior a 0.05: volvemos a obtener evidencia significativa  de que la muestra no proviene de una población normal.



## El test de Kolgomorov-Smirnov

El **test de Kolgomorov-Smirnov** (**K-S**) es un test genérico para contrastar la bondad de ajuste a distribuciones continuas.  Se puede usar con muestras pequeñas (se suele recomendar 5 elementos como el tamaño mínimo para que el resultado sea significativo), pero  la muestra no puede contener valores repetidos: si los contiene, la distribución del estadístico de contraste bajo la hipótesis nula  no es la que predice la teoría sino que solo se aproxima a ella, y por lo tanto los p-valores que se obtienen son aproximados. 

Hay que tener en cuenta que el test K-S realiza un contraste en el que la hipótesis nula es que la muestra proviene de una distribución continua  completamente especificada. Es decir, no sirve para contrastar si la muestra proviene, pongamos, de "alguna" distribución normal, sino solo para contrastar si proviene de una distribución normal con una media y una desviación típica concretas. Así pues, si queremos contrastar que la muestra proviene de alguna distribución de una familia concreta y  estimamos sus parámetros a partir de la muestra, el test K-S solo nos permite rechazar o no la hipótesis de que la muestra proviene de la distribución de esa familia con exactamente esos parámetros. Por lo tanto, si el resultado es rechazar la hipótesis nula, esto no excluye que la muestra provenga de una distribución de la misma familia con otros parámetros.  En la próxima sección veremos algunos tests que permiten contrastar, en general, si una muestra proviene de alguna distribución normal.


La función básica para realizar el test K-S es `ks.test`. Su sintaxis
básica para una muestra es
```{r, eval=FALSE}
ks.test(x, y, parámetros)
```

donde:

* `x` es la muestra de una variable continua.

* `y` puede ser un segundo vector, y entonces se contrasta si ambos vectores han sido generados por la misma distribución continua, o el nombre de la función de distribución (empezando con `p`) que queremos contrastar, entre comillas; por ejemplo `"pnorm"` para
la distribución normal.

* Los `parámetros` de la función de distribución si se ha especificado una; por
ejemplo `mean=0, sd=1` para una distribución normal estándar.


```{example}
Efectuemos el test de Kolmogorov-Smirnov para contrastar si las longitudes de sépalos de flores iris siguen una distribución normal de media y desviación típica sus estimaciones máximo verosímiles a partir la muestra `iris.sl`. Recordemos que tenemos guardados de los dos últimos ejemplos los valores de estas estimaciones en las variables `mu` y `sigma`:
  
  
```

```{r, warning=TRUE}
round(c(mu,sigma),3)
ks.test(iris.sl, "pnorm", mean=mu, sd=sigma)
```

Obtenemos un p-valor de `r round(ks.test(iris.sl, "pnorm", mean=mu, sd=sigma)$p.value,3)`, que no nos permite rechazar la hipótesis de que siguen una ley N(`r round(c(mu,sigma),3)`). Pero R nos avisa de que hay empates. ¿Hay muchos? Vamos a calcular su frecuencia.
  
La función `unique` aplicada a un vector nos da el vector de sus elementos sin repeticiones. De esta manera podemos saber cuántos elementos diferentes hay en un vector, y por consiguiente también cuántas repeticiones.

```{r}
length(unique(iris.sl))
1-length(unique(iris.sl))/length(iris.sl)
```
Por tanto, el vector (de 150 entradas) de longitudes de sépalos solo tiene `r length(unique(iris.sl))` valores diferentes. El resto, un `r round(100*(1-length(unique(iris.sl))/length(iris.sl)),2)`%, son valores repetidos. Hay muchos empates, y el resultado de este test en este caso es poco fiable 


Como hemos comentado, el test K-S también se puede usar para contrastar si dos muestras se han obtenido de poblaciones con la misma distribución continua. Para hacerlo, se ha de aplicar la función `ks.test` a las dos muestras.

```{example}
La tabla de datos `Salaries` del paquete **car** contiene información sobre los sueldos de  397 profesores de una universidad norteamericana en el curso 2008-09. Démosle un vistazo.


```


```{r}
library(car)
str(Salaries)
```

La variable `sex` nos da el sexo del profesor y la variable `salary` su sueldo anual en dólares. Queremos contrastar si los sueldos de hombres y mujeres siguen la misma distribución. Para ello, vamos a suponer que provienen de distribuciones continuas y usaremos el test K-S. Primero miraremos si hay muchos empates.
```{r}
sal.female=Salaries[Salaries$sex=="Female",]$salary  #Salarios de mujeres
sal.male=Salaries[Salaries$sex=="Male",]$salary  #Salarios de hombres
1-length(unique(sal.female))/length(sal.female) #Proporción de salarios de mujeres repetidos
1-length(unique(sal.male))/length(sal.male)  #Proporción de salarios de hombres repetidos
1-length(unique(Salaries$salary))/length(Salaries$salary)  #Proporción global de salarios repetidos
```

Las repeticiones en cada lista significan alrededor del 5% de los datos, y en total un `r round(100*(1-length(unique(Salaries$salary))/length(Salaries$salary)),1)`%. No son muchas, así que vamos a arriesgarnos con el test K-S.

```{r,}
ks.test(sal.male,sal.female)
```

El p-valor pequeño nos permite rechazar que los salarios de hombres y mujeres sigan la misma distribución. Pero no nos paremos aquí.

Si dibujamos un boxplot (véase la Figura \@ref(fig:duros)) de los salarios según el sexo, observaremos que los sueldos de los hombres tienen mayor mediana y variabilidad que los de las mujeres, incluyendo algunos valores atípicos grandes (¿el rector y otros altos cargos académicos?). 

```{r, fig.cap="Boxplot de sueldos según el sexo en la tabla de datos *Salaries*", label=duros}
boxplot(salary~sex, data=Salaries, main="")
```



Si cancelamos este efecto, estandarizando las muestras, ¿siguen saliendo distribuciones diferentes?
```{r, warning=TRUE}
ks.test(scale(sal.male),scale(sal.female))
```

Al estandarizar, ya no tenemos evidencia de que provengan de distribuciones diferentes. Es decir, podemos aceptar que sus valores tipificados siguen la misma distribución.



## Tests de normalidad

Existen algunos tests específicos de normalidad que permiten contrastar si una muestra proviene de alguna distribución normal. El más conocido es el **test de normalidad de Kolmogorov-Smirnov-Lilliefors** (**K-S-L**).  Se trata de una variante del test K-S, y se puede realizar aplicando a la muestra la función `lillie.test` del paquete **nortest**.

Vamos a usar el test K-S-L para contrastar si las longitudes de los sépalos de las iris siguen una ley normal. 
```{r}
library(nortest)
iris.sl=iris$Sepal.Length
lillie.test(iris.sl)
```
El p-valor es muy pequeño, y nos permite rechazar que la muestra provenga de una población normal.

La ventaja del test K-S-L es que es muy conocido, ya que es una variante del K-S (incluso usa el mismo estadístico), pero tiene un inconveniente: aunque es muy sensible a las diferencias entre la muestra y la distribución teórica alrededor de sus valores medios, le cuesta detectar diferencias prominentes en un extremo u otro de la distribución. Esto afecta su potencia. Por ejemplo, sabemos que una t de Student se parece bastante a una normal estándar, pero su densidad es algo más aplanada y hace que en los dos extremos esté por encima de la de la normal.
Al test K-S-L le cuesta detectar esta discrepancia, como podemos ver en el siguiente ejemplo:
```{r}
set.seed(100)
x=rt(50,3)   #Una muestra de una t de Student con 3 g.l.
lillie.test(x)
```


Este inconveniente del test K-S-L lo resuelve el **test de normalidad de Anderson-Darling** (**A-D**).  Para realizarlo podemos usar la función `ad.test` del paquete **nortest**. Encontraréis los detalles del estadístico que usa en la Ayuda de la función.

```{r}
ad.test(iris.sl)
```
De nuevo obtenemos un p-valor muy pequeño. Veamos ahora que este test sí que detecta que la muestra anterior de una t de Student con 3 grados de libertad no proviene de una normal:
```{r}
set.seed(100)
x=rt(50,3)
ad.test(x)
```


Un inconveniente común a los tests K-S-L y A-D es que, si bien pueden usarse con muestras pequeñas (pongamos de más de 5 elementos), se comportan mal con muestras grandes, de varios miles de elementos. En muestras de este tamaño, cualquier pequeña divergencia de la normalidad se magnifica y en estos dos tests aumenta la probabilidad de errores de tipo I. Un test que resuelve este problema es el de **Shapiro-Wilk** (**S-W**), implementado en la función  `shapiro.test` de la instalación básica de R. 
Este test es importante, porque un experimento reciente ha mostrado evidencia significativa de que su potencia es mayor que la de los tests anteriores.^[Véase N. M. Razali, Y. B. Wah,
"Power comparisons of Shapiro-Wilk, Kolmogorov-Smirnov, Lilliefors and Anderson-Darling tests." *S.  Stat. Model.  Anal.*  2 (2011), pp. 21--33.]
De nuevo, los detalles del estadístico que usa los encontraréis en la Ayuda de la función.

```{r}
shapiro.test(iris.sl)
set.seed(100)
x=rt(50,3)
shapiro.test(x)
```

Un último inconveniente que afecta a todos los tests explicados hasta ahora es el de los empates. Sus estadísticos  tienen las distribuciones que se usan para calcular los p-valores cuando la muestra no tiene datos repetidos, y por lo tanto, si hay muchos, el p-valor puede no tener ningún significado. De los tres, el menos sensible a repeticiones es el S-W, pero si hay muchas es conveniente usar un test que no sea sensible a ellas, como por ejemplo el **test omnibus de D'Agostino-Pearson**. Este test se encuentra implementado en la función `dagoTest` del paquete **fBasics**, y lo que hace es cuantificar lo diferentes que son la asimetría y la curtosis de la muestra (dos parámetros estadísticos relacionados con la forma de la gráfica de la función de densidad muestral)  respecto de los esperados en una distribución normal, y resume esta discrepancia en un p-valor con el significado usual.
  
```{r,warning=FALSE,message=FALSE}
library(fBasics)
dagoTest(iris.sl)
```

El p-valor relevante es el del *"Omnibus test"*, en este caso `r round(dagoTest(iris.sl)@test$p.value[1],4)` cae en la zona de penumbra.
  
Queremos hacer una última advertencia en esta sección. Aunque los tests que hemos explicado se pueden aplicar a muestras pequeñas, es muy difícil rechazar la normalidad de una muestra  muy pequeña. Por ejemplo, una muestra de 10 valores escogidos con distribución uniforme entre 0 y 5 pasa holgadamente todos los tests de normalidad (salvo el de D'Agostino-Pearson, que requiere una muestra de al menos 20 elementos):

```{r,error=TRUE,warning=TRUE}
set.seed(100)
x=runif(10,0,5)
lillie.test(x)
ad.test(x)
shapiro.test(x)
dagoTest(x)
```


## Guía rápida

* `qqPlot` del paquete **car**, sirve para dibujar un Q-Q-plot de una muestra contra una distribución teórica.  Sus parámetros principales son:

    * `distribution`: el nombre de la familia de distribuciones, entre comillas.
    * Los parámetros de la distribución: `mean` para la media, `sd` para la desviación típica, `df` para los grados de libertad, etc.
    *  Los parámetros usuales de `plot`.


* `chisq.test` sirve para realizar tests $\chi^2$ de bondad de ajuste. Sus parámetros principales son:

    * `p`: el vector de probabilidades teóricas.
    *  `rescale.p`: igualado a `TRUE`, indica que  los valores de `p` no son probabilidades, sino sólo proporcionales a las probabilidades.
    *  `simulate.p.value`: igualado a `TRUE`, R calcula el p-valor mediante simulaciones.
    *  `B`: en este último caso, permite especificar el número de simulaciones.

* `ks.test` realiza el test de Kolmogorov-Smirnov. Tiene dos tipos de uso:

    * `ks.test(x,y)`: contrasta si los vectores `x` e `y`  han sido generados por la misma distribución continua.
    * `ks.test(x, "distribución", parámetros)`: contrasta si el vector `x`  ha sido generado por la distribución especificada, que se ha de indicar con el nombre de la función de distribución de R  (la que empieza con p).

*  `lillie.test` del paquete **nortest**, realiza el test de normalidad de Kolmogorov-Smirnov-Lilliefors.

*  `ad.test` del paquete **nortest**, realiza el test de normalidad de Anderson-Darling.

*  `shapiro.test`, realiza el test de normalidad de Shapiro-Wilk.

*  `dagoTest` del paquete **fBasics**, realiza el test ómnibus de D'Agostino-Pearson.

## Ejercicios

### Modelo de test {-}

*(1)*  Un determinado experimento tiene cinco resultados posibles:  A, B, C, D, E. Lo repetimos un cierto número de veces y obtenemos 65 veces el resultado A, 95 veces el resultado B, 87 veces el resultado C, 70 veces el resultado D y 193 veces el resultado E. Realizad un test $\chi^2$ para contrastar si los resultados A, B, C y D tienen la misma probabilidad y E tiene el doble de probabilidad que cada uno de los otros resultados. Dad el p-valor del contraste (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI, en mayúsculas y sin acento, o NO) si tendríamos que rechazar la hipótesis nula con un nivel de significación $\alpha=0.05$. Tenéis que dar las respuestas en este orden y separadas por un único espacio en blanco.

*(2)* Queremos contrastar si una determinada variable sigue una distribución de Poisson. Hemos efectuado algunas observaciones y hemos obtenido 10 veces el resultado 0, 32 veces el resultado 1, 18 veces el resultado 2, 19 veces el resultado 3 y 6 veces el resultado 4. Tenéis que calcular el estimador máximo verosímil del parámetro $\lambda$ de una variable de Poisson que haya generado estas observaciones (redondeado a 3 cifras decimales, y sin ceros innecesarios a la derecha), calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test $\chi^2$ para determinar si la muestra sigue *alguna* distribución de Poisson habiendo estimado como su parámetro $\lambda$ este valor redondeado, y decir (contestando SI  o NO) si, con un nivel de significación $\alpha=0.1$, tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria de Poisson. Dad las tres respuestas en este orden y separadas por un único espacio en blanco.


*(3)* Queremos contrastar si una cierta variable sigue una distribución normal. Hemos efectuado 150 observaciones y hemos obtenido 9 veces un valor dentro de ]0,3], 27 veces un valor dentro de ]3,6], 51 veces un valor dentro de ]6,9], 46 veces un valor dentro de ]9,12] y 17 veces un valor dentro de ]12,15]. Tenéis que: calcular los estimadores máximo verosímiles del parámetro $\mu$ y del parámetro $\sigma$ de una variable normal que haya generado estas observaciones (ambos redondeados a 2 cifras decimales y sin ceros innecesarios a la derecha): calcular el p-valor (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) del test $\chi^2$ para contrastar si la muestra sigue *alguna* distribución normal, empleando estos valores estimados redondeados de los parámetros que habéis dado para especificar la distribución teórica; y decir (contestando SI  o NO) si, con un nivel de significación $\alpha=0.05$, tendríamos que rechazar la hipótesis nula de que esta muestra proviene de una variable aleatoria normal. Dad las cuatro respuestas en este orden y separadas por un único espacio en blanco.

*(4)* Generad una muestra aleatoria *x* de 25 valores de una distribución $\chi^2$ con 10 grados de libertad, fijando antes `set.seed(2014)`, y aplicad el test de Kolmogorov-Smirnov para contrastar si *x* proviene de una distribución N(10,3.16). Dad el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decid (contestando SI o NO) si, con un nivel de significación $\alpha=0.05$, tendríamos que rechazar la hipótesis nula de que esta muestra proviene de esta distribución. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco.

*(5)* Queremos contrastar si la muestra siguiente sigue una distribución normal: 4.6, 0.97,  0.3,  1.11,  2.16, 15.52,  1.13,  0.17, 0.64,  2.00. Dad el p-valor (redondeado a 3 cifras decimales y sin ceros innecesarios a la derecha) del test de Kolmogorov-Smirnov-Lilliefors para esta muestra y decid (contestando SI  o NO) si, con un nivel de significación $\alpha=0.05$, tendríamos que rechazar la hipótesis nula de que esta muestra sigue una distribución normal. Tenéis que dar las dos respuestas en este orden y separadas por un único espacio en blanco.

*(6)* Generad una muestra aleatoria *x* de 15 valores de una distribución normal con $\mu=2$ y $\sigma=0.8$ fijando antes `set.seed(2014)`, y una muestra aleatoria *y* de 25 valores de una distribución exponencial de parámetro $1/\lambda=0.5$  fijando antes `set.seed(1007)`. Aplicad el test de Kolmogorov-Smirnov para contrastar si *x* e *y* provienen de una misma distribución continua. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI  o NO) si, con un nivel de significación $\alpha=0.05$, tendríamos que rechazar la hipótesis nula de que estas dos muestras provienen de la misma distribución continua. Dad las dos respuestas en este orden y separadas por un único espacio en blanco.



### Respuestas al test {-}

```{r,include=FALSE}
#1
x=c(65,95,87,70,193)
R.Chi1=chisq.test(x,p=c(1,1,1,1,2),rescale.p=T)$p.value
#2
x=c(10,32,18,19,6)
lambda=fitdistr(rep(0:4,x),"poisson")$estimate
c(dpois(0:3,round(lambda,3)),1-ppois(3,round(lambda,3)))*85
ChiT2=chisq.test(x,p=c(dpois(0:3,round(lambda,3)),1-ppois(3,round(lambda,3))))
R.Chi2=1-pchisq(ChiT2$statistic,3)
#3
x=c(9,27,51,46,17)
sum(x)
mostra=rep(c(1.5,4.5,7.5,10.5,13.5),x)
fitdistr(mostra,"normal")
mu=round(fitdistr(mostra,"normal")$estimate[1],2)
sigma=round(fitdistr(mostra,"normal")$estimate[2],2)
left=c(-Inf,3,6,9,12)
right=c(3,6,9,12,Inf)
probs=pnorm(right,mu,sigma)-pnorm(left,mu, sigma)
ChiT3=chisq.test(x,p=probs)
R.Chi3=1-pchisq(ChiT3$statistic,2)
#4
set.seed(2014)
x=rchisq(25,10)
R.Chi4=ks.test(x,"pnorm",mean=10,sd=3.16)$p.value
# 5 
x=c(4.6, 0.97,  0.3,  1.11,  2.16, 15.52,  1.13,  0.17,  0.64,  2.00)
R.Chi5=lillie.test(x)$p.value
# 6
set.seed(2014)
x=rnorm(15,2,0.8)
set.seed(1007)
y=rexp(25,0.5)
R.Chi6=ks.test(x,y)$p.value
```


*(1)* `r round(R.Chi1,3)` SI

Nosotros lo hemos calculado con
```{r}
x=c(65,95,87,70,193)
round(chisq.test(x,p=c(1,1,1,1,2),rescale.p=T)$p.value,3)
```


*(2)* `r round(lambda,3)` `r round(R.Chi2,3)` SI

Nosotros lo hemos calculado con

```{r}
x=c(10,32,18,19,6)
lambda=round(fitdistr(rep(0:4,x),"poisson")$estimate,3)  #Estimamos la lambda
c(dpois(0:3,lambda),1-ppois(3,lambda))*85  #Frecuencias esperadas
ChiT2=chisq.test(x,p=c(dpois(0:3,lambda),1-ppois(3,lambda)))  #Test
p.valor2=1-pchisq(ChiT2$statistic,3)  #p-valor correcto
c(lambda,round(p.valor2,3))
```


*(3)* `r round(mu,1)` `r round(sigma,2)` `r round(R.Chi3,3)` NO

Nosotros lo hemos calculado con

```{r}
x=c(9,27,51,46,17)
sum(x)
muestra=rep(c(1.5,4.5,7.5,10.5,13.5),x)
fitdistr(muestra,"normal")
mu=round(fitdistr(muestra,"normal")$estimate[1],2)
sigma=round(fitdistr(muestra,"normal")$estimate[2],2)
left=c(-Inf,3,6,9,12)
right=c(3,6,9,12,Inf)
probs=pnorm(right,mu,sigma)-pnorm(left,mu, sigma)
ChiT3=chisq.test(x,p=probs)
p.valor3=1-pchisq(ChiT3$statistic,2)
c(mu,sigma,round(p.valor3,3))
```


*(4)* `r round(R.Chi4,3)` NO

Nosotros lo hemos calculado con

```{r}
set.seed(2014)
x=rchisq(25,10)
round(ks.test(x,"pnorm",mean=10,sd=3.16)$p.value,3)
```

*(5)* `r round(R.Chi5,3)` SI

Nosotros lo hemos calculado con

```{r}
x=c(4.6, 0.97,  0.3,  1.11,  2.16, 15.52,  1.13,  0.17,  0.64,  2.00)
round(lillie.test(x)$p.value,3)
```


*(6)* `r round(R.Chi6,3)` NO

Nosotros lo hemos calculado con

```{r}
set.seed(2014)
x=rnorm(15,2,0.8)
set.seed(1007)
y=rexp(25,0.5)
round(ks.test(x,y)$p.value,3)
```


<!--chapter:end:06-BondadAjuste.Rmd-->

# Contrastes de independencia y  homogeneidad {#chap:indep}

El test $\chi^2$ explicado en la lección anterior permite contrastar, en situaciones adecuadas, si una muestra proviene de una determinada distribución y por lo tanto también si una muestra  sigue la misma distribución que otra muestra. Como, en última instancia, la independencia de dos variables cualitativas se puede describir en términos de igualdades de probabilidades, el test $\chi^2$ también nos permitirá contrastar si dos variables cualitativas son independientes, tanto en el sentido de que las probabilidades conjuntas sean el producto de las probabilidades marginales (con un contraste de independencia) como en el sentido de que las distribuciones condicionadas de una respecto de los valores de la otra sean todas iguales (con un contraste de homogeneidad).


Aunque, como veremos, los contrastes de independencia y homogeneidad son idénticos desde el punto de vista matemático e incluso utilizan el mismo estadístico $\chi^2$ y la misma definición de p-valor, provienen de diseños experimentales diferentes:
  
* En un **contraste de independencia** se toma una **muestra transversal** de la población, es decir, se selecciona al azar una cierta cantidad de individuos de la
población, se observan las dos variables sobre cada uno de ellos, y se contrasta si las probabilidades conjuntas son iguales al producto de las probabilidades marginales de
cada variable. Formalmente, si $X$ e $Y$ son las dos variables, se contrasta si para cada par de posibles valores $x$ de $X$ e $y$ de $Y$ se tiene que
$$
P(X=x,Y=y)=P(X=x)\cdot P(Y=y)
$$
o si por el contrario hay algún par de valores $x$, $y$ para los que esta igualdad sea falsa.

* En un **contraste de homogeneidad** se  escoge una de las variables y para cada uno de sus posibles valores se toma una muestra aleatoria, de tamaño prefijado, de individuos con ese valor para esa variable; su unión forma una **muestra estratificada** en el sentido de la Sección \@ref(sec:muestreo).  A continuación, se observa sobre cada uno de estos individuos la otra variable. En esta situación contrastamos si la distribución de probabilidades de la segunda variable es la misma en los diferentes estratos definidos por los niveles de la primera variable. Formalmente, si $Y$ es la variable que usamos en primer lugar para clasificar los individuos de la población y tomar una muestra de cada clase, con posibles valores $y_1,\ldots,y_k$, y $X$ es la variable que medimos en segundo lugar sobre los individuos escogidos, se contrasta si, para cada posible valor $x$ de $X$,
$$
P(X=x|Y=y_1)=P(X=x|Y=y_2)=\cdots=P(X=x|Y=y_k)
$$
o si por el contrario existen $x$, $y_i$, $y_j$ tales que $P(X=x|Y=y_i)\neq P(X=x|Y=y_j)$.

En ambos contrastes, la hipótesis nula es que las variables son independientes, bajo una u otra formulación matemática, y la hipótesis alternativa es que son **dependientes** (o que **hay asociación** entre ellas). La hipótesis nula se rechaza si se obtiene evidencia que hace inverosímiles las igualdades de probabilidades que se contrastan.
 

Para ilustrar esta lección, hemos generado una muestra aleatoria de cadenas formadas por las 
bases  "a", "c", "g" y "t". En concreto, hemos generado cadenas de longitud 100 de tres tipos: *A*, *B*  y  *C.* Estos tipos se distinguen por los vectores de probabilidades  que han determinado las frecuencias de  las cuatro bases en las secuencias.  Queremos  investigar si hay relación entre el tipo (A, B o C) de una cadena, y la base de frecuencia máxima en ella. 

Los datos y el método de generación se encuentran en el repositorio siguiente: 
https://github.com/biocom-uib/Experimento-Cadenas. Este directorio contiene:
 
* El fichero [*LeemeGeneracionDatos*](https://biocom-uib.github.io/Experimento-Cadenas/), que explica cómo se han generado las muestras.

* El fichero [*MuestraTotalBases.txt*](https://raw.githubusercontent.com/biocom-uib/Experimento-Cadenas/master/MuestraTotalBases.txt), que contiene una tabla de datos de 10000 observaciones  de las dos variables siguientes sobre cadenas: el `tipo`, que es un factor con los niveles `A`, `B` y `C`, y  `max.frec`, que es otro factor que indica qué base tiene mayor frecuencia en la cadena. Este fichero es de formato texto, con una primera fila con el nombre de las variables y sus columnas separadas por comas
 
El código siguiente carga la tabla de datos, comprueba que no ha habido problemas, y extrae tres subtablas, una para cada tipo de cadena. Observad cómo se hace para cargar una tabla de datos de un *url* que empieza con **https**, por ejemplo en un repositorio GitHub:

```{r}
library(RCurl)
datos=getURL("https://raw.githubusercontent.com/biocom-uib/Experimento-Cadenas/master/MuestraTotalBases.txt")
poblacion=read.table(text=datos,header=TRUE,sep=",")
str(poblacion)
head(poblacion)
poblacionA=subset(poblacion,tipo=="A")
poblacionB=subset(poblacion,tipo=="B")
poblacionC=subset(poblacion,tipo=="C")
```

## Tablas de contingencia

Ya  estudiamos en la Lección \@ref(chap:edqual) de la primera parte las tablas de contingencia.
En esta sección vamos a repasar y ampliar algunas de las funciones de R para el manejo de
esta clase de tablas.

La tabla de contingencia de frecuencias absolutas conjuntas de las dos variables del *data frame* `poblacion` se calcula de la manera siguiente:
```{r}
tabla=table(poblacion$tipo,poblacion$max.frec)
tabla
```

Su tabla de  frecuencias relativas conjuntas en el total de la muestra, en términos de proporciones (tantos por uno), es:
```{r}
prop.table(tabla)
```


Para añadir las **distribuciones marginales** de la tabla de contingencia (o **márgenes** de
la tabla), se añade una nueva fila con las sumas de cada columna y una nueva columna con las sumas de cada fila. Con R, esto se puede llevar a cabo fácilmente con la función
`addmargins`. Su sintaxis básica es 
```{r,eval=FALSE}
addmargins(tabla, margin=..., FUN=...)
```

donde:
 
* `tabla` es una `table`.

* `margin` es un parámetro que puede tomar los valores siguientes:
 
    * `1` si queremos una nueva fila con las marginales de cada columna.
    * `2`  si queremos una nueva columna con las marginales de cada fila.
    * `c(1,2)`,  que es el valor por defecto para tablas de contingencia bidimensionales (y por lo tanto no hace falta especificarlo), si queremos las marginales por filas y por columnas.^[El valor por defecto de `margin` es el vector de  todas las  dimensiones  de la tabla. Hay que recordar que, aunque ahora sólo tratamos con tablas bidimensionales, con `table` se pueden especificar tablas de contingencia de un número arbitrario de dimensiones.]
 

* `FUN` es la función que se aplica a las filas o columnas para obtener el valor marginal. Por defecto es la suma, que es la función que nos interesa en esta lección, y por tanto tampoco hace falta especificarlo.
 
El resultado es otro objeto de la clase `table` al que se le han añadido una
o varias filas o columnas. Éstas contienen  los márgenes  resultantes
de aplicar la función indicada  por `FUN`. La etiqueta de las nuevas filas o columnas  es la
función que se aplica.

Por ejemplo, para obtener las tablas marginales completas de frecuencias absolutas y relativas en nuestro ejemplo, haríamos:
```{r}
addmargins(tabla) 
addmargins(prop.table(tabla)) 
```

También podemos calcular la tabla de proporciones por filas y  con su marginal por filas comprobar que efectivamente la suma de cada fila es 1: 
```{r}
addmargins(prop.table(tabla,margin=1),margin=2)
```
Y viceversa, podemos calcular la tabla de proporciones por columnas y  con su marginal por columnas comprobar que efectivamente la suma de cada columna es 1: 
```{r}
addmargins(prop.table(tabla,margin=2),margin=1)
```

Observad que el significado de `margin` en `addmargins` es diferente de, por ejemplo, en `prop.table` o en `apply`: en estas dos últimas instrucciones indica la dimensión en la que calculamos las proporciones o aplicamos la función, mientras que  en `addmargins` indica la dimensión en la que *añadimos* el margen, que, por lo tanto, se calcula aplicando la función en la otra dimensión.

Si sólo nos interesa la fila o la columna de marginales, podemos usar las instrucciones
`colSums` y `rowSums`, que suman una tabla por columnas y por filas, respectivamente. Por ejemplo, para obtener los vectores de marginales por columnas y por filas, respectivamente, podríamos entrar:
```{r}
colSums(tabla)
rowSums(tabla)
```
También podemos obtener estos márgenes extrayendo los márgenes de la tabla con márgenes, obtenida aplicando `addmargins` a la tabla original, por medio de las instrucciones usuales para extraer filas y columnas.

```{r}
addmargins(tabla)["Sum",-dim(addmargins(tabla))[2]]
addmargins(tabla)[-dim(addmargins(tabla))[1],"Sum"]
```

Observad, por ejemplo, la construcción 
```{r,eval=FALSE}
addmargins(tabla)["Sum",-dim(addmargins(tabla))[2]]
```

Con `addmargins(tabla)["Sum",]` obtendríamos la fila `Sum` de la tabla con las marginales, incluyendo la última entrada, correspondiente a la columna `Sum`. Por lo tanto, hay que eliminarla: como `dim(addmargins(tabla))[2]` es el número de columnas de la tabla `addmargins(tabla)`, es decir, la longitud del vector `addmargins(tabla)["Sum",]`,  la última entrada (correspondiente a la última columna) se puede eliminar especificando `-dim(addmargins(tabla))[2]` en las columnas al extraer la fila  `Sum`.

## Contraste de independencia

El contraste de independencia para tablas de contingencia bidimensionales consiste en decidir si las dos variables de la tabla tienen distribuciones independientes, es decir, si la distribución de probabilidades conjunta es igual al producto de las probabilidades marginales.  En nuestro ejemplo, queremos decidir si podemos aceptar que las variables `tipo` y `max.frec` son independientes o si por el contrario hay evidencia de que la distribución de las bases de máxima frecuencia depende del tipo de cadena.

Vamos a extraer una muestra aleatoria simple  de la población y observar los valores de las dos variables.  En concreto, seleccionaremos una muestra *transversal* de 150 filas, al azar y con reposición, de entre las 10000 filas  del *data frame* `poblacion`. El código es el siguiente (fijamos la semilla de aleatoriedad para que sea reproducible):

```{r}
set.seed(42)
n=150
indices.muestra=sample(1:10000, size=n, replace=TRUE)  
muestra.test.indep= poblacion[indices.muestra, ] #Las filas que forman la muestra
```

Ahora calculamos la tabla de contingencia con sus marginales.
```{r}
tabla.ind=table(muestra.test.indep$tipo, muestra.test.indep$max.frec)
tabla.ind
tabla.ind.marg=addmargins(tabla.ind)
tabla.ind.marg
```

Extraemos sus dos márgenes. Las frecuencias marginales de las filas:
```{r}
frec.abs.tipo=tabla.ind.marg[-dim(tabla.ind.marg)[1],"Sum"]
frec.abs.tipo
```
Las frecuencias marginales de las columnas:
```{r}
frec.abs.max.frec=tabla.ind.marg["Sum",-dim(tabla.ind.marg)[2]]
frec.abs.max.frec
```

El test de independencia usa las frecuencias absolutas esperadas bajo la hipótesis nula de independencia, que se obtienen, para cada celda *(i,j)*, multiplicando la frecuencia marginal de la fila *i* por la de la columna *j* y dividiendo por el tamaño de la muestra. En nuestro ejemplo estas frecuencias esperadas son
$$
\begin{array}{l|cccc}
& \mbox{a} & \mbox{c} & \mbox{g} & \mbox{t} \\ \hline
\mbox{A} & `r frec.abs.tipo[1]`\cdot `r frec.abs.max.frec[1]`/`r n` & `r frec.abs.tipo[1]`\cdot `r frec.abs.max.frec[2]`/`r n` & `r frec.abs.tipo[1]`\cdot `r frec.abs.max.frec[3]`/`r n` & `r frec.abs.tipo[1]`\cdot `r frec.abs.max.frec[4]`/`r n` \\
\mbox{B} & `r frec.abs.tipo[2]`\cdot `r frec.abs.max.frec[1]`/`r n` & `r frec.abs.tipo[2]`\cdot `r frec.abs.max.frec[2]`/`r n` & `r frec.abs.tipo[2]`\cdot `r frec.abs.max.frec[3]`/`r n` & `r frec.abs.tipo[2]`\cdot `r frec.abs.max.frec[4]`/`r n` \\
\mbox{C} & `r frec.abs.tipo[3]`\cdot `r frec.abs.max.frec[1]`/`r n` & `r frec.abs.tipo[3]`\cdot `r frec.abs.max.frec[2]`/`r n` & `r frec.abs.tipo[3]`\cdot `r frec.abs.max.frec[3]`/`r n` & `r frec.abs.tipo[3]`\cdot `r frec.abs.max.frec[4]`/`r n` 
\end{array}
$$
y podemos obtenerlas fácilmente mediante un producto de matrices:
$$
\frac{1}{`r n`}\cdot\left(\begin{array}{c} `r frec.abs.tipo[1]` \\ `r frec.abs.tipo[2]` \\ `r frec.abs.tipo[3]`\end{array}\right)\cdot
\big( `r frec.abs.max.frec[1]` ,`r frec.abs.max.frec[2]` ,  `r frec.abs.max.frec[3]` , `r frec.abs.max.frec[4]`\big).
$$
Por lo tanto, con R obtenemos esta tabla de frecuencias esperadas de la manera siguiente:
```{r}
frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/n
frec.esperadas
```
Aunque vayamos a realizar el test de independencia con una función de R, es necesario comprobar que todas estas frecuencias esperadas (o al menos la gran mayoría)  son mayores o iguales que 5, por lo que no podemos evitar este cálculo.  En este caso vemos que  se cumple esta condición.

Si queremos realizar el test $\chi^2$ de independencia a mano, 
podemos calcular el estadístico de forma directa con
```{r}
chi2.estadistico=sum((tabla.ind-frec.esperadas)^2/frec.esperadas)
chi2.estadistico
```
y el p-valor del contraste, con 
```{r}
p.valor=1-pchisq(chi2.estadistico,df=(dim(tabla.ind)[1]-1)*(dim(tabla.ind)[2]-1))
p.valor
```

Para realizar el test $\chi^2$ de independencia con R, es suficiente aplicar la función `chisq.test` a la tabla de contingencia de frecuencias absolutas:
```{r}
chisq.test(tabla.ind)
```
Como el p-valor es muy pequeño,  podemos rechazar la hipótesis de que las variables objeto de estudio sean independientes: hemos obtenido evidencia estadísticamente significativa de que la distribución de las bases de frecuencia máxima *sí que depende* del tipo de cadena.

Si algunas frecuencias absolutas esperadas fueran inferiores a 5, la
aproximación del p-valor por una distribución $\chi^2$ podría no ser adecuada.
En este caso, al ser las variables cualitativas, no podemos recurrir al
agrupamiento de valores consecutivos, puesto que no tienen orden. Si se da esta
situación, lo mejor es recurrir a simular el p-valor usando el parámetro
`simulate.p.value=TRUE`.

Por ejemplo, consideremos la situación siguiente:
```{r}
set.seed(300)
n2=100
indices.muestra2=sample(1:10000,size=n2,replace=TRUE)
muestra.test.indep2= poblacion[indices.muestra2,]
tabla.ind2=table(muestra.test.indep2$tipo,muestra.test.indep2$max.frec)
tabla.ind2
```
Si aplicamos a esta tabla la función `chisq.test`, obtenemos:
```{r,warning=TRUE}
chisq.test(tabla.ind2)
```

¡Vaya! Veamos la tabla de frecuencias esperadas:
```{r}
frec.abs.tipo2=rowSums(tabla.ind2)
frec.abs.max.frec2=colSums(tabla.ind2)
frec.esperadas2=frec.abs.tipo2%*%t(frec.abs.max.frec2)/n2
frec.esperadas2
```

Hay frecuencias esperadas inferiores a 5. Por lo tanto, lo recomendable es calcular el p-valor del test $\chi^2$ de independencia mediante simulaciones. Pero ahora tenemos que ir con cuidado en una cosa: hemos fijado la semilla de aleatoriedad para obtener una muestra de cadenas con frecuencias esperadas inferiores a 5. Lo recomendable es reiniciar esta semilla a un valor aleatorio con `set.seed(NULL)`.

```{r}
set.seed(NULL)
chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value
chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value
chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value
chisq.test(tabla.ind2,simulate.p.value=TRUE,B=5000)$p.value
```
El p-valor es sistemáticamente pequeño, lo que nos permite rechazar la hipótesis de que las variables son independientes.


## Contraste de homogeneidad {#sec:hom}

Como ya hemos dicho, la diferencia entre el contraste de homogeneidad y el de independencia está en el diseño del experimento: en cada contraste se selecciona la muestra  de una manera diferente. 

En nuestro caso, para contrastar si la distribución de probabilidades de la base de mayor frecuencia es la misma para cada tipo de cadena o no, lo que vamos a hacer es tomar una muestra aleatoria de 50 cadenas de cada tipo, juntarlas en una sola muestra *estratificada*, y aplicar el test $\chi^2$ a esta muestra. El código siguiente realiza el muestreo  en cada subpoblación de `tipo` y guarda la muestra total en el *data frame* `muestra.test.homo`. Fijamos de nuevo la semilla de aleatoriedad (otra), para que el test sea reproducible.

```{r}
set.seed(100)
```

Generamos los vectores de índices de las muestras:
```{r}
n3=50
indices.muestraA=sample(1:dim(poblacionA)[1],size=n3,replace=TRUE)
indices.muestraB=sample(1:dim(poblacionB)[1],size=n3,replace=TRUE)
indices.muestraC=sample(1:dim(poblacionC)[1],size=n3,replace=TRUE)
```
Finalmente, tomamos las filas de cada muestra y las combinamos en un *data frame*:
```{r}
muestraA.50=poblacionA[indices.muestraA,]
muestraB.50=poblacionB[indices.muestraB,]
muestraC.50=poblacionC[indices.muestraC,]
muestra.test.homo=rbind(muestraA.50,muestraB.50,muestraC.50)
str(muestra.test.homo)
```

Calculamos la tabla de contingencia de la muestra:
```{r}
tabla.homo=table(muestra.test.homo$tipo,muestra.test.homo$max.frec)
tabla.homo
```
Añadimos los márgenes:
```{r}
addmargins(tabla.homo)
```

Confirmamos que hemos tomado 50 cadenas de cada grupo. Ahora calculamos las frecuencias esperadas bajo la hipótesis nula, para comprobar si son todas mayores o iguales que 5:
```{r}
frec.abs.tipo=rowSums(tabla.homo)
frec.abs.tipo
frec.abs.max.frec=colSums(tabla.homo)
frec.abs.max.frec
frec.esperadas=frec.abs.tipo%*%t(frec.abs.max.frec)/sum(frec.abs.tipo)
frec.esperadas  
```

Todas las frecuencias son mayores o iguales que 5, así que aplicamos la función `chisq.test` sin simular el p-valor:
```{r}
chisq.test(tabla.homo)
```
El p-valor es muy pequeño, por lo que podemos rechazar  que las distribuciones de los valores de las bases de máxima frecuencia sean la misma para cada valor de la variable `tipo`. 

En definitiva, el tipo de cadena afecta a la distribución de la base de mayor frecuencia. Es la misma conclusión a la que habíamos llegado con el test de independencia, solo que ahora hemos realizado un tipo de experimento diferente.

## Potencia de un contraste $\chi^2$

La potencia de un contraste $\chi^2$, tanto de bondad de ajuste como de independencia o de homogeneidad, se puede calcular de manera similar a cómo lo hacíamos en otros tipos de contrastes de uno o dos parámetros. La instrucción para llevarlo a cabo es  `pwr.chisq.test` del paquete **pwr**. Su sintaxis básica es
```{r, eval=FALSE}
pwr.chisq.test(N=..., df=..., sig.level=..., w=..., power=...)
```

donde:
 
* `N` es el tamaño de la muestra.
* `df` es el número de grados de libertad del estadístico (recordad que  en un test de bondad de ajuste es el número de clases menos 1 y menos el número de parámetros estimados, y en un test de independencia o de homogeneidad es el número de niveles de una variable menos 1 por el número de niveles de la otra variable menos 1).
* `sig.level` es  el nivel de significación $\alpha$.
* `w` es la magnitud del efecto, que en este tipo de tests se define como $\sqrt{X^2/N}$, siendo $X^2$ el valor del estadístico de contraste y $N$ el tamaño de la muestra completa.
* `power` es  la potencia $1-\beta$.
 
Si se especifican todos estos parámetros menos uno, la función da el valor del  parámetro que falta. Normalmente, querremos saber la potencia de un contraste *a posteriori* o el tamaño de la muestra necesario para tener la potencia deseada para una magnitud del efecto esperada concreta.

Veamos algunos ejemplos de uso de esta función.

```{example}
Vamos a calcular la potencia del contraste del Ejemplo \@ref(exm:dado).
En ese ejemplo, el tamaño de la muestra fue $N=40$, el número de grados de libertad fue 5
y obtuvimos que $X^2=7.7$, por lo que la magnitud del efecto fue $w=\sqrt{7.7/40}$. Tomaremos el nivel de significación usual, $\alpha=0.05$. 


```

```{r}
library(pwr)
pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40))
```

La potencia del contraste ha sido de, aproximadamente, un `r round(100*pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40))$power)`%.

Para obtener solo la potencia, podéis usar el sufijo `$power`:

```{r}
pwr.chisq.test(N=40, df=5, sig.level=0.05, w=sqrt(7.7/40))$power
```



```{example}
Vamos a calcular la potencia del contraste de normalidad de las longitudes de los sépalos de flores iris del Ejemplo \@ref(exm:testnormiris1). En ese ejemplo el tamaño de muestra fue $N=150$; como usamos 7 clases, pero estimamos 2 parámetros, el número de grados de libertad fue  4; obtuvimos que $X^2=11.0637$, por lo que $w=\sqrt{11.0637/150}$; y ahora, por variar, tomaremos $\alpha=0.1$.


```

```{r}
pwr.chisq.test(N=150, df=4, sig.level=0.1, w=sqrt(11.0637/15))$power
```
La potencia da  `r round(pwr.chisq.test(N=150, df=4, sig.level=0.1, w=sqrt(11.0637/15))$power)`: la probabilidad de que aceptáramos que  la muestra seguía una distribución normal si no fuera verdad es prácticamente 0.



```{example}
En el contraste de homogeneidad de la Sección \@ref(sec:hom) hemos tomado tres muestras de 50 individuos cada una, en total 150 individuos. El estadístico de contraste ha valido $X^2=28.59$, por lo que la magnitud del efecto en ese test ha sido de $w=\sqrt{28.59/150}=0.4366$, entre mediana y grande según la función `cohen.ES`, que nos da las magnitudes del efecto que por convención se entienden como pequeñas, medianas o grandes para los diferentes tests considerados en el  paquete **pwr**:
  
  
```

```{r}
cohen.ES(test="chisq", size="medium")$effect.size
cohen.ES(test="chisq", size="large")$effect.size
```

¿De qué tamaño deberíamos haber tomado las muestras para garantizar una potencia del 90%, suponiendo que esperásemos una magnitud del efecto mediana y tomásemos un nivel de significación $\alpha=0.05$?



```{r}
pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9)
```
Hubiéramos necesitado como mínimo un total de unos `r round(pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9)$N)` individuos: si queríamos tomar las tres muestras del mismo tamaño, esto significa tres muestras de como mínimo `r ceiling(round(pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9)$N)/3)` individuos cada una.

Para obtener solo el tamaño de la muestra, se puede añadir el sufijo `$N`:

```{r}
pwr.chisq.test(df=6, sig.level=0.05, w=0.3, power=0.9)$N
```



## Guía rápida


 
* `table` calcula tablas de contingencia de frecuencias absolutas.

* `prop.table` calcula tablas de contingencia de frecuencias relativas.

* `addmargins` sirve para añadir  a una `table` una fila o una columna obtenidas aplicando una función a todas las columnas o a todas las filas de la tabla, respectivamente. Sus parámetros principales son:
 
    * `margin`: igualado a `1`, se aplica la función por columnas, añadiendo una nueva fila; igualado a `2`, se aplica la función por filas, añadiendo una nueva columna; 
igualado a `c(1,2)`, que es su valor por defecto,  hace ambas cosas.

    * `FUN`:  la función que se aplica a las filas o columnas; su valor por defecto es `sum`.
 

* `colSums`  calcula un vector con las sumas de las columnas de una matriz o una tabla.

* `rowSums`   calcula un vector con las sumas de las filas de una matriz o una tabla.

* `chisq.test` sirve para realizar tests $\chi^2$ de independencia y homogeneidad. El resultado es una `list` formada, entre otros, por los objetos siguientes:  `statistic` (el valor del estadístico $X^2$), `parameter` (los grados de libertad) y  `p.value` (el p-valor). Sus parámetros principales en el contexto de esta lección son:
 
    * `simulate.p.value`: igualado a `TRUE`, calcula el p-valor mediante simulaciones.
    * `B`: en este último caso, permite especificar el número de simulaciones.
 


* `pwr.chisq.test` del paquete **pwr**, sirve para calcular uno de los parámetros siguientes a partir de los otros cuatro:
 
    * `N`: el tamaño de la muestra.
    * `df`: el número de grados de libertad del contraste.
    * `sig.level`:  el nivel de significación $\alpha$.
    * `power`:  la potencia $1-\beta$.
    * `w`: la magnitud del efecto.
 

## Ejercicios

 
### Modelo de test {-}

*(1)* Hemos observado dos variables cualitativas en una muestra de una población. Cada variable tiene 3 niveles. La tabla de contingencia resultante ha sido la siguiente: 
$$
\begin{array}{c|ccc} &X&Y&Z\cr\hline A & 2 & 17 & 11\cr B & 8 & 10 & 25\cr C & 3 & 14 & 5 \end{array}
$$
¿Es verdad que, si estas variables aleatorias fueran independientes, las frecuencias esperadas de cada combinación de niveles, uno de cada variable, serían todas $\geq 5$? Tenéis que contestar SI, en mayúsculas y sin acento, o NO.

*(2)*  Hemos observado dos variables cualitativas en una muestra de una población. Una variable tiene 4 niveles y la otra 3. La tabla de contingencia resultante ha sido la siguiente: 
$$
\begin{array}{c|cccc} &A&B&C & D\cr\hline X & 50 & 19&17 & 21\cr Y &69 & 47 & 56 & 37 \cr Z &33 & 23 & 18 & 21 \end{array}
$$
Emplead la función `chisq.test` para contrastar si estas dos variables son independientes o no. Tenéis que dar el p-valor del test (redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha) y decir (contestando SI  o NO) si, con un nivel de significación $\alpha=0.1$, podríamos  rechazar la hipótesis nula de que estas dos variables son independientes. Dad las dos respuestas en este orden y separadas por un único espacio en blanco.


*(3)* Hemos realizado un test $\chi^2$ de independencia sobre una  muestra de 200 individuos,
con un nivel de significación de 0.1. Las variables objeto de estudio tenían 5 y 6 niveles, respectivamente. 
El estadístico de contraste ha valido $16.56$. ¿Cuál es el p-valor del contraste? ¿Cuál es la potencia del
contraste realizado?  Tenéis que dar ambos valores en este orden, redondeados a 3 cifras decimales sin ceros innecesarios
a la derecha,  y separados por un único espacio en blanco.




### Respuestas al test {-}

```{r, include=FALSE}
#1
A=matrix(c(2,17,11,8,10,25,3,14,5),nrow=3,byrow=T)
RS=rowSums(A)
CS=colSums(A)
frec.esperadas=RS%*%t(CS)/sum(A)
frec.esperadas
#2
X=matrix(c(50,19,17,21,69,47,56,37,33,23,18,21),nrow=3,byrow=T)
chisq.test(X)$p.value
#3
1-pchisq(16.56,(5-1)*(6-1))
pwr.chisq.test(N=200, df=(5-1)*(6-1), sig.level=0.1, w=sqrt(16.56/200))$power
```

*(1)* NO

Nosotros lo hemos mirado con

```{r,eval=FALSE}
A=matrix(c(2,17,11,8,10,25,3,14,5),nrow=3,byrow=TRUE)
RS=rowSums(A)
CS=colSums(A)
frec.esperadas=RS%*%t(CS)/sum(A)
frec.esperadas
```


*(2)* `r round(chisq.test(X)$p.value,3)` NO

Nosotros lo hemos resuelto con

```{r,eval=FALSE}
X=matrix(c(50,19,17,21,69,47,56,37,33,23,18,21),nrow=3,byrow=T)
round(chisq.test(X)$p.value,3)
```

*(3)* `r round(1-pchisq(16.56,(5-1)*(6-1)),3)` `r round(pwr.chisq.test(N=200, df=(5-1)*(6-1), sig.level=0.1, w=sqrt(16.56/200))$power,3)`

Nosotros lo hemos resuelto con

```{r,eval=FALSE}
p.valor=1-pchisq(16.56,(5-1)*(6-1))
potencia=pwr.chisq.test(N=200, df=(5-1)*(6-1), sig.level=0.1, w=sqrt(16.56/200))$power
round(c(p.valor,potencia),3)
```



<!--chapter:end:07-Independencia.Rmd-->

# Introducción a la estadística descriptiva multidimensional

En general, los datos que se recogen en  experimentos son multidimensionales: medimos varias variables aleatorias sobre una misma muestra de individuos, y organizamos esta información en tablas de datos en las que las filas representan los individuos observados y cada columna corresponde a una variable diferente. En las lecciones finales de la primera parte ya aparecieron datos cualitativos y ordinales multidimensionales,  para los que  calculamos y representamos gráficamente sus frecuencias globales y marginales; en esta lección estudiamos algunos estadísticos específicos para resumir y representar la relación existente entre diversas variables cuantitativas.

## Matrices de datos cuantitativos

Supongamos  que hemos medido los valores de $p$ variables aleatorias $X_1,\ldots,X_p$ sobre un conjunto de $n$ individuos u objetos. Es decir, tenemos $n$ observaciones de $p$ variables. En cada observación, los valores que toman estas variables forman un vector que será una realización del vector aleatorio $\underline{X}=(X_1,X_2,\ldots,X_p)$. Para trabajar con estas observaciones, las dispondremos en una tabla de datos donde cada fila corresponde a un individuo y cada columna, a una variable. En R, lo más conveniente es definir esta tabla en forma de *data frame*, pero, por conveniencia de lenguaje, en el texto de esta lección la representaremos como una matriz
$$
{X}=\begin{pmatrix}
x_{1 1} & x_{1 2} &\ldots & x_{1 p}\\
x_{2 1} & x_{2 2} &\ldots & x_{2 p}\\
\vdots & \vdots   &   \ddots    &\vdots\\ 
x_{n 1} & x_{n 2} &\ldots & x_{n p}
\end{pmatrix}.
$$
Utilizaremos las  notaciones siguientes:


* Denotaremos la $i$-ésima fila de $X$ por 
$$
{x}_{i\bullet}=(x_{i 1}, x_{i 2}, \ldots, x_{i p}).
$$
Este vector está compuesto por las observaciones de las $p$ variables sobre el $i$-ésimo individuo.
 
* Denotaremos la  $j$-ésima columna de $X$ por
$$
x_{\bullet j}=\begin{pmatrix}x_{1 j} \\ x_{2 j}\\ \vdots \\ x_{n j}
\end{pmatrix}.
$$
Esta columna está formada por todos los valores de la $j$-ésima variable, es decir, es una muestra de $X_j$.


Observad que, en cada caso, la bolita $\bullet$ en el subíndice representa el índice "variable" de los elementos del vector o de la columna.

De esta manera, podremos expresar la matriz de datos $X$ tanto por filas (individuos) como por columnas (muestras de variables):
$$
{X}=\begin{pmatrix}{x}_{1\bullet}\\x_{2\bullet}\\\vdots \\
{x}_{n\bullet}\end{pmatrix}=({x}_{\bullet1}, {x}_{\bullet 2}, \ldots, {x}_{\bullet p}).
$$

Con estas notaciones, podemos generalizar al caso multidimensional los estadísticos de una variable cuantitativa,  definiéndolos como los vectores  que se obtienen aplicando el estadístico concreto a cada columna de la tabla de datos.  Así:

* El **vector de medias** de $X$ es el vector formado por las medias aritméticas de sus columnas:
$$
\overline{X}=(\overline{{{x}}}_{\bullet1}, \overline{{x}}_{\bullet 2}, \ldots, \overline{{x}}_{\bullet p}), 
$$
donde, para cada $j=1, \ldots, p$, 
$$
\overline{{x}}_{\bullet j}=\frac{1}{n}\sum\limits_{i=1}^n x_{i j}.
$$

Observemos que
$$
\begin{array}{rl}
\overline{X} & \displaystyle = (\overline{{{x}}}_{\bullet1}, \overline{x}_{\bullet 2},\ldots,\overline{x}_{\bullet p})
= \frac{1}{n}
\Big(\sum_{i=1}^n x_{i 1}, \sum_{i=1}^n x_{i 2},\ldots,
\sum_{i=1}^n x_{i p}\Big)\\[1ex] & \displaystyle =\frac{1}{n} \sum_{i=1}^n
(x_{i 1}, x_{i 2},\ldots,x_{i p} )
=
\frac{1}{n} \sum_{i=1}^n {{x}_{i\bullet}}
\end{array}
$$
Es decir, el **vector de medias** de $X$ es la media aritmética de sus vectores fila.


* El **vector de varianzas**  de $X$ es el vector  formado por  las varianzas  de sus columnas:
$$
s^2_{X}=(s^2_{1}, s^2_2, \ldots, s^2_p), 
$$
donde 
$$
s_j^2=\frac{1}{n}\sum_{i=1}^n {(x_{ij}-\overline{{x}}_{\bullet j})^2}.
$$

* El **vector de varianzas muestrales**  de $X$ está formado por  las varianzas muestrales  de sus columnas:
$$
\widetilde{s}^2_{X}=(\widetilde{s}^2_{1}, 
\widetilde{s}^2_2, \ldots, \widetilde{s}^2_p), 
$$
donde 
$$
\widetilde{s}_j^2=\frac{1}{n-1}\sum_{i=1}^n {(x_{ij}-\overline{{x}}_{\bullet j})^2}=\frac{n}{n-1}s_j^2.
$$


* Los **vectores de desviaciones típicas** $s_{X}$ y **de desviaciones típicas muestrales** $\widetilde{s}_{X}$ de $X$ son los  formados por las desviaciones típicas y las desviaciones típicas muestrales de sus columnas, respectivamente:
$$
\begin{array}{l}
s_{X}=(s_{1}, s_2, \ldots, s_p)=(+\sqrt{\vphantom{s_p^2}{s}^2_{1}}, 
+\sqrt{\vphantom{s_p^2}{s}^2_2}, \ldots, +\sqrt{s_p^2})\\[1ex]
\widetilde{s}_{X}=(\widetilde{s}_{1}, 
\widetilde{s}_2, \ldots, \widetilde{s}_p)=(+\sqrt{\vphantom{s_p^2}\widetilde{s}^2_{1}}, 
+\sqrt{\vphantom{s_p^2}\widetilde{s}^2_2}, \ldots, +\sqrt{\widetilde{s}^2_p})
\end{array}
$$

Como en el caso unidimensional, $\overline{X}$ es un estimador insesgado de la esperanza $E(\underline{X})=\boldsymbol\mu$ del vector aleatorio $\underline{X}$ del cual $X$ es una muestra. Por lo que refiere a ${s}^2_{X}$ y $\widetilde{s}^2_{X}$, ambas son estimadores del vector de varianzas de $\underline{X}$: $\widetilde{s}^2_{X}$ es insesgado y,  cuando todas las variables aleatorias del vector son normales, ${s}^2_{X}$ es el máximo verosímil. 

Estos vectores de estadísticos se pueden calcular con R aplicando la función correspondiente al estadístico a todas las columnas de la tabla de datos. La manera más sencilla de hacerlo en un solo paso es usando la función `sapply`, si  tenemos guardada la tabla como un *data frame*, o  `apply` con `MARGIN=2`, si la tenemos guardada en forma de matriz.



```{example, label=multex0}
Consideremos la tabla de datos
$$
X=\left(\begin{array}{rrr}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{array}\right)
$$
formada por 4 observaciones de 3 variables; por lo tanto, $n=4$ y $p=3$. Vamos a guardarla en un *data frame* y a calcular sus estadísticos. 



```

```{r}
X=data.frame(V1=c(1,1,2,3),V2=c(-1,0,3,0),V3=c(3,3,0,1))
X
```

* Su vector de medias es:
```{r}
sapply(X, mean) 
```

* Su vector de varianzas muestrales es:
```{r}
sapply(X, var) 
```

* Su vector de desviaciones típicas muestrales es:
```{r}
sapply(X, sd) 
```

* Su vector de varianzas es:
```{r}
var_ver=function(x){var(x)*(length(x)-1)/length(x)} #Varianza "verdadera"
sapply(X, var_ver) 
```

* Su vector de desviaciones típicas es:
```{r}
sd_ver=function(x){sqrt(var_ver(x))} #Desv. típica "verdadera"
sapply(X, sd_ver) 
```


```{remark} 
De ahora en adelante, supondremos que todos los vectores de datos cuantitativos que aparezcan en lo que queda de lección, incluidas las columnas de tablas de datos, son no constantes y, por lo tanto, tienen desviación típica no nula.
```





## Transformaciones lineales

A veces es conveniente aplicar una transformación lineal a una tabla de datos $X$, sumando a cada columna un valor y luego multiplicando cada columna resultante por otro valor. Los dos ejemplos más comunes de    trasformación lineal son el **centrado** y la **tipificación** de datos.

Para **centrar** una matriz de datos $X$, se resta a cada columna su media aritmética:
$$
\widetilde{X}=
\begin{pmatrix}
x_{1 1}- \overline{x}_{\bullet 1}& x_{1 2}- \overline{x}_{\bullet 2} &\ldots & x_{1 p}-
\overline{x}_{\bullet p}\\
x_{2 1} - \overline{x}_{\bullet 1}& x_{2 2}- \overline{x}_{\bullet 2} &\ldots & x_{2 p}-
\overline{x}_{\bullet p}\\
\vdots & \vdots   & \ddots      &\vdots\\ 
x_{n 1} - \overline{x}_{\bullet 1}& x_{n 2}- \overline{x}_{\bullet 2} &\ldots & x_{n p}-
\overline{x}_{\bullet p}
\end{pmatrix}.
$$
Llamaremos a esta matriz la **matriz de datos centrados** de $X$.



```{example, label=multex1-1}
Consideremos de nuevo la matriz de datos del Ejemplo \@ref(exm:multex0),
$$
{X}=\left(\begin{array}{rrr}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{array}\right)
$$
  
  

```


Para centrarla, hemos de restar a cada columna su media. Ya hemos calculado estas medias hace un momento:
```{r}
sapply(X, mean) 
```


Por lo tanto, su matriz de datos centrados es
$$
\widetilde{X}=\left(\begin{array}{rrr}
`r X[1,1]`-`r sapply(X, mean)[1]`&`r X[1,2]`-`r sapply(X, mean)[2]`&`r X[1,3]`-`r sapply(X, mean)[3]`\\
`r X[2,1]`-`r sapply(X, mean)[1]`&`r X[2,2]`-`r sapply(X, mean)[2]`&`r X[2,3]`-`r sapply(X, mean)[3]`\\
`r X[3,1]`-`r sapply(X, mean)[1]`&`r X[3,2]`-`r sapply(X, mean)[2]`&`r X[3,3]`-`r sapply(X, mean)[3]`
\\
`r X[4,1]`-`r sapply(X, mean)[1]`&`r X[4,2]`-`r sapply(X, mean)[2]`&`r X[4,3]`-`r sapply(X, mean)[3]`
\end{array}\right)=
\left(\begin{array}{rrr}
`r X[1,1]- sapply(X, mean)[1]`&`r X[1,2]- sapply(X, mean)[2]`&`r X[1,3]- sapply(X, mean)[3]`\\
`r X[2,1]- sapply(X, mean)[1]`&`r X[2,2]- sapply(X, mean)[2]`&`r X[2,3]- sapply(X, mean)[3]`\\
`r X[3,1]- sapply(X, mean)[1]`&`r X[3,2]-sapply(X, mean)[2]`&`r X[3,3]- sapply(X, mean)[3]`\\
`r X[4,1]- sapply(X, mean)[1]`&`r X[4,2]- sapply(X, mean)[2]`&`r X[4,3]- sapply(X, mean)[3]`
\end{array}\right).
$$





Dado un vector de datos formado por una muestra de una variable cuantitativa, su **vector de datos tipificados**  es el vector que se obtiene restando a cada entrada la media aritmética del vector y dividiendo  el resultado por su desviación típica.  De esta manera, se obtiene un vector de datos de  media aritmética 0 y varianza 1. Tipificar un vector de datos es conveniente cuando se quiere trabajar con estos datos sin que influyan ni su media ni las unidades en los que están medidos: al dividir por su desviación típica, los valores resultantes son  adimensionales.
Por lo tanto, tipificar las variables de una tabla de datos permite compararlas dejando de lado las diferencias que pueda haber entre sus valores medios o sus varianzas. 

La **matriz tipificada**  de una matriz de datos $X$ es la matriz $Z$ que se obtiene tipificando cada columna; es decir, para tipificar una matriz de datos $X$, restamos a cada columna su media y a continuación dividimos cada columna por la
desviación típica de la columna original en $X$ (que coincide con la desviación típica de la columna "centrada", puesto que sumar o restar constantes no modifica la desviación típica):
$$
Z=\begin{pmatrix}
\frac{x_{1 1}- \overline{x}_{\bullet 1}}{s_1}& \frac{x_{1 2}- \overline{x}_{\bullet 2}}{s_2} &\ldots & \frac{x_{1 p}- \overline{x}_{\bullet p}}{s_p}\\[2ex]
\frac{x_{2 1} - \overline{x}_{\bullet 1}}{s_1}& \frac{x_{2 2}- \overline{x}_{\bullet 2}}{s_2} &\ldots & \frac{x_{2 p}- \overline{x}_{\bullet p}}{s_p}\\[2ex]
\vdots & \vdots   & \ddots      &\vdots\\ 
\frac{x_{n 1} - \overline{x}_{\bullet 1}}{s_1}& \frac{x_{n 2}- \overline{x}_{\bullet 2}}{s_2} &\ldots & \frac{x_{n p}-
\overline{x}_{\bullet p}}{s_p}
\end{pmatrix}.
$$



```{example, label=multex1-3}
Vamos a tipificar  a mano la tabla de datos 
$$
{X}=\left(\begin{array}{rrr}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{array}\right)
$$
del Ejemplo \@ref(exm:multex0). Ya la hemos centrado en el Ejemplo \@ref(exm:multex1-1). Para tipificarla, tenemos que dividir cada columna de esta matriz centrada por la desviación típica de la columna correspondiente en la matriz original. Hemos calculado estas desviaciones típicas en el Ejemplo \@ref(exm:multex0):
  
  
  
```  

```{r}
sapply(X, sd_ver) 
```


Dividiendo cada columna de la matriz centrada $\widetilde{X}$ por la correspondiente desviación típica obtenemos:

$$
\begin{array}{rl}
Z & =\left(\begin{array}{rrr}
`r X[1,1]- sapply(X, mean)[1]`/`r sapply(X, sd_ver)[1]`&`r X[1,2]- sapply(X, mean)[2]`/`r sapply(X, sd_ver)[2]`&`r X[1,3]- sapply(X, mean)[3]`/`r sapply(X, sd_ver)[3]`\\
`r X[2,1]- sapply(X, mean)[1]`/`r sapply(X, sd_ver)[1]`&`r X[2,2]- sapply(X, mean)[2]`/`r sapply(X, sd_ver)[2]`&`r X[2,3]- sapply(X, mean)[3]`/`r sapply(X, sd_ver)[3]`\\
`r X[3,1]- sapply(X, mean)[1]`/`r sapply(X, sd_ver)[1]`&`r X[3,2]-sapply(X, mean)[2]`/`r sapply(X, sd_ver)[2]`&`r X[3,3]- sapply(X, mean)[3]`/`r sapply(X, sd_ver)[3]`\\
`r X[4,1]- sapply(X, mean)[1]`/`r sapply(X, sd_ver)[1]`&`r X[4,2]- sapply(X, mean)[2]`/`r sapply(X, sd_ver)[2]`&`r X[4,3]- sapply(X, mean)[3]`/`r sapply(X, sd_ver)[3]`
\end{array}\right)\\
& =
\left(\begin{array}{rrr}
`r format(round((X[1,1]- sapply(X, mean)[1])/sapply(X, sd_ver)[1],7), nsmall = 7)`&`r format(round((X[1,2]- sapply(X, mean)[2])/sapply(X, sd_ver)[2],7), nsmall = 7)`&`r (X[1,3]- sapply(X, mean)[3])/sapply(X, sd_ver)[3]`\\
`r format(round((X[2,1]- sapply(X, mean)[1])/sapply(X, sd_ver)[1],7), nsmall = 7)`&`r (X[2,2]- sapply(X, mean)[2])/sapply(X, sd_ver)[2]`&`r (X[2,3]- sapply(X, mean)[3])/sapply(X, sd_ver)[3]`\\
`r (X[3,1]- sapply(X, mean)[1])/sapply(X, sd_ver)[1]`&`r (X[3,2]-sapply(X, mean)[2])/sapply(X, sd_ver)[2]`&`r (X[3,3]- sapply(X, mean)[3])/sapply(X, sd_ver)[3]`\\
`r (X[4,1]- sapply(X, mean)[1])/sapply(X, sd_ver)[1]`&`r (X[4,2]- sapply(X, mean)[2])/sapply(X, sd_ver)[2]`&`r (X[4,3]- sapply(X, mean)[3])/sapply(X, sd_ver)[3]`
\end{array}\right)
\end{array}
$$




La manera más sencilla de aplicar con R  una transformación lineal a una tabla de datos $X$, y en particular de centrarla o tipificarla, es usando la instrucción 
```{r,eval=FALSE}
scale(X, center=..., scale=...)
```
donde:

* `X`  puede ser tanto  una matriz como un *data frame*; el resultado será siempre una matriz.


*  El valor del parámetro `center` es el vector que restamos a sus columnas, en el sentido de que cada entrada de este vector se restará a todas las entradas de la columna correspondiente.  Su valor por defecto (que no es necesario especificar, aunque también se puede especificar con `center=TRUE`) es el vector $\overline{X}$ de medias de $X$; para especificar que no se reste nada, podemos usar `center=FALSE`. 

* El valor del parámetro `scale` es el vector por el que dividimos las columnas  de $X$:cada columna se divide por la entrada correspondiente de este vector.Su valor por defecto (de nuevo, se puede especificar igualando el parámetro a `TRUE`) es el vector  $\widetilde{s}_X$ de  desviaciones típicas *muestrales*; para especificar que no se divida por nada, podemos usar `scale=FALSE`. 


En particular, la instrucción  `scale(X)` centra la tabla de datos $X$ y divide sus columnas por sus *desviaciones típicas muestrales*; por lo tanto, no la tipifica según nuestra definición, ya que no las divide por sus desviaciones típicas "verdaderas". 

```{example, label=multex1}
Recordemos la tabla de datos $X$ del Ejemplo \@ref(exm:multex0).



```

```{r}
X
```
Su matriz centrada es:
```{r}
X_centrada=scale(X, center=TRUE, scale=FALSE)
X_centrada
```
Coincide con la matriz obtenida en el Ejemplo \@ref(exm:multex1-1).

Observad la estructura del resultado: en primer lugar nos da la matriz centrada, y a continuación nos dice que tiene un atributo llamado `"scaled:center"` cuyo valor es el vector usado para centrarla. Este atributo no interferirá para nada en las operaciones que realicéis con la matriz centrada, pero, si os molesta, recordad que se puede eliminar sustituyendo el resultado de centrar la matriz en los puntos suspensivos de la instrucción siguiente:
```{r,eval=FALSE}
attr(... , "scaled:center")=NULL
```

En nuestro ejemplo:
```{r}
attr(X_centrada, "scaled:center")=NULL
X_centrada
```

Como ya hemos avisado, para tipificar esta tabla de datos  *no* podemos hacer lo siguiente: 
```{r}
X_tip=scale(X)
X_tip
```

Para hacerlo bien según la definición que hemos dado, tenemos dos opciones. Una es  multiplicar la matriz anterior por $\sqrt{n/(n-1)}$, donde $n$ es el número de filas de la tabla. (El motivo es que, como $\widetilde{s}_X=\sqrt{\frac{n}{n-1}}\cdot s_X$, se tiene que $\frac{1}{s_X}=\sqrt{\frac{n}{n-1}}\cdot \frac{1}{\widetilde{s}_X}$; por lo tanto, si queríamos dividir por $s_X$ y `scale(X)` ha dividido por $\widetilde{s}_X$, basta multiplicar su resultado por $\sqrt{\frac{n}{n-1}}$ para obtener el efecto deseado.)

```{r}
n=dim(X)[1]  #Número de filas de X
X_tip=scale(X)*sqrt(n/(n-1)) 
X_tip
```
Ahora sí que coincide con la matriz obtenida "a mano" en el Ejemplo \@ref(exm:multex1-3).

Otra posibilidad es usar, como valor del parámetro `scale`, el vector  $s_X$ de desviaciones típicas de las columnas.
```{r}
X_tip1=scale(X, scale=sapply(X, sd_ver)) 
X_tip1
```

Observaréis que la matriz resultante es la misma, pero el atributo que indica el vector por el que hemos dividido las columnas es diferente: en este caso, es el de desviaciones típicas.
Ahora, en ambos casos, podemos usar la función `attr` para eliminar los dos atributos, `"scaled:center"` y `"scaled:scale"`, que se han añadido a la matriz tipificada. Por ejemplo:
```{r}
attr(X_tip, "scaled:center")=NULL
attr(X_tip, "scaled:scale")=NULL
X_tip
```



## Covarianzas y correlaciones

La **covarianza** entre dos variables es una medida de la tendencia que tienen ambas variables a variar conjuntamente. Cuando la covarianza es positiva, si una de las dos variables crece o decrece, la otra tiene el mismo comportamiento; en cambio, cuando la covarianza es negativa, esta tendencia se invierte: si una variable crece, la otra decrece y viceversa. Puesto que interpretar el valor de la covarianza más allá de su signo es difícil, se suele usar una versión "normalizada" de la misma, la **correlación de Pearson**, que mide de manera más precisa la relación lineal entre dos variables. 

La covarianza generaliza la varianza, en el sentido de que la varianza  de una variable es su covarianza consigo misma. Y como en el caso de la varianza, definiremos dos versiones de la covarianza: la **"verdadera"** y la **muestral**. La diferencia estará de nuevo en el denominador.


Formalmente, la **covarianza** de las variables ${x}_{\bullet i}$ y
${x}_{\bullet j}$ de una matriz de datos $X$ es
$$
s_{i j}=\frac{1}{n} \sum_{k =1}^n\big((x_{k i}-\overline{{x}}_{\bullet i})(x_{kj}-\overline{{x}}_{\bullet j})\big)= 
\frac{1}{n} \Big(\sum_{k =1}^n x_{k i} x_{k j}\Big) - \overline{{x}}_{\bullet i} \overline{{x}}_{\bullet j},
$$
y  su **covarianza muestral**  es
$$
\widetilde{s}_{ij} =
\frac{1}{n-1} \sum_{k =1}^n\big((x_{k i}-\overline{{x}}_{\bullet i})(x_{kj}-\overline{{x}}_{\bullet j})\big)= 
\frac{n}{n-1} s_{ij}.
$$


El estadístico $\tilde{s}_{ij}$ es siempre un estimador insesgado de la covarianza $\sigma_{i j}$ de las variables aleatorias $X_i$ y $X_j$ de las que ${x}_{\bullet i}$ y ${x}_{\bullet j}$ son muestras, mientras que  $s_{i j}$ es su estimador máximo verosímil  cuando la distribución conjunta de $X_i$ y $X_j$ es [normal bivariante](http://es.wikipedia.org/wiki/Distribución_normal_multivariante}).


Es inmediato comprobar a partir de sus definiciones que ambas covarianzas son simétricas, y que la covarianza de una variable consigo misma es su varianza:
$$ 
s_{i j}= s_{j i}, \quad \widetilde{s}_{i j}= \widetilde{s}_{j i}, \quad
s_{i i}=s_{i}^2, \quad \widetilde{s}_{ii}=\widetilde{s}_i^2.
$$

```{example,label=multex-cov-vect1}
La covarianza de las dos primeras columnas de la matriz de datos
$$
X=\left(\begin{array}{rrr}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{array}\right)
$$
del Ejemplo \@ref(exm:multex0) se calcularía de la manera siguiente:
  

```  

$$
s_{12}=\frac{1}{4}(`r X[1,1]`\cdot (`r X[1,2]`)+`r X[2,1]`\cdot `r X[2,2]`+`r X[3,1]`\cdot `r X[3,2]`+`r X[4,1]`\cdot `r X[4,2]`)-`r mean(X[,1])`\cdot `r mean(X[,2])`=
  `r sum(X[,1]*X[,2])/4`-`r mean(X[,1])* mean(X[,2])`=`r (sum(X[,1]*X[,2]))/4- mean(X[,1])* mean(X[,2])`
$$
Su covarianza muestral se obtendría multiplicando por $4/3$ este valor:
$$
\widetilde{s}_{12} = \frac{4}{3} s_{12}=`r (sum(X[,1]*X[,2])/4- mean(X[,1])* mean(X[,2]))*(4/3)`.
$$




La covarianza *muestral* de dos vectores numéricos de la misma longitud $n$ se puede calcular con R mediante la función `cov`.
Para obtener su covarianza "verdadera", hay que multiplicar el resultado de `cov` por $(n-1)/n$.

```{example}
La covarianza muestral de las dos primeras columnas de la tabla de datos $X$, que tenemos guardada en el *data frame* `X`, es:


```

```{r}
cov(X$V1, X$V2)
```

y su covarianza "verdadera" es:
```{r}
n=dim(X)[1]
((n-1)/n)*cov(X$V1, X$V2) 
```

Queremos recalcar que, como en el caso de la varianza con `var`, R calcula con `cov` la versión muestral de la covarianza. 

Las **matrices de covarianzas**  y de  **covarianzas muestrales**  de una tabla de datos $X$ son, respectivamente, 
$$
{S}=
\begin{pmatrix}  
 s_{1 1} & s_{1 2} & \ldots & s_{1 p}\\
 s_{2 1} & s_{2 2} & \ldots & s_{2 p}\\
  \vdots & \vdots  &   \ddots     & \vdots\\
 s_{p 1} & s_{p 2} & \ldots & s_{p p}
\end{pmatrix},\ 
\widetilde{{S}}=
\begin{pmatrix}  
 \widetilde{s}_{1 1} & \widetilde{s}_{1 2} & \ldots & \widetilde{s}_{1 p}\\
 \widetilde{s}_{2 1} & \widetilde{s}_{2 2} & \ldots & \widetilde{s}_{2 p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
\widetilde{s}_{p 1} & \widetilde{s}_{p 2} & \ldots & \widetilde{s}_{p p}
\end{pmatrix},
$$
donde cada $s_{i j}$ y cada $\widetilde{s}_{i j}$ son, respectivamente, la covarianza  y la covarianza muestral  de las correspondientes columnas ${x}_{\bullet i}$ y ${x}_{\bullet j}$.
Estas matrices de covarianzas  miden la tendencia a la variabilidad conjunta de
los datos de $X$ y, si $n$ es el número de filas de $X$, se tiene que
$$
S=\frac{n-1}{n}\widetilde{{S}}.
$$

La matriz de covarianzas muestrales $\widetilde{{S}}$ es un estimador insesgado de la matriz de covarianzas $\Sigma$ del vector de variables aleatorias $\underline{X}$, y si este tiene distribución normal multivariante, $S$ es un estimador máximo verosímil de $\Sigma$.
Ambas matrices de covarianzas son simétricas, puesto que $s_{i j}=s_{j i}$, y tienen  todos sus valores propios  $\geq 0$. 

La matriz de covarianzas *muestrales* de una tabla de datos se calcula aplicando la función `cov` al *data frame* o a la matriz que contenga dicha tabla.  Para obtener su matriz de covarianzas "verdaderas", es suficiente multiplicar el resultado de  `cov` por $(n-1)/n$, donde $n$ es el número de filas de la tabla de datos.



```{example,label=multex1-cov}
Continuemos con la matriz 
$$
X=\left(\begin{array}{rrr}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{array}\right)
$$
del Ejemplo \@ref(exm:multex0).


```

<!--
Vamos a calcular a mano su matriz de covarianzas, luego la calcularemos con R.
Para realizar los cálculos a mano, es útil organizar los datos y los cálculos intermedios necesarios en una tabla como la siguiente:
$$
\begin{array}{|c||c|c|c|c|c|c|c|c|c|}
\hline
i&{x}_{\bullet 1}&{x}_{\bullet 2}&{x}_{\bullet 3}&{x}_{\bullet 1}^2&{x}_{\bullet 2}^2&{x}_{\bullet 3}^2&{x}_{\bullet 1}{x}_{\bullet 2}&{x}_{\bullet 1}{x}_{\bullet 3}&{x}_{\bullet 2}{x}_{\bullet 3}
\\\hline\hline
1&1&-1&3&1&1&9&-1&3&-3\\
2&1&0&3&1&0&9&0&3&0\\
3&2&3&0&4&9&0&6&0&0\\
4&3&0&1&9&0&1&0&3&0\\\hline
Suma&7&2&7&15&10&19&5&9&-3\\\hline
Media &{7}/{4} & {2}/{4} & {7}/{4} & {15}/{4} & {10}/{4} & {19}/{4} & {5}/{4} & {9}/{4} & -{3}/{4}\\\hline
\end{array}
$$


Así tenemos que 
$$
\begin{array}{l}
\displaystyle 
s_1^2=\frac{1}{4}\Big(\sum_{i=1}^4 x_{i 1}^2\Big)-\overline{x}_{\bullet 1}^2=\frac{15}{4}
-\left( \frac{7}{4}\right)^2=\frac{11}{16}=0.6875\\[2ex]
\displaystyle 
s_2^2=\frac{1}{4}\Big(\sum_{i=1}^4 x_{i 2}^2\Big)-\overline{x}_{\bullet 2}^2=\frac{10}{4} -\left(
\frac{2}{4}\right)^2=\frac{9}{4}=2.25\\[2ex]
\displaystyle 
s_3^2=\frac{1}{4}\Big(\sum_{i=1}^4 x_{i 3}^2\Big)-\overline{x}_{\bullet 3}^2=\frac{19}{4}
-\left( \frac{7}{4}\right)^2=\frac{27}{16}=1.6875\\
\displaystyle 
s_{1 2}=\frac{1}{4}\Big(\sum_{i=1}^n x_{i 1} x_{i 2}\Big) -\overline{x}_{\bullet 1}
\overline{x}_{\bullet 2}=  \frac{5}{4}-\frac{7}{4}\cdot \frac{2}{4}=\frac{3}{8}=0.375\\[2ex]
\displaystyle 
s_{1 3}=\frac{1}{4}\Big(\sum_{i=1}^n x_{i 1} x_{i 3}\Big) -\overline{x}_{\bullet 1}
\overline{x}_{\bullet 3}=  \frac{9}{4}-\frac{7}{4}\cdot  \frac{7}{4}=-\frac{13}{16}=-0.8125\\[2ex]
\displaystyle 
s_{2 3}=\frac{1}{4}\Big(\sum_{i=1}^n x_{i 2} x_{i 3}\Big) -\overline{x}_{\bullet 2}
\overline{x}_{\bullet 3}=  \frac{-3}{4}-\frac{2}{4}\cdot  \frac{7}{4}=-\frac{13}{8}=-1.625
\end{array}
$$
Y por lo tanto, la matriz de covarianzas es 
$$
{S}= \left(\begin{array}{rrr}
0.6875 & 0.375& -0.8125 \\
0.375 & 2.25  & -1.625\\
   -0.8125 & -1.625&  1.6875
 \end{array}\right).
$$
-->


Su matriz de covarianzas muestrales es
```{r}
cov(X)  
```
y su matriz de covarianzas es
```{r}
n=dim(X)[1]
((n-1)/n)*cov(X)  
```

Como la matriz de covarianzas es difícil de interpretar  como medida de variabilidad de una tabla de datos, debido a que no es una única cantidad sino toda una matriz, interesa cuantificar  esta variabilidad mediante un único índice. No hay consenso sobre este índice, y entre los que se usan destacamos:

* La **varianza total** de $X$: la suma de las varianzas de sus columnas.

* La **varianza media** de $X$: la media de las varianzas de sus columnas, es decir, la varianza total partida por el número de columnas.

* La **varianza generalizada** de $X$: el determinante de su matriz de covarianzas. 

* La  **desviación típica generalizada** de $X$:
la raíz cuadrada positiva de su varianza generalizada. 

De cada uno de estos índices se definen, naturalmente, una versión muestral y una "verdadera", según el tipo de varianzas o covarianzas que se usen en su cálculo.

Pasemos ahora a la **correlación lineal de Pearson** (o, de ahora en adelante, simplemente **correlación de Pearson**) de dos variables ${x}_{\bullet i}$ y ${x}_{\bullet j}$ de $X$, que se define como
$$
r_{i j}=\frac{s_{i j}}{s_i\cdot s_j}.
$$
Observad que 
$$
\frac{\widetilde{s}_{i j}}{\widetilde{s}_i\cdot \widetilde{s}_j}=
\frac{\frac{n}{n-1}\cdot {s}_{i j}}{\sqrt{\frac{n}{n-1}}\cdot {s}_i \cdot\sqrt{\frac{n}{n-1}}\cdot{s}_j}=
\frac{s_{i j}}{s_i \cdot s_j}=r_{i j},
$$
y, por lo tanto, esta correlación se puede calcular también a partir de las versiones muestrales de la covarianza y las desviaciones típicas  por medio de la misma fórmula. 

El estadístico $r_{ij}$ es un estimador máximo verosímil de la correlación de Pearson  $\rho_{i
j}=Cor(X_i,X_j)$ de las variables aleatorias $X_i$ y $X_j$
cuando su distribución conjunta es normal bivariante, y aunque es sesgado,  su sesgo  tiende a 0 cuando $n$ tiende a $\infty$.
Las propiedades más importantes de $r_{i,j}$ son las siguientes:

* Es simétrica: $r_{i j}=r_{j i}$.

* $-1\leq r_{i j}\leq 1$.

* $r_{i i}=1$.

* $r_{i j}$ tiene el mismo signo que $s_{i j}$.

* $r_{i j}=\pm 1$ si y, sólo si, existe una relación lineal perfecta entre las
variables ${x}_{\bullet i}$ y ${x}_{\bullet j}$: es decir, si, y sólo si, existen valores $a, b\in \mathbb{R}$ tales que 
$$
\left(\begin{array}{c}
x_{1j}\\  \vdots \\ x_{nj}\end{array}\right)=
a\cdot \left(\begin{array}{c}
x_{1i}\\ \vdots \\ x_{ni}\end{array}\right) +b.
$$
La pendiente $a$ de esta relación lineal tiene el mismo signo
que $r_{i j}$.

* El coeficiente de determinación $R^2$ de la regresión lineal por mínimos cuadrados de  ${x}_{\bullet j}$ respecto de ${x}_{\bullet i}$ 
es igual al cuadrado de su correlación de Pearson, $r_{i j}^2$; por lo tanto, cuánto más se aproxime el valor absoluto de $r_{ij}$  a 1,
más se acercan las
variables ${x}_{\bullet i}$ y ${x}_{\bullet j}$ a depender linealmente la una de la otra.


Así pues, la correlación de Pearson entre dos variables viene a ser una covarianza "normalizada", ya que, como vemos, su valor está entre -1 y 1, y  mide la tendencia de las variables a estar relacionadas según una función lineal. En concreto, cuanto más se acerca dicha correlación  a 1 (respectivamente, a -1), más se acerca una (cualquiera) de las variables a ser función lineal creciente (respectivamente, decreciente) de la otra.  

Con R, la correlación de Pearson de dos vectores se puede calcular aplicándoles la función `cor`.




```{example} 
En ejemplos anteriores hemos calculado la covarianza y las varianzas de las dos primeras columnas de la matriz de datos
$$
{X}=\left(\begin{array}{rrr}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{array}\right)
$$


```

Hemos obtenido los valores siguientes
$$
s_{12}=`r cov(X[,1],X[,2])*(3/4)`,\quad s_1=`r sqrt(var(X[,1])*(3/4))`,\quad s_2=`r  sqrt(var(X[,2])*(3/4))`.
$$
Por lo tanto, su correlación de Pearson es
$$
r_{1 2}=\frac{`r cov(X[,1],X[,2])*(3/4)`}{`r sqrt(var(X[,1])*(3/4))`\cdot `r  sqrt(var(X[,2])*(3/4))`}=`r (cov(X[,1],X[,2])*(3/4))/(sqrt(var(X[,1])*(3/4))*sqrt(var(X[,2])*(3/4)))`.
$$



Ahora vamos a calcularla con R, y aprovecharemos para confirmar su relación con el valor de $R^2$ de la regresión lineal de la segunda columna respecto de la primera. Recordemos que esta tabla de datos sigue guardada en el *data frame* `X`.
```{r}
X
```

La correlación de sus dos primeras columnas es:

```{r}
cor(X$V1, X$V2)
```
que coincide con el valor obtenido "a mano". Comprobemos ahora que su cuadrado es igual al valor de $R^2$ de la regresión lineal:
```{r}
cor(X$V1, X$V2)^2
summary(lm(X$V2~X$V1))$r.squared
```


La **matriz de correlaciones de  Pearson** de $X$ es 
$$
{R}=
\begin{pmatrix}
1 & r_{1 2} & \ldots & r_{1 p}\\
r_{2 1} & 1 & \ldots & r_{2 p}\\
\vdots & \vdots & \ddots & \vdots\\
r_{p 1} & r_{p 2} & \ldots & 1
\end{pmatrix},
$$
donde cada $r_{i j}$ es la correlación de Pearson de las  columnas correspondientes de  $X$. Esta matriz de correlaciones tiene siempre determinante entre 0 y 1 (ambos extremos incluidos) y  todos sus valores propios son mayores o iguales que 0, y  con R se puede calcular aplicando la misma instrucción `cor` a la tabla de datos, sea en forma de matriz o de *data frame*.

Así, la matriz de correlaciones de nuestra tabla de datos $X$ es:

```{r}
cor(X)
```



Se tiene el teorema siguiente, que se puede demostrar mediante un simple, aunque farragoso, cálculo algebraico:

```{theorem}
La matriz de correlaciones de Pearson de $X$ es igual a:

* La matriz de covarianzas de su matriz tipificada.

* La matriz de covarianzas muestrales de su matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales en vez de por las "verdaderas".

 
```


La importancia de este resultado es que, si la tabla de datos es muy grande, suele ser más eficiente calcular la matriz de covarianzas de su matriz tipificada que la matriz de correlaciones de Pearson de la tabla original. 

Observad, por otro lado, que las dos matrices de covarianzas mencionadas en el enunciado coinciden, puesto que la matriz tipificada se obtiene multiplicando por $\sqrt{n/(n-1)}$ la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Esto implica que la matriz de covarianzas muestrales de la matriz tipificada se obtiene multiplicando por $n/(n-1)$ la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales. Finalmente, la matriz de covarianzas se obtiene a partir de la de covarianzas muestrales multiplicándola por $(n-1)/n$. Entonces, los factores $n/(n-1)$ y $(n-1)/n$ se compensan y resulta que la matriz de covarianzas de la matriz tipificada coincide con la matriz de covarianzas muestrales de la matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales.

Recordemos que si aplicamos la función `scale` a una tabla de datos $X$, la tipifica dividiendo por las desviaciones típicas muestrales. Por lo tanto, otra manera de reformular el teorema anterior es decir que 

<center>
`cor(X)` *da lo mismo que* `cov(scale(X))`.

</center>

Comprobemos esta igualdad para nuestra matriz de datos $X$. 

```{r}
cor(X)
cov(scale(X))
```

Cuando se calcula la covarianza o la correlación de Pearson de dos vectores que contienen valores NA, lo usual es no tenerlos en cuenta: es decir, si un vector contiene un NA en una  posición, se eliminan de los dos vectores sus entradas en dicha posición. De esta manera, se tomaría como covarianza de
$$
\left(\begin{array}{c}
1\\ 2\\ NA\\ 4\\ 6\\ 2\end{array}\right)\mbox{ y }
\left(\begin{array}{c} 2\\ 4\\ -3\\ 5\\ 7\\ NA \end{array}\right)
$$
la de
$$
\left(\begin{array}{c}1\\ 2\\ 4\\ 6\end{array}\right)\mbox{ y }
\left(\begin{array}{c} 2\\ 4\\ 5\\ 7\end{array}\right).
$$

Como ya nos pasaba con las funciones de estadística descriptiva univariante com `mean` o `var`, cuando aplicamos `cov` o `cor` a un par de vectores que contengan entradas NA, obtenemos por defecto NA. En las funciones univariantes usábamos `na.rm=TRUE` para pedir a R que  obviara los NA, pero esta solución ahora no es posible, porque las posiciones de los NA también cuentan, y si los borramos tal cual se desmonta el emparejamiento de los datos. Así que, si se quiere que R calcule el valor de `cov` o `cor` sin tener en cuenta los NA, se ha de especificar  añadiendo el parámetro `use="complete.obs"`, que le indica que ha de usar las observaciones completas, es decir, las posiciones que no tienen NA en ninguno de los dos vectores.

Veamos el efecto sobre los dos vectores anteriores. Llamémosles $x$ e $y$, y sean $x_1$ e $y_1$ los vectores que se obtienen eliminando las entradas que contienen un NA en alguno de los dos vectores.

```{r}
x=c(1,2,NA,4,6,2)
y=c(2,4,-3,5,7,NA)
x1=x[is.na(x)!=TRUE & is.na(y)!=TRUE]  
y1=y[is.na(x)!=TRUE & is.na(y)!=TRUE]  
x1
y1
```

Si calculamos la covarianza de $x$ e $y$ tal cual con la función `cov`, da NA:

```{r}
cov(x, y)
```

Usando `use="complete.obs"`, obtenemos la covarianza de $x_1$ e $y_1$:

```{r}
cov(x, y, use="complete.obs")
cov(x1, y1)
```

Lo mismo sucede con la función `cor`:

```{r}
cor(x, y)
cor(x, y, use="complete.obs")
cor(x1, y1)
```

Al calcular las matrices de covarianzas o correlaciones de una tabla de datos que contenga valores NA, se suele seguir una de las dos estrategias siguientes, según lo que interese al usuario:

* Para cada par de columnas, se calcula su covarianza o su correlación 
con la estrategia explicada más arriba para dos vectores, obviando el hecho de que forman parte de una tabla de datos mayor; es decir, al efectuar el cálculo para cada par de columnas concreto, se eliminan de cada una de ellas solo las entradas de las filas en las que alguna de las dos tiene un NA, sin tener en cuenta para nada las otras columnas.  Esta opción se especifica dentro de la función `cov` o `cor` con el parámetro `use="pairwise.complete.obs"`.

* Antes de nada, se eliminan  las filas de la tabla que contienen algún NA en alguna columna, dejando solo en la tabla las filas "completas", las que no contienen ningún NA. Luego se calcula la matriz de covarianzas o de correlaciones  de  la tabla resultante. Esta opción se especifica con el parámetro   `use="complete.obs"`. 

Veamos un ejemplo. Consideremos la matriz de datos $Y$ siguiente, cuyas dos primeras columnas son los vectores $x$ e $y$ anteriores:
```{r}
Y=cbind(c(1,2,NA,4,6,2), c(2,4,-3,5,7,NA), c(-2,1,0,2,NA,0))
Y
```

Supongamos que queremos calcular su matriz de correlaciones de Pearson.
Como todas las filas de $Y$ tienen entradas NA, todas las correlaciones fuera de la diagonal dan NA (R sabe que la correlación de un columna consigo misma siempre es 1, y ya no la calcula):
```{r}
cor(Y)
```

Una opción es calcular las correlaciones de Pearson de cada par de variables eliminando sus valores NA pero sin tener en cuenta los posibles valores NA de la otra variable:
```{r}
cor(Y, use="pairwise.complete.obs")
```

Observad que la entrada (1,2) de esta matriz es la correlación de los vectores  $x$ e $y$ calculada con  `use="complete.obs"`. 

Calculemos ahora la matriz de correlaciones de Pearson de la matriz con filas completas:

```{r}
cor(Y, use="complete.obs")
```

Veamos que efectivamente coincide con la matriz de correlaciones de Pearson de la matriz que se obtiene eliminando las filas que contienen algún NA. Esta matriz es:

```{r}
noNAs=is.na(Y[,1])!=TRUE & is.na(Y[,2])!=TRUE & is.na(Y[,3])!=TRUE
Y1=Y[noNAs,] 
Y1
```

y su matriz de correlaciones de Pearson es:

```{r}
cor(Y1)
```

## Correlación de Spearman


La correlación de Pearson mide específicamente la tendencia de dos variables cuantitativas continuas a depender linealmente una de otra. En circunstancias en las que no esperemos esta dependencia lineal, o en las que nuestras variables sean cuantitativas discretas o simplemente cualitativas, usar la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la **correlación de Spearman**. Este índice asigna a cada valor de cada vector su **rango** (su posición en el vector ordenado de menor a mayor, y en caso de empates la media de las posiciones que ocuparían todos los empates) y calcula la correlación de Pearson de estos rangos. Con R, la correlación de Spearman se calcula directamente con la función `cor` entrándole el parámetro `method="spearman"`. (El valor por defecto del parámetro `method` es `"pearson"` y por eso no lo indicamos cuando calculamos la correlación de Pearson.)

```{example} 
Vamos a calcular la correlación de Spearman de las dos primeras columnas de la matriz de datos $X$ que hemos venido usando en nuestros ejemplos. En la tabla siguiente calculamos los rangos de sus entradas:
  
  
  
```

$$
\begin{array}{|c|c|c|c|}
\hline
{x}_{\bullet 1}& rango & {x}_{\bullet 2}& rango
\\\hline\hline
`r X[1,1]`& `r rank(X$V1)[1]` & `r X[1,2]`& `r rank(X$V2)[1]` \\
`r X[2,1]`&`r rank(X$V1)[2]` & `r X[2,2]` & `r rank(X$V2)[2]`\\
`r X[3,1]`&`r rank(X$V1)[3]` & `r X[3,2]`& `r rank(X$V2)[3]` \\
`r X[4,1]`&`r rank(X$V1)[4]` & `r X[4,2]`&  `r rank(X$V2)[4]`\\\hline
\end{array}
$$
¿Cómo hemos obtenido los rangos? Fijaos por ejemplo en la primera columna: los dos 1 ocuparían la posición 1 y 2, les asignamos a ambos como rango la media de estas posiciones, 1.5; el 2 ocuparía la posición 3 y el 3 ocuparía la posición 4, y estos son también sus rangos. Dejamos como ejercicio que comprobéis los rangos de los elementos de $x_{\bullet 2}$.


Con R estos rangos se calculan con la función `rank`. Así, los rangos de los elementos de $x_{\bullet 1}$ son

```{r,echo=FALSE}
X=data.frame(V1=c(1,1,2,3),V2=c(-1,0,3,0),V3=c(3,3,0,1))
```

```{r}
rank(X$V1)
```

y los de los elementos de $x_{\bullet 2}$ son

```{r}
rank(X$V2)
```


Por lo tanto, la correlación de Spearman de
$$
(1,1,2,3)\mbox{ y }(-1,0,3,0)
$$
es la correlación de Pearson de
$$
(`r rank(X$V1)`)\mbox{ y }(`r rank(X$V2)`)
$$

Veámoslo:

```{r}
cor(X$V1,X$V2,method="spearman")
cor(rank(X$V1),rank(X$V2))
```


## Contrastes de correlación


Como ya hemos comentado, podemos usar la correlación de Pearson $r_{xy}$ de dos vectores $x$ e $y$, formados por los valores de dos variables cuantitativas $X,Y$ medidos sobre una misma muestra de individuos, para estimar la correlación $\rho_{XY}$ de estas variables poblacionales. Cuando además ambas variables aleatorias son normales, disponemos de una fórmula para calcular  intervalos de confianza para la correlación poblacional y de un método para efectuar contrastes de hipótesis con hipótesis nula $H_0: \rho_{XY}=0$ ("no hay correlación entre $X$ e $Y$"). No vamos a entrar en los detalles de las fórmulas ni de los teoremas en que se basan, pero es importante que recordéis que la función de R que lleva a cabo dichos contrastes "de correlación" es la función `cor.test`. En particular, esta función calcula el intervalo de confianza asociado a un contraste de estos: si el contraste es bilateral, es decir, con hipótesis alternativa $H_1: \rho_{XY}\neq 0$,   el intervalo que produce esta función es el intervalo de confianza usual para $\rho_{XY}$ con nivel de confianza correspondiente al nivel de significación del contraste. 

La sintaxis de `cor.test`   es la misma que la del resto de funciones para realizar contrastes de hipótesis básicos:
```{r, eval=FALSE}
cor.test(x, y, alternative=..., conf.level=...)
```
donde `x` e `y` son los dos vectores de datos, que también se pueden especificar mediante una fórmula. Estos dos vectores han de tener la misma longitud, puesto que se entiende que son mediciones sobre el mismo conjunto de individuos. El parámetro `alternative` puede tomar los tres  valores usuales y su valor por defecto es, como siempre, `"two.sided"`, que corresponde al contraste bilateral, con hipótesis alternativa $H_1: \rho_{XY}\neq 0$. Los valores `alternative="greater"` y  `alternative="less"` permiten contrastar si $X$ e $Y$ tienen correlación mayor o menor que 0, respectivamente.

Como en el resto de funciones de contrastes, el resultado es una `list` que, entre otros objetos, contiene:


*  `p.value`: El p-valor del test.

*  `conf.int`: Un intervalo de confianza del nivel de confianza especificado.

* `estimate`: El valor de la correlación de Pearson (calculado con `use="complete.obs"` si algún vector contiene valores NA).


```{example} 
Queremos contrastar si hay correlación positiva entre el peso de una madre en el momento de la concepción del hijo y el peso de su hijo en el momento de nacer. Para ello vamos a usar la tabla de datos  `birthwt` incluida en el paquete **MASS** que ya usamos en una lección anterior, que contiene información sobre recién nacidos y sus madres, y que en particular dispone de las variables `bwt`, que da el peso del recién nacido en gramos, y `lwt`, que da el peso de la madre en libras en el momento de su última menstruación. Vamos a suponer que ambos pesos siguen distribuciones normales. Si denotamos por $X$ e $Y$ las correspondientes variables poblacionales, queremos realizar el contraste
$$
\left\{\begin{array}{l}
H_0: \rho_{XY}=0\\
H_1: \rho_{XY}>0
\end{array}\right.
$$
Vamos a usar la función `cor.test`.



```

```{r}
library(MASS)
cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")
```

El p-valor `r round(cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")$p.value,3)`  nos da evidencia estadísticamente significativa de que, en efecto, hay una correlación positiva entre el peso de la madre y el peso del recién nacido. El último valor, el `r cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")$estimate` bajo el *cor*, es la correlación de Pearson de los dos vectores de pesos, y el *95 percent confidence interval* es el intervalo de confianza del 95% del contraste unilateral planteado y nos dice que tenemos un 95% de confianza en que la correlación entre el peso de la madre y el peso del recién nacido es superior a `r round(cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")$conf.int[1],3)`. 

Podríamos haber obtenido el p-valor del contraste de correlación anterior directamente con la instrucción
```{r}
cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")$p.value
```


Si hubiéramos querido calcular un intervalo de confianza del 95% para $\rho_{XY}$ que repartiera por igual a ambos lados el 5% de probabilidad de no contener su valor real, hubiéramos podido usar el intervalo de confianza del contraste bilateral:
```{r}
cor.test(birthwt$bwt, birthwt$lwt)$conf.int
```

La potencia de un contraste de correlación se calcula con la función `pwr.r.test` del paquete **pwr**. En este caso, el tamaño del efecto es simplemente la correlación de Pearson, que se entra en la función mediante el parámetro `r`. Apliquémosla para calcular la potencia del contraste de correlación anterior:

````{r}
library(pwr)
dim(birthwt)
round(cor(birthwt$bwt,birthwt$lwt),4)
pwr.r.test(n=189,r=0.1857,sig.level=0.05,alternative="greater")
````

La probabilidad de error de tipo II en este contraste era de un poco menos del `r 100-round(100*pwr.r.test(n=189,r=0.186,sig.level=0.05,alternative="greater")$power)`%.

Si quisiéramos realizar este contraste de correlación con una potencia del 90% suponiendo que la magnitud del efecto va ser pequeña, usaríamos primero `cohen.ES` con `test="r"` para determinar qué magnitud del efecto se considera pequeña y a continuación `pwr.r.test` dejando sin especificar la `n`:

```{r}
cohen.ES(test="r",size="small")
pwr.r.test(power=0.9,r=0.1,sig.level=0.05,alternative="greater")
```

Hubiéramos necesitado datos de al menos `r ceiling(pwr.r.test(power=0.9,r=0.1,sig.level=0.05,alternative="greater")$n)` recién nacidos.

## Un ejemplo

Recordaréis el *data frame* `iris`, que tabulaba las longitudes y anchuras de los pétalos y los sépalos de una muestra de flores iris de tres especies. Vamos a extraer una subtabla con sus cuatro variables numéricas, que llamaremos `iris_num`, y calcularemos sus matrices de covarianzas y correlaciones.
```{r}
str(iris)
iris_num=iris[, 1:4]
n=dim(iris_num)[1] #Número de filas
```

Su matriz de covarianzas muestrales es:
```{r}
cov(iris_num)  
```

Su matriz de covarianzas "verdaderas" es:
```{r}
cov(iris_num)*(n-1)/n  
```

Su matriz de correlaciones de Pearson es:

```{r}
cor(iris_num)   
```

Observamos, por ejemplo, una gran correlación  de Pearson positiva entre la longitud y la anchura de los pétalos, `r round(cor(iris_num)[3,4],3)`, lo que indica una estrecha relación lineal con pendiente positiva entre estas magnitudes. Valdría la pena, entonces, calcular la recta de regresión lineal de una de estas medidas en función de la otra.
```{r}
lm(Petal.Length~Petal.Width, data=iris_num)
```


En cambio, la correlación de Pearson entre la longitud y la anchura de los sépalos es `r cor(iris_num)[1,2]`, muy cercana a cero, lo que es señal de que la variación conjunta de las longitudes y anchuras de los sépalos no tiene una tendencia clara.



Vamos a ordenar ahora los pares de variables numéricas de `iris` en orden decreciente de su correlación  en valor absoluto, para saber cuáles están más correlacionadas (en positivo o negativo). Para ello,  en primer lugar creamos un *data frame* cuyas filas están formadas por pares diferentes de variables numéricas de `iris`, su correlación de Pearson y el valor absoluto de esta última, y a continuación ordenamos las filas de este *data frame* en orden decreciente de estos valores absolutos.  Todo esto lo llevamos a cabo en el siguiente bloque de código, que luego explicamos:
```{r}
medidas=names(iris_num)
n=length(medidas)  #En este caso, n=4
indices=upper.tri(diag(n))
medida1=matrix(rep(medidas, times=n), nrow=n, byrow=FALSE)[indices]
medida2=matrix(rep(medidas, times=n), nrow=n, byrow=TRUE)[indices]
corrs=as.vector(cor(iris_num))[indices]  
corrs.abs=abs(corrs)
corrs_df=data.frame(medida1, medida2, corrs, corrs.abs)
corrs_df_sort=corrs_df[order(corrs_df$corrs.abs, decreasing=TRUE), ]
corrs_df_sort
```
Vemos que el par de variables con mayor correlación de Pearson en valor absoluto son `Petal.Length` y `Petal.Width`, como ya habíamos observado, seguidos por  `Petal.Length` y `Sepal.Length`.


Vamos a explicar el código. La función `upper.tri`, aplicada a una matriz cuadrada $M$, produce la matriz **triangular superior** de valores lógicos del mismo orden que $M$, cuyas entradas $(i,j)$ con $i<j$ son todas  `TRUE`  y el resto todas  `FALSE`. Existe una función similar, `lower.tri`, para producir  matrices **triangulares inferiores** de valores lógicos.
```{r}
upper.tri(diag(4))
lower.tri(diag(4))
```
Ambas funciones disponen del parámetro `diag` que, igualado a `TRUE`, define también como `TRUE` las entradas de la diagonal principal.
```{r}
upper.tri(diag(4), diag=TRUE)
```

Si $M$ es una matriz y $L$ es una matriz de valores lógicos del mismo orden, `M[L]` produce el vector construido de la manera siguiente: de cada columna, se queda sólo con las entradas de $M$ cuya entrada correspondiente en $L$ es `TRUE`, y a continuación concatena estas columnas, de izquierda a derecha, en un vector. Así, por ejemplo, tomemos la  matriz $M$ siguiente:
```{r}
M=matrix(1:16, nrow=4, byrow=T)
M
```

El vector formado por las entradas de su triángulo superior, concatenadas por columnas, se obtiene de la manera siguiente:
```{r}
M[upper.tri(diag(4))]
```

Ahora definimos las matrices siguientes, formadas por 4 copias (la primera por columnas, la segunda, por filas) del vector, al que hemos llamado `medidas`, de nombres de las variables numéricas de `iris`:
```{r}
matrix(rep(medidas, times=4), nrow=4, byrow=FALSE)
matrix(rep(medidas, times=4), nrow=4, byrow=TRUE)
```

Al aplicar estas dos matrices a la matriz de valores lógicos `upper.tri(diag(4))` obtenemos los nombres de las variables correspondientes a las filas y las columnas del triángulo superior, respectivamente, y al aplicar la matriz de correlaciones a esta matriz de valores lógicos, obtenemos sus entradas en este triángulo; en los tres vectores, las entradas siguen el mismo orden. Esto nos permite construir el *data frame* `corrs_df` cuyas filas están formadas por pares diferentes de variables numéricas de `iris`, su correlación de Pearson (columna `corrs`) y, aplicando `abs` a esta última variable, dicha correlación  en valor absoluto  (columna `corrs.abs`).

```{r}
corrs_df
```



Finalmente, la función `order` ordena los valores del vector al que se aplica, en orden decreciente si se especifica el parámetro `decreasing=TRUE`. Cuando aplicamos un *data frame* a una de sus variables reordenada de esta manera, reordena sus filas según el orden de esta variable. En este caso hubiéramos conseguido lo mismo con la función `sort`, pero la función `order` se puede aplicar a más de una variable del *data frame*: esto permite ordenar las filas del *data frame* en el orden de la primera variable de manera que, en caso de empate, queden ordenadas por la segunda variable,  y así sucesivamente.



## Representación gráfica de datos multidimensionales


La representación gráfica de tablas de datos multidimensionales tiene la dificultad de las dimensiones; para dos o tres variables es  sencillo visualizar las relaciones entre las mismas, pero para más variables ya no nos bastan nuestras tres dimensiones espaciales y tenemos que usar algunos trucos, tales como representaciones gráficas conjuntas de pares de variables.

La manera más sencilla de representar gráficamente una tabla de datos formada por dos variables numéricas   es aplicando la función `plot` a la matriz de datos  o al *data frame*. Esta función produce el **diagrama de dispersión** (*scatter plot*) de los datos: el gráfico de los puntos del plano definidos por las filas de la tabla.

A modo de ejemplo, si extrajéramos de la tabla `iris` una subtabla conteniendo sólo las longitudes y anchuras de los pétalos y quisiéramos visualizar la relación entre estas dimensiones, podríamos dibujar su diagrama de dispersión con el código del bloque siguiente. El resultado es la Figura \@ref(fig:iris1), que muestra una clara tendencia positiva: cuanto más largos son los pétalos, más anchos tienden a ser. Esto se corresponde con la correlación de Pearson de `r round(cor(iris_num)[3,4],3)` que hemos obtenido en la sección anterior. 

```{r,label=iris1,fig.cap="Diagrama de dispersión  de las longitudes y anchuras de los pétalos  de las flores de la tabla iris."}
iris.pet=iris[ ,c("Petal.Length","Petal.Width")]
plot(iris.pet, pch=20, xlab="Largo", ylab="Ancho")
```





Para tablas de datos de tres columnas numéricas, podemos usar con un fin similar la instrucción `scatterplot3d` del paquete homónimo, que dibuja un diagrama de dispersión tridimensional. Como `plot`, se puede aplicar a un *data frame* o a una matriz; por ejemplo, para representar gráficamente las tres primeras variables numéricas de `iris`, podríamos usar el código siguiente y obtendríamos la Figura \@ref(fig:iris2):

```{r,label=iris2,fig.cap="Diagrama de dispersión tridimensional de las tres primeras columnas de la tabla iris."}
library(scatterplot3d)
scatterplot3d(iris[ , 1:3], pch=20)
```

Podéis consultar la Ayuda de la instrucción para saber cómo modificar su apariencia: cómo ponerle un título, poner nombres adecuados a los ejes, usar colores, cambiar el estilo del gráfico, etc.


Una representación gráfica muy popular de las tablas de datos de tres o más columnas numéricas son las matrices formadas por los diagramas de dispersión de todos sus pares de columnas. Si la tabla de datos es un *data frame*, esta matriz de diagramas de dispersión  se obtiene simplemente aplicando la función `plot` al *data frame*; por ejemplo, la instrucción
```{r,eval=FALSE}
plot(iris[ , 1:4])
```
produce el gráfico de la Figura \@ref(fig:iris3). En este gráfico, los cuadrados en la diagonal indican  a qué variables corresponden cada fila y cada columna, de manera que podamos identificar fácilmente qué variables compara cada diagrama de dispersión; así, en el diagrama de la primera fila y segunda columna de esta figura, las abscisas corresponden a anchuras de sépalos y las ordenadas a longitudes de sépalos. Observad que la nube de puntos no muestra una tendencia clara y en todo caso ligeramente negativa, lo que se corresponde con la correlación de Pearson entre estas variables  de `r round(cor(iris_num)[1,2],3)` que hemos obtenido en la sección anterior.

```{r,echo=FALSE,label=iris3,fig.cap="Matriz de diagramas de dispersión  de la tabla iris."}
plot(iris[ , 1:4])
```


Podemos usar los parámetros usuales de `plot` para mejorar el gráfico resultante; por ejemplo, podemos usar colores para distinguir las flores según su especie. Así, la instrucción siguiente produce el gráfico de  la Figura \@ref(fig:iris3b).

```{r,label=iris3b,fig.cap="Matriz de diagramas de dispersión  de la tabla iris, con las especies distinguidas por colores."}
plot(iris[ , 1:4], col=iris$Species, pch=20, cex=0.7)
```


Para obtener la matriz de diagramas de dispersión  de una tabla de datos multidimensional también se puede usar la función `pairs`: así,  `pairs(iris[, 1:4])` produce exactamente el mismo gráfico que `plot(iris[, 1:4])`. La ventaja principal de  `pairs` es que se puede aplicar a una matriz para obtener la matriz de diagramas de dispersión de sus columnas, mientras que `plot` no.
 
El paquete **car** incorpora una función que permite dibujar matrices de diagramas de dispersión enriquecidos con información descriptiva extra de las variables de la tabla de datos y que además facilita el  control del gráfico resultante, por lo que os recomendamos su uso frente a las funciones básicas `plot` y `pairs`. Se trata de la función `spm` (abreviatura de `scatterplotMatrix`); por ejemplo, el código siguiente produce el gráfico  la Figura \@ref(fig:iris51).

```{r, label=iris51, fig.cap="Una matriz de diagramas de dispersión de la tabla iris producida con la función spm.", message=FALSE, warning=FALSE}
library(car)
spm(iris[ , 1:4], var.labels=c("Long. Sep.","Ancho Sep.","Long. Pet.","Ancho Pet."))
```

Observad para empezar que hemos cambiado los nombres que identifican las variables en los cuadrados de la diagonal, con el parámetro `var.labels`, y que en dichos cuadrados aparecen además unas curvas: se trata de la curva de densidad estimada de la variable correspondiente de la que hablábamos en la Lección \@ref(chap:agrup) de la primera parte del curso. La información gráfica contenida en estos cuadrados de la diagonal se puede modificar con el parámetro `diagonal`: podemos pedir, por ejemplo, que dibuje un histograma de cada variable (con `diagonal=list(method ="histogram")`) o su *boxplot* (con `diagonal=list(method="boxplot")`) o un QQ-plot (con `diagonal=list(method="qqplot")`). Así, el código siguiente produce el gráfico  la Figura \@ref(fig:iris52).

```{r, label=iris52, fig.cap="Matriz de diagramas de dispersión de la tabla iris con boxplots en la diagonal.",message=FALSE,warning=FALSE}
spm(iris[ , 1:4], var.labels=c("Long. Sep.","Ancho Sep.","Long. Pet.","Ancho Pet."), 
    diagonal=list(method="boxplot"), pch=20,cex=0.75)
```


Observad también que los diagramas de dispersión de la matriz producida con `spm` contienen algunas líneas. La línea recta es la recta de regresión por mínimos cuadrados y, sin entrar en detalle sobre su significado exacto, las curvas discontinuas representan la tendencia de los datos. Podéis eliminar la recta de regresión con `regLine=FALSE` (no os lo recomendamos) y las curvas discontinuas con `smooth=FALSE`; si las queréis mantener, consultad la Ayuda de la función para saber cómo cambiar su estilo, color, etc.

A veces querremos agrupar los datos de las variables  numéricas de una tabla de datos. Los  motivos serán los mismos que cuando se trata de una sola variable: por ejemplo, si los datos son aproximaciones de valores reales, o si son muy heterogéneos.  Cuando tenemos dos variables emparejadas agrupadas, se pueden representar gráficamente las frecuencias de sus pares de clases mediante un **histograma bidimensional**, que divide el conjunto de todos los pares de valores en rectángulos definidos por los pares de intervalos
e indica sobre cada rectángulo su frecuencia absoluta, por ejemplo mediante colores o intensidades de gris (dibujar barras verticales sobre las regiones es una mala idea, las de delante pueden ocultar las de detrás). Hay muchos paquetes de R que ofrecen funciones para dibujar histogramas bidimensionales; aquí explicaremos la función `hist2d` del paquete **gplots**. Su sintaxis básica es
```{r,eval=FALSE}
hist2d(x,y, nbins=..., col=...)
```

donde:

* `x` e `y` son los vectores de primeras y segundas coordenadas de los puntos. Si son las dos columnas de un *data frame* de dos variables numéricas, lo podemos entrar en su lugar.

* `nbins` sirve para indicar los números de clases: podemos igualarlo a un único valor, y tomará ese número de clases sobre cada vector, o a un vector de dos entradas que indiquen el número de clases de cada vector.

* `col` sirve para especificar los colores a usar. Por defecto, los rectángulos vacíos aparecen de color negro, y el resto se colorean con tonalidades de rojo, de manera que los tonos más cálidos indican  frecuencias mayores. 


Además, podemos usar los parámetros usuales de `plot` para poner un título, etiquetar los ejes, etc.

A modo de ejemplo, vamos a dibujar el histograma bidimensional de las longitudes y anchuras de los pétalos de las flores iris, agrupando ambas dimensiones en los números de clases que da la regla de Freedman-Diaconis (y que calcula la función `nclass.FD`):
```{r,eval=FALSE}
library(gplots)
hist2d(iris$Petal.Length, iris$Petal.Width, 
  nbins=c(nclass.FD(iris$Petal.Length),nclass.FD(iris$Petal.Width)))
```
Obtenemos (junto con una serie de información en la consola que hemos omitido) la Figura \@ref(fig:hist2d1), que podéis comparar con el diagrama de dispersión de los mismos datos de la Figura \@ref(fig:iris1).

```{r,echo=FALSE, label=hist2d1,fig.cap="Histograma bidimensional de longitudes y anchuras de pétalos de flores iris.", message=FALSE, warning=FALSE, results="hide"}
library(gplots)
hist2d(iris$Petal.Length, iris$Petal.Width, 
  nbins=c(nclass.FD(iris$Petal.Length),nclass.FD(iris$Petal.Width)))
```

En los histogramas bidimensionales con muchas regiones de diferentes frecuencias, es conveniente usar de manera adecuada los colores para representarlas. Una posibilidad es usar el paquete **RColorBrewer**, que permite elegir esquemas de colores bien diseñados. Las dos funciones básicas son:

* `brewer.pal(n,"paleta predefinida")`,  que carga en un vector de colores (una **paleta**) una secuencia de $n$ colores  de la **paleta predefinida** en el paquete.  Los nombres y contenidos de todas las paletas predefinidas que se pueden usar en esta función se obtienen, en la ventana de gráficos, ejecutando la instrucción `display.brewer.all()`. Por ejemplo, la paleta de colores de la Figura \@ref(fig:pal1) se define con el código siguiente:

```{r,include=FALSE}
library(RColorBrewer)
```


```{r,eval=FALSE}
brewer.pal(11,"Spectral")
```


```{r, echo=FALSE, label=pal1,fig.cap='Paleta brewer.pal(11,"Spectral")',out.height = "350px",out.width = "500px"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/figpal1.png")
```

* `colorRampPalette(brewer.pal(...))(m)`, produce una nueva paleta de $m$ colores  a partir del resultado de `brewer.pal`, interpolando nuevos colores. Luego se puede usar la función `rev` para invertir el orden de los colores, lo que es conveniente en los histogramas bidimensionales si queremos que las frecuencias bajas correspondan a tonos azules y las frecuencias altas a tonos rojos. Así, la paleta de colores que se define con
```{r,eval=FALSE}
rev(colorRampPalette(brewer.pal(11,"Spectral"))(50))
```
es la de la Figura \@ref(fig:pal2).



```{r, echo=FALSE, label=pal2,fig.cap='Paleta rev(colorRampPalette(brewer.pal(11,"Spectral"))(50))',out.width = "500px",out.height = "350px"}
knitr::include_graphics("AprendeR-Parte-II_files/figure-html/figpal2.png")
```


Vamos a usar esta última paleta en un histograma bidimensional de la tabla de alturas de padres e hijos recogidas por Karl Pearson en 1903 y que tenemos guardada en  el url https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt; el resultado es la Figura \@ref(fig:hist2dpearson).

```{r, label=hist2dpearson, fig.cap="Histograma bidimensional de las alturas de padres e hijos recogidas por Karl Pearson.", results="hide"}
library(RCurl)
df_pearson=read.table(text=getURL("https://raw.githubusercontent.com/AprendeR-UIB/Material/master/pearson.txt"),header=TRUE)
hist2d(df_pearson, nbins=30,
   col=rev(colorRampPalette(brewer.pal(11,"Spectral"))(50)))
```



Para terminar, veamos cómo producir un gráfico conjunto de un histograma bidimensional y los dos histogramas unidimensionales. Se trata de una modificación del gráfico similar explicado en http://www.everydayanalytics.ca/2014/09/5-ways-to-do-2d-histograms-in-r.html, el cual a su vez  se inspira en un gráfico de la p. 62 de *Computational Actuarial Science with R* de Arthur Charpentier (Chapman and Hall/CRC, 2014). Considerad la función siguiente, cuyos parámetros son un *data frame* `df` de dos variables y un número `n` de clases, común para las dos variables:
```{r, eval=FALSE}
hist.doble=function(df,n){
  par.anterior=par()
  h1=hist(df[,1], breaks=n, plot=F)
  h2=hist(df[,2], breaks=n, plot=F)
  m=max(h1$counts, h2$counts)
  par(mar=c(3,3,1,1))
  layout(matrix(c(2,0,1,3), nrow=2, byrow=T), heights=c(1,3), widths=c(3,1))
  hist2d(df, nbins=n, col=rev(colorRampPalette(brewer.pal(11,"Spectral"))(50)))
  par(mar=c(0,2,1,0))
  barplot(h1$counts, axes=F, ylim=c(0, m), col="red")
  par(mar=c(2,0,0.5,1))
  barplot(h2$counts, axes=F, xlim=c(0, m), col="red", horiz=T)
  par.anterior}
```

Entonces, la instrucción

```{r, eval=FALSE}
hist.doble(df_pearson,25)
```

produce la Figura \@ref(fig:hist2dcomplet).

```{r, echo=FALSE,results="hide", fig.cap="Histograma bidimensional con histogramas unidimensionales de las alturas de padres e hijos recogidas por Karl Pearson.", label=hist2dcomplet}
    par(mar=c(3,3,1,1))
df=df_pearson
  n=25
  h1=hist(df[,1], breaks=n, plot=F)
  h2=hist(df[,2], breaks=n, plot=F)
  m=max(h1$counts, h2$counts)
  layout(matrix(c(2,0,1,3),nrow=2,byrow=T), 
     heights=c(1,3), widths=c(3,1))
  hist2d(df, nbins=n, 
     col=rev(colorRampPalette(brewer.pal(11,"Spectral"))(50)))
  par(mar=c(0,2,1,0))
  barplot(h1$counts, axes=F, ylim=c(0, m), col="red")
  par(mar=c(2,0,0.5,1))
  barplot(h2$counts, axes=F, xlim=c(0, m), col="red", horiz=T)
```


Algunas explicaciones sobre el código, por si lo queréis modificar:

* Hemos "simulado" los histogramas mediante diagramas de barras de sus frecuencias absolutas, para poder dibujar horizontal el de la segunda variable.

* El parámetro `axes=FALSE` en los `barplot` indica que no dibuje sus ejes de coordenadas.

* La función `par` establece los parámetros generales básicos de los gráficos. Como con esta función los modificamos, guardamos los parámetros anteriores en `par.anterior` y al final los restauramos. 

* El parámetro `mar` de la función `par` sirve para especificar, por este orden, los márgenes inferior, izquierdo, superior y derecho  de la próxima figura, en números de líneas. 

* La instrucción `layout` divide la figura a producir en sectores con la misma estructura que la matriz de su primer argumento.
Dentro de esta matriz, cada entrada indica qué figura de las próximas se ha de situar en ese sector. Las alturas y amplitudes relativas de los sectores
se especifican con los parámetros `heights` y `widths`, respectivamente. Así, la instrucción
```{r,eval=FALSE}
layout(matrix(c(2,0,1,3),nrow=2,byrow=T), heights=c(1,3),widths=c(3,1))
```
divide la figura en 4 sectores. Los sectores de la izquierda serán el triple de anchos que los de la derecha (`widths=c(3,1)`), y los sectores inferiores serán el triple de altos que los superiores (`heights=c(1,3)`). En estos sectores, R dibujará los próximos gráficos según el esquema definido por la matriz del argumento:
$$
\left(\begin{array}{cc}
\mbox{segundo} & \mbox{ninguno}\\
\mbox{primero} & \mbox{tercero}
\end{array}\right).
$$



## Guía rápida


* `sapply(data_frame,función)` aplica la `función` a las columnas del `data_frame`.

* `scale` sirve para aplicar una transformación lineal a una matriz o a un *data frame*. Sus parámetros son:
    *  `center`: especifica  el vector que restamos a sus columnas; por defecto, el vector de medias muestrales.

    * `scale`: especifica  el vector por el que dividimos sus columnas; por defecto, el vector de desviaciones típicas muestrales.


* `cov`, aplicada a dos vectores, calcula su covarianza muestral; aplicada a un *data frame* o a una matriz, calcula su matriz de covarianzas muestrales.
Dispone del parámetro `use`, que:

    * Para dos vectores:
    
          * Igualado a `"complete.obs"`, calcula las covarianzas teniendo en cuenta sólo  sus observaciones completas (las posiciones en las que ninguno de los dos vectores tiene un NA).
    
    * Para más de dos vectores:

         * Igualado a `"pairwise.complete.obs"`, calcula la covarianza de cada par de columnas  teniendo en cuenta sólo sus observaciones completas, independientemente del resto de la tabla; es decir, como si en el cálculo de la covarianza de cada par de columnas usáramos `use="complete.obs"`, sin tener en cuenta que forman parte de una tabla de datos con más columnas.

         * Igualado a `"complete.obs"`, calcula las covarianzas de las columnas teniendo en cuenta sólo las filas completas de toda la matriz.


* `cor`, aplicada a dos vectores, calcula su correlación de Pearson; aplicada a un *data frame* o a una matriz, calcula su matriz de correlaciones de Pearson. Se puede usar el parámetro `use`  de `cov`. Usando el parámetro `method="spearman"` calcula la correlación (o la matriz de correlaciones, si se aplica a un *data frame* o a una matriz) de Spearman.


* `cor.test` realiza un contraste de correlación, con hipótesis nula que la correlación poblacional sea 0. Su sintaxis es la usual en funciones de contrastes.

* `pwr.r.test` del paquete **pwr**, sirve para calcular la potencia de un contraste de correlación. Sus parámetros son: `n`, el tamaño de las muestras; `r`, su correlación de Pearson;  `sig.level`, el nivel de significación; `power`, la potencia; y  `alternative`, el tipo de contraste. Si se entran los valores de tres de los cuatro primeros parámetros, se obtiene el cuarto.

* `upper.tri`, aplicada a una matriz cuadrada *M*, produce la matriz  triangular superior de valores lógicos del mismo orden que *M*. Con el parámetro `diag=TRUE` se impone que el triángulo de valores `TRUE` incluya la diagonal principal.


* `lower.tri`, aplicada a una matriz cuadrada *M*, produce la matriz  triangular inferior de valores lógicos del mismo orden que *M*. Dispone del mismo  parámetro `diag=TRUE`.

* `order` ordena el primer vector al que se aplica, desempatando empates mediante el orden de los vectores subsiguientes a los que se aplica; el parámetro `decreasing=TRUE` sirve para especificar  que sea en orden decreciente. 

* `plot`, aplicado a un *data frame* de dos variables numéricas, dibuja su diagrama de dispersión; aplicado a un *data frame* de más de dos variables numéricas, produce la matriz formada por los diagramas de dispersión de todos sus pares de variables.

* `pairs` es equivalente a `plot` en el sentido anterior, y se puede aplicar a matrices.

* `spm` del paquete **cars**, produce matrices de dispersión más informativas y fáciles de modificar.

* `scatterplot3d` del paquete **scatterplot3d**,  dibuja diagramas de dispersión tridimensionales. 

* `hist2d` del paquete **gplots**, dibuja histogramas bidimensionales. Dispone de los parámetros específicos siguientes:

    * `nbins`: indica los números de clases.

    * `col`: especifica la paleta de colores que ha de usar para representar las frecuencias. 

* `brewer.pal(n,"paleta predefinida")` del paquete **RColorBrewer**,  carga en una paleta de colores una secuencia de *n* colores  de la *paleta predefinida* en dicho paquete.  



* `colorRampPalette(brewer.pal(...))(m)` del paquete **RColorBrewer**,  genera una nueva paleta de $m$ colores  a partir del resultado de `brewer.pal`, interpolando nuevos colores. 

* `display.brewer.all()` del paquete **RColorBrewer**, muestra los nombres y contenidos de todas las paletas predefinidas en dicho paquete. 


* `par` sirve para establecer los parámetros generales básicos de los gráficos. 

* `layout` divide en sectores la figura a producir, para que pueda incluir varios gráficos independientes simultáneamente.


## Ejercicios

 
### Modelo de test {-}

*(1)* Considerad la matriz de datos 
$$\left(\begin{array}{ccc} 10.6 & 2.4 & 7.5 \\ 7.4 & 3.7 & 10.9\\ 10.7 & 2.6 & 9.6 \\ 8.4 & 4.9 & 9.9\\16.7 & 6.2 & 13.2 \\ 11.3 & 4.3 & 7.7\end{array} \right).$$
Calculad la entrada (4,2) de su matriz de datos tipificada, redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha.

*(2)* Considerad la matriz de datos $$\left(\begin{array}{ccc} 10.6 & 2.4 & 7.5 \\ 7.4 & 3.7 & 10.9\\ 10.7 & 2.6 & 9.6 \\ 8.4 & 4.9 & 9.9\\16.7 & 6.2 & 13.2 \\ 11.3 & 4.3 & 7.7\end{array}\right).$$ 
Calculad la covarianza muestral $\widetilde{s}_{3,2}$, redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha.

*(3)* Considerad la matriz de datos $$\left(\begin{array}{ccc} 10.6 & 2.4 & 7.5 \\ 7.4 & 3.7 & 10.9\\ 10.7 & 2.6 & 9.6 \\ 8.4 & 4.9 & 9.9\\16.7 & 6.2 & 13.2 \\ 11.3 & 4.3 & 7.7\end{array}\right).$$ Calculad la correlación de Pearson $r_{3,2}$, redondeada a 3 cifras decimales y sin ceros innecesarios a la derecha.

*(4)* Usando la función `cor.test`, realizad el contraste bilateral de correlación entre el perímetro del tronco y la altura  de los cerezos negros americanos usando la muestra del dataframe *trees* que viene con la instalación básica de R. Dad el p-valor redondeado a 3 cifras decimales, sin ceros innecesarios a la derecha, e indicad si la conclusión, con un nivel de significación del 5%, es que hay correlación o no entre estas dos variables, escribiendo SI o NO, según corresponda. Separad el p-valor de la conclusión con un único espacio en blanco.

*(5)* Calculad la correlación de Spearman de los vectores  $x=(4,8,6,9,5,9 ,4,7,10, 8)$ e $y=(0,6,2,1,4,4,3,7,11,5)$. Dad el resultado redondeado a 3 cifras decimales sin ceros innecesarios a la derecha.

*(6)* ¿Cuál de las cuatro matrices siguientes es la matriz de covarianzas de una tabla de datos de 2 columnas y 5 filas? Solo hay una.
$$
\begin{array}{l}
A=\left(\begin{array}{cc} 0.7 & 3 \cr 3 & 1.2 \end{array}\right)\\
B=\left(\begin{array}{cc} 0.6 & 0.8\cr -0.8 & 0.6 \end{array}\right)\\
C=\left(\begin{array}{cc} 0.6 & 0.8\cr 0.8 & -0.6 \end{array}\right)\\
D=\left(\begin{array}{ccccc}
 0.7 & 0.2 &  1.3 & 0.5& -0.1\cr
 0.2 &  0.7 & -0.3 & -0.1 &-0.1\cr
 1.3 & -0.3 & 3.1 & 1.3 & 0.4\cr
 0.5& -0.1& 1.3 & 0.6 & 0.2\cr
 -0.1 & -0.1 &  0.4 &  0.2& 2.9\end{array}\right)
\end{array}
$$

<!--
### Ejercicio  {-}

El fichero https://raw.githubusercontent.com/AprendeR-UIB/Material/master/NotasMatesI14.csv recoge las notas medias (sobre 100) obtenidas en las diferentes actividades de evaluación de la asignatura Matemáticas I del grado de Biología, en el curso 2013/14, por parte de los estudiantes que fueron considerados "presentados" en la primera convocatoria. Estas actividades consistieron en:

* Dos controles (columnas `Control1` y `Control2`).
* Talleres de resolución de problemas (columna `Talleres`).
* Ejercicios para resolver en casa (columna `Casa`).
* Cuestionarios en línea sobre los contenidos de la asignatura y sobre R  (columnas `TestsCont` y `TestsR`, respectivamente).

Cargad este fichero en un *data frame*.

a. Calculad el vector de medias y el vector de desviaciones típicas de esta tabla de datos. ¿Cuáles son las actividades de evaluación cuyas notas presentan mayor y menor variabilidad? 

a. Calculad las matrices de covarianzas "verdaderas" y de correlaciones de Pearson de esta tabla de datos. 

a. ¿Qué variable tiene la mayor correlación media con las otras variables? ¿Cuál  tiene la menor?

a. Ordenad los pares de variables de esta tabla por su correlación. ¿Cuáles son los dos pares con mayor correlación?  ¿Cuáles son los dos pares con menor correlación?
 
a. Comprobad en esta tabla de datos que su matriz de correlaciones es igual a la matriz de covarianzas de su tabla tipificada.

a. Dibujad una matriz de diagramas de dispersión de estas notas añadiendo en cada uno la recta de regresión lineal por mínimos cuadrados (pero sin otras curvas que indiquen la tendencia de los datos). ¿Se pueden ver en este diagrama los  dos pares de actividades de evaluación con mayor correlación y los dos pares con menor correlación que habéis encontrado en el apartado (d)?
--> 

### Respuestas al test {-}

```{r,include=FALSE}
X=matrix(c(10.6, 2.4, 7.5, 7.4, 3.7, 10.9,10.7, 2.6, 9.6 , 8.4, 4.9 , 9.9,16.7 , 6.2, 13.2 , 11.3 , 4.3 ,7.7),nrow=6,byrow=TRUE)
n=dim(X)[1]
X.tip.Ex=scale(X)*sqrt(n/(n-1)) 
tip42=X.tip.Ex[4,2]
cov42=cov(X)[3,2]
cor42=cor(X)[3,2]
pval=cor.test(trees$Height,trees$Girth)$p.value
x=c(4,8,6,9,5,9 ,4,7,10, 8)
y=c(0,6,2,1,4,4,3,7,11,5)
sper=cor(x,y,method = "spearman")
```


*(1)* `r round(tip42,3)`

Nosotros lo hemos calculado con
```{r}
X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE)
n=dim(X)[1]
X.tip.Ex=scale(X)*sqrt(n/(n-1)) 
round(X.tip.Ex[4,2],3)
```


*(2)* `r round(cov42,3)`

Nosotros lo hemos calculado con
```{r}
X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE)
round(cov(X)[3,2],3)
```

*(3)* `r round(cor42,3)`

Nosotros lo hemos calculado con
```{r}
X=matrix(c(10.6,2.4,7.5,7.4,3.7,10.9,10.7,2.6,9.6,8.4,4.9,9.9,16.7,6.2,13.2,11.3,4.3,7.7), nrow=6, byrow=TRUE)
round(cor(X)[3,2],3)
```


*(4)* `r round(pval,3)` SI

Nosotros lo hemos calculado con
```{r}
round(cor.test(trees$Height,trees$Girth)$p.value,3)
```

*(5)* `r round(sper,3)`

Nosotros lo hemos calculado con
```{r}
x=c(4,8,6,9,5,9 ,4,7,10, 8)
y=c(0,6,2,1,4,4,3,7,11,5)
round(cor(x,y,method="spearman"),3)
```

*(6)* A





<!--chapter:end:08-EstMult.Rmd-->

# ANOVA básico {#chap:ANOVA}

En esta lección explicamos cómo efectuar con R los ANOVA básicos que se estudian en cursos introductorios de estadística inferencial: de uno y dos factores y de bloques completos aleatorios. El tema central de la lección son los aspectos técnicos del ANOVA con R y los tests posteriores de comparación de pares de medias. Incluimos además una sección con  algunas instrucciones que permiten contrastar las condiciones necesarias sobre los datos para que un contraste ANOVA tenga sentido y una sección con algunos contrastes no paramétricos alternativos al ANOVA que se puedan usar justamente cuando no tiene sentido realizar un ANOVA.


## Los modelos del ANOVA en R {#sec:modelos}

Los modelos a los que se aplica un ANOVA  u otras muchas funciones, como por ejemplo la función `lm` para calcular la recta de regresión lineal, se especifican en R mediante **fórmulas**.  El operador básico para construir una fórmula es la tilde, `~`. Las fórmulas suelen tener la forma `Y~modelo`, donde la *Y* es un vector  y el *modelo* es una combinación de vectores o factores que representa el modelo con el que queremos explicar el vector *Y* (en palabras técnicas, al que queremos **ajustar** los datos del vector  *Y*). Por ejemplo, para calcular la recta de regresión por mínimos cuadrados de un vector *Y* respecto de un vector *X*, usamos `lm(Y~X)`. Esto significa que aplicamos la función `lm` a la fórmula `Y~X` que indica que queremos explicar  *Y* en función de *X*.


En los ANOVA que consideramos en esta lección se usan cuatro tipos de fórmulas. Concretamente, si *X* es una variable numérica y  *F1* y *F2*  son dos factores:

* La fórmula `X~F1` se usa  para indicar el ANOVA de un factor,  *F1*, de la variable *X*.

* La fórmula `X~F1+F2` se usa  para indicar el ANOVA de dos factores, *F1* y *F2*, de la variable *X*, sin tener en cuenta la interacción entre los  factores; es decir, suponiendo que sus efectos se suman, sin que haya interacción entre los mismos. Es el tipo de fórmula que se usa en los ANOVA de bloques.


* La fórmula `X~F1*F2` se usa para indicar el ANOVA de dos factores, *F1* y *F2*, de la variable *X*,  teniendo en cuenta además la posible  interacción entre estos factores. 

* La fórmula `X~F1:F2` se usa para indicar el ANOVA de un factor que tiene como niveles los pares ordenados de niveles de  *F1* y *F2*.

La función básica de R para realizar un ANOVA es `aov`. Su
sintaxis genérica es
```{r, eval=FALSE}
aov(fórmula, data=...)
```
con los argumentos siguientes:

* `fórmula`:  Una fórmula que especifique un modelo de ANOVA.

*  `data`:   Opcional, sirve para especificar, si es necesario, el *data frame* al que pertenecen  las variables  utilizadas en la fórmula.


Así, por ejemplo, si tenemos un *data frame* llamado `DF`, con una variable numérica `X` y un factor `Fact`, para realizar el ANOVA de la variable  `X` respecto del factor `Fact` con la función `aov` podríamos entrar
```{r, eval=FALSE}
aov(X~Fact, data=DF) 
```
o
```{r, eval=FALSE}
aov(DF$X~DF$Fact)
```


Otra posibilidad, que por ahora no usaremos pero sí más adelante, es aplicar la función `anova` (no la confundáis con `aov`) al resultado de  `lm`. La sintaxis sería entonces
```{r, eval=FALSE}
anova(lm(fórmula, data=...))
```


## ANOVA de un factor {#sec:ANOVA-1}


Para ilustrar el ANOVA de un factor con R utilizaremos un experimento en el que se quiso determinar si cuatro dietas concretas tenían alguna influencia en el tiempo de coagulación de la sangre en mamíferos.^[Véase *Statistics for Experimenters* (2a edición), de G. P. Box, W. G. Hunter y J. S. Hunte (Wiley, 2005), p. 133.] Para ello se escogieron 24 animales, se repartieron de manera aleatoria en 4 grupos de 6 ejemplares cada uno, y a cada grupo se le asignó de manera aleatoria una de las 4 dietas objeto de estudio, que indicaremos con A, B, C y D. Al cabo de un cierto tiempo se midió el tiempo de coagulación de la sangre en estos animales. Los resultados (redondeados a enteros) se muestran en la Tabla \@ref(tab:1). Observad que estamos ante un diseño experimental  de un solo factor: la dieta. 

```{r,label=1,echo=FALSE}
knitr::kable(data.frame(
A=c(62,60,63,59,63,59),
B=c(63,67,71,64,65,66),
C=c(68,66,71,67,68,68),
D=c(56,62,60,61,63,64)),
booktabs=TRUE,
caption ="Tiempos de coagulación bajo diferentes dietas.")
```


Para contrastar si los tiempos medios de coagulación son los mismos para las cuatro dietas o no, vamos a realizar un ANOVA de estos datos. Para ello, lo primero que tenemos que hacer es recoger los datos en un *data frame*, formado por una variable numérica con los valores de la tabla y un factor cuyos niveles sean las diferentes dietas, de manera que a cada valor en la variable numérica le corresponda la dieta con la que se obtuvo. Nosotros entraremos los datos de la tabla anterior por filas, y entonces el factor tendrá que ser

    A, B, C, D, A, B, C, D, A... 

Otra opción sería entrar los datos por columnas, y entonces entraríamos como factor

    A, A, A, A, A, A, B, B, B,... 

Entramos pues los datos de la tabla, por filas:
```{r}
coag=c(62,63,68,56,60,67,66,62,63,71,71,60,59,64,67,61,63,65,68,63,59,66,68,64)
```

Definimos de manera adecuada el factor de las dietas, como 6 copias de la fila A,B,C,D:

```{r}
diet=rep(c("A","B","C","D"), times=6)
```

Finalmente, definimos el *data frame* y comprobamos que es correcto:

```{r}
coagulacion=data.frame(coag,diet)
str(coagulacion)
head(coagulacion)
```


Para analizar la igualdad de los tiempos medios de coagulación bajo las cuatro dietas, realizaremos un ANOVA de la variable `coag` separándola según (**ajustándola a**) el factor `diet`. Antes de empezar, es conveniente visualizar los datos para hacernos una idea de su distribución; por ejemplo, por medio de un  diagrama de cajas con una caja por cada nivel:
```{r,eval=FALSE}
boxplot(coag~diet, data=coagulacion)
```
Observad que también hemos empleado una fórmula para indicar que queremos los diagramas de cajas de la variable `coag` separada por la variable `diet` del *data frame* `coagulacion`. Obtenemos la  Figura \@ref(fig:241), donde podemos ver que las medias muestrales para las dietas A y C son muy diferentes, y que en cambio seguramente no podremos rechazar que las medias poblacionales de las dietas A y D sean iguales. Por lo tanto, el resultado que esperamos del ANOVA es que nos permita rechazar la hipótesis nula de que los cuatro tiempos medios de coagulación son iguales. Ahora bien, hasta que no realicemos el ANOVA no sabremos si las diferencias que observamos en este diagrama son estadísticamente  significativas o no.





```{r, echo=FALSE,results="hide", label=241, fig.cap="Diagrama de cajas de los tiempos de coagulación según las diferentes dietas."}
boxplot(coag~diet, data=coagulacion)
```

Como hemos comentado, para realizar el ANOVA deseado, entramos:
```{r}
aov(coag~diet, data=coagulacion)
```
El resultado no es la tabla del ANOVA. Para obtenerla, hay que aplicar `summary` al resultado de `aov`:

```{r}
summary(aov(coag~diet, data=coagulacion))
```
El resultado de esta  función `summary` es la tabla ANOVA usual:

* En la primera columna, dos etiquetas: el nombre del factor, en este caso `diet`, y `Residuals`, que representa los errores o residuos del ANOVA.
 
* La segunda columna, etiquetada `Df`, nos da los grados de libertad correspondientes al factor (su número de niveles menos 1) y a los residuos (el número de individuos en la tabla, menos el número de niveles del factor).

* La tercera columna,   `Sum Sq`, nos muestra las sumas de los cuadrados del factor, $SS_{Tr}$, y de los residuos, $SS_E$.

* La cuarta columna,   `Mean Sq`, contiene las medias de los cuadrados del factor, $MS_{Tr}$, y de los residuos, $MS_E$.

* La quinta columna,   `F value`, nos da el valor
del estadístico de contraste.

* En la sexta columna,    `Pr(>F)`, aparece el p-valor del contraste.

* La séptima columna, sin etiqueta, indica el nivel de significación del p-valor según el código usual, explicado en la última línea del resultado.  A mayor número de 
asteriscos, más significativo es el  p-valor y por lo tanto es más fuerte la evidencia
de que las medias comparadas no son todas iguales.


En nuestro caso, hemos obtenido un p-valor del orden de $`r round(summary(aov(coag~diet, data=coagulacion))[[1]]$"Pr(>F)"[1],6)`$. Esto nos permite rechazar la hipótesis nula y concluir que no todos los tiempos medios de coagulación para las diferentes dietas consideradas son iguales, como intuíamos. Recordad que esto no significa que hayamos obtenido evidencia de que todos los tiempos medios de coagulación son diferentes, solo de que hay al menos un par de dietas que dan tiempos medios diferentes. Si ahora queremos determinar de qué pares de dietas se trata, tendremos que realizar algún test de comparaciones de pares de medias: véase la Sección \@ref(sec:pares).

Para ahorrar espacio vertical, en lo que queda de lección vamos a eliminar los símbolos que marcan los niveles de significación de los p-valores. Para ello entramos la siguiente instrucción:
```{r}
options(show.signif.stars=FALSE)
```
A partir de ahora, y mientras no cerremos la sesión, estos símbolos no aparecerán más en las tablas ANOVA. 
```{r}
summary(aov(coag~diet, data=coagulacion))
```

Si en algún momento de la sesión queréis volver a ver los códigos de significación, basta que entréis
```{r,eval=FALSE}
options(show.signif.stars=TRUE)
```

Como hemos comentado, una manera alternativa de realizar un ANOVA es mediante `anova(lm(...))`. Con esta construcción obtenemos directamente la tabla, sin necesidad de aplicarle `summary`.
```{r}
anova(lm(coag~diet, data=coagulacion))
```

Para extraer  los datos de la tabla ANOVA obtenida con la función `summary(aov(...))`, y así poder operar directamente con ellos, hay que añadirle los sufijos adecuados. Consultemos su estructura.

```{r}
tabla=summary(aov(coag~diet, data=coagulacion)) 
str(tabla)
```
Vemos que la tabla ANOVA obtenida con `summary(aov(...))` es una `list` formada por un solo objeto  (`List of 1`),  que a su vez es un *data frame* cuyas variables son las columnas numéricas de la tabla. Por lo tanto, para obtener una columna de estas, primero hemos de añadir el sufijo `[[1]]`, que extrae el *data frame* de la  `list`, y a continuación el sufijo `$columna` correspondiente a la `columna` de la tabla ANOVA que nos interesa. Por ejemplo, la columna de las sumas de los cuadrados es:
```{r}
tabla[[1]]$"Sum Sq"
```
De manera similar, como el p-valor es el primer elemento de la columna `Pr(>F)`, lo obtenemos con:
```{r}
tabla[[1]]$"Pr(>F)"[1]
```
Fijaos en que, como los nombres de estas columnas contienen espacios en blanco u otros símbolos no aceptados en los nombres de objetos de R (recordad la Lección 2 del primer volumen), hay que especificarlos entre comillas. 

El resultado de `anova(lm(...))` ya es directamente un *data frame*, por lo que para extraer los valores de la tabla ANOVA que produce podemos usar la sintaxis usual de los *data frames*.
```{r}
tabla2=anova(lm(coag~diet, data=coagulacion))
str(tabla2)
tabla2$"Sum Sq"
tabla2$"Pr(>F)"[1]
```

Veamos otro ejemplo de ANOVA de un factor. 

```{example, label="rabbits"}
En un experimento, se estudió el efecto de seis dietas sobre el crecimiento de crías de conejo doméstico.^[Véase *Experimental Design and Analysis*, de  M. Lentner y T. Bishop (Valley Book Co. 1986), p. 428.] Los datos obtenidos están recogidos en la tabla de datos `rabbit` del  paquete **faraway**.


```

```{r}
library(faraway)
str(rabbit)
```
Consultando la Ayuda de `rabbit` nos enteramos de que el factor `treat` indica la dieta, con niveles  a,b,c,d,e,f, y de que la variable `gain` indica el aumento de peso; la variable `block`, que indica la camada, es irrelevante en este análisis concreto. 

Si dibujamos el diagrama de cajas de los crecimientos para cada dieta, obtenemos  la  Figura \@ref(fig:242), donde no observamos grandes diferencias  en los crecimientos medios.

```{r,fig.cap="Diagrama de cajas de los crecimientos de crías de conejo  según las diferentes dietas.", label=242}
boxplot(gain~treat, data=rabbit)
```

Para determinar si hay diferencia en los aumentos medios de peso bajo las seis dietas, realizaremos un ANOVA de la variable `gain` ajustándola al factor `treat`. 
```{r}
summary(aov(gain~treat, data=rabbit))
```
El p-valor es `r round(summary(aov(gain~treat, data=rabbit))[[1]]$"Pr(>F)"[1],3)`, lo que indica que, efectivamente, no hay evidencia de que las dietas den lugar a crecimientos medios diferentes. Si hubiéramos querido obtener solo el p-valor, podríamos haber entrado:
```{r}
summary(aov(gain~treat, data=rabbit))[[1]]$"Pr(>F)"[1]
```


Es muy importante tener presente que, al usar las instrucciones `aov` o `anova(lm(...))` para realizar un ANOVA, se tiene que  emplear un factor (o varios, en las próximas secciones) para separar la variable numérica en subpoblaciones. Veamos un ejemplo de lo que pasa si nos descuidamos en este punto.

```{example,label=bliss}
En un experimento se estudió el efecto de la vitamina C en el crecimiento de los dientes.^[Véase *The Statistics of Bioassay* de C. I. Bliss (Academic Press, 1952), p. 499-501.] Se tomaron 60 cobayas y se trató cada uno de ellos con una combinación diferente de dosis de vitamina C (0.5, 1 o 2 mg) y  método de suministro de la misma (mediante zumo de naranja o como ácido ascórbico) durante 6 semanas, y se cuantificó el crecimiento de sus dientes  durante dicho período (más en concreto, se midió la longitud media de sus odontoblastos al final del mismo). El resultado es una tabla de 60 datos, que aparecen recogidos en el fichero `ToothGrowth` del paquete **UsingR**.



```
 
```{r,message=FALSE}
library(UsingR)
str(ToothGrowth)
```
La Ayuda de `ToothGrowth` nos dice que la variable `len` contiene la longitud media final de los odontoblastos del animal, la variable `supp` el método de suministro (OJ indica zumo de naranja, *orange juice*, y VC indica ácido ascórbico puro, *vitamine C*), y la variable `dose` la dosis.

Nos vamos a olvidar por el momento de la variable `supp`, y vamos a contrastar si la dosis de vitamina C tiene influencia en el crecimiento de los dientes. Para ello realizamos un ANOVA de un factor.
```{r}
summary(aov(len~dose, data=ToothGrowth))
```

¿Veis algo raro? Hemos comentado que se usaron tres dosis diferentes, por lo que el número de grados de libertad en la fila del factor, `dose`, tendría que ser 2, y no 1. ¿Qué ha pasado? Muy sencillo: `dose` es una variable numérica, y para usar `aov` el factor ha de ser eso, un factor. Así que lo primero que tenemos que hacer es convertir esta variable en un factor. Los haremos sobre un duplicado de la tabla `ToothGrowth` original, a la que llamaremos `ToothGrowth2`.

```{r}
ToothGrowth2=ToothGrowth
ToothGrowth2$dose=as.factor(ToothGrowth2$dose)
str(ToothGrowth2)
summary(aov(len~dose, data=ToothGrowth2))
```
Ahora está bien. El p-valor prácticamente 0 nos permite rechazar la hipótesis nula y concluir que hay dosis de vitamina C que dan lugar a diferentes crecimientos medios de los odontoblastos.



## ANOVA de bloques completos aleatorios

Para ilustrar este tipo de ANOVA, analizaremos un  experimento sobre producción de penicilina.^[Véase *Practical Regression and Anova using R*, de J. Faraway, p. 186. Este texto se puede descargar de la página web https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf.] En dicho experimento, se evaluaron cuatro procesos diferentes para determinar si había diferencias en su efectividad. Los cuatro procesos usaban una técnica de cultivo sumergido y empleaban agua de macerado de maíz como fuente de nitrógeno orgánico. Como la composición de este líquido puede afectar la producción final de penicilina, para evaluar los cuatro procesos se prepararon 5 mezclas diferentes de agua de macerado de maíz, de cada mezcla se tomaron 4 muestras y se asignaron de manera aleatoria a los cuatro procesos de producción. Los resultados obtenidos son los de la Tabla \@ref(tab:2), y aparecen recogidos en la tabla de datos `penicillin`  del paquete **faraway**.


```{r,label=2,echo=FALSE}
knitr::kable(data.frame(
Mezcla=1:5,  
A=c(89,84,81,87,79),
B=c(88,77,87,92,81),
C=c(97,92,87,89,80),
D=c(94,79,85,84,88)),
booktabs=TRUE,
caption ="Producción de penicilina bajo diferentes procesos de producción y mezclas.")
```

Observad que estamos ante un diseño experimental  de bloques completos aleatorios. Los bloques son las mezclas de agua de macerado de maíz y  
los tratamientos (los procesos de producción) se han asignado de manera aleatoria a las unidades experimentales (las muestras) de cada bloque, de tal manera que cada bloque contiene exactamente una unidad experimental para cada tratamiento. Este es un ejemplo paradigmático de uso de bloques: para evitar la influencia de la variable "extraña" dada por la composición  del agua de macerado de maíz, que puede influir en la producción, se escogen al azar unas mezclas y se prueban todos los procesos de manera independiente sobre cada mezcla.

Demos un vistazo a esta tabla de datos.
```{r}
str(penicillin)
head(penicillin)
```
Según la Ayuda de `penicillin`, la variable `treat` es el proceso de producción, con valores `A`, `B`, `C` y `D`; la variable `blend` es la mezcla usada (los bloques), con valores `Blend1` a `Blend5`; y la variable numérica `yield` es un valor que cuantifica la producción de penicilina. 

Veamos cómo son los diagramas de cajas de la producción de penicilina separada por procesos de producción y por mezclas:
```{r,fig.cap="Diagrama de cajas de la producción de penicilina bajo los diferentes procesos de producción."}
boxplot(yield~treat, data=penicillin)
```

```{r,fig.cap="Diagrama de cajas de la producción de penicilina según las diferentes mezclas."}
boxplot(yield~blend, data=penicillin)
```
Vemos que no hay mucha diferencia entre las producciones  para los diferentes procesos (sin tener en cuenta los bloques), y que sí que hay algunas diferencias en las producciones  según la composición del  agua de macerado de maíz; ya hemos comentado que es bien sabido que su composición influye en la producción. 


Si realizamos un ANOVA de un factor para cada uno de estos dos factores por separado, obtenemos resultados consistentes con esta observación visual:
```{r}
summary(aov(yield~treat, data=penicillin))
summary(aov(yield~blend, data=penicillin))
```


El p-valor del ANOVA separando por procesos de producción es `r round(summary(aov(yield~treat, data=penicillin))[[1]]$"Pr(>F)"[1],3)`, lo que indica que no podemos rechazar la hipótesis nula de que los procesos de producción tengan igual productividad media (sin tener en cuenta las mezclas), y el p-valor del ANOVA separando por mezclas es `r round(summary(aov(yield~blend, data=penicillin))[[1]]$"Pr(>F)"[1],3)`, lo que, con un nivel de significación del 5%, nos permite rechazar que todas las mezclas produzcan la misma cantidad media de penicilina. Pero precisamente, el hecho de que la mezcla influya en la producción es lo que hace que el primer ANOVA no sea fiable: a lo mejor el efecto de las mezclas enmascara las diferencias en las productividades de los procesos bajo estudio. Para determinarlo, vamos a realizar un ANOVA de bloques, es decir, de dos factores y efectos acumulados.
```{r}
summary(aov(yield~treat+blend, data=penicillin))
```


El p-valor que nos interesa en esta tabla es el de la fila `treat`: es `r round(summary(aov(yield~treat+blend, data=penicillin))[[1]]$"Pr(>F)"[1],4)`, por lo que en este experimento de bloques tampoco detectamos evidencia de que haya diferencias en la productividad media de los procesos estudiados. 
El segundo p-valor de la tabla, `r round(summary(aov(yield~treat+blend, data=penicillin))[[1]]$"Pr(>F)"[2],4)`, es el del ANOVA de bloques que resulta de intercambiar los bloques y el tratamiento, tomando las mezclas como el factor a analizar y los procesos de producción como los bloques: 
```{r}
summary(aov(yield~blend+treat, data=penicillin))
```



Podemos extraer los resultados de una
tabla de un ANOVA de bloques generada por  `summary(aov(...))` añadiendo los mismos sufijos que en el caso de un factor. Así, por ejemplo, los p-valores son:
```{r}
tabla=summary(aov(yield~treat+blend, data=penicillin))
tabla[[1]]$"Pr(>F)"
```
y si solo nos interesa el del factor `treat`:
```{r}
tabla[[1]]$"Pr(>F)"[1]
```


Veamos otro ejemplo de ANOVA de bloques completos aleatorios. 



```{example, label="notes"}
Para estudiar si las diferentes actividades de evaluación llevadas a cabo en la asignatura de Matemáticas I tienen una dificultad similar (o mejor dicho, para mirar de confirmar nuestras sospechas de que no es así), escogimos una muestra aleatoria de 15 estudiantes de Biología o Bioquímica que se hubieran presentado al control 2 de dicha asignatura en el curso 2012-2013, y anotamos las notas obtenidas por estos estudiantes en los apartados de Tests, Talleres, Ejercicios de Casa, Control 1 y Control 2. Si obtenemos evidencia de que no todas las medias de las notas de las diferentes actividades fueron iguales, podremos concluir que no todas las actividades tienen la misma dificultad. Los datos obtenidos son los de la tabla \@ref(tab:3).


```


```{r,label=3,echo=FALSE}
knitr::kable(data.frame(
Estudiante=1:15,
Tests=c(48,94,98,60,52,78,84,83,75,24,47,53,82,66,49),
Casa=c(42,86,94,58,56,73,84,76,55,49,47,42,93,62,40),
Talleres=c(85,100, 93, 71, 79, 84, 94, 99, 70, 57, 64, 78, 92, 74, 74),
"Control 1"=c(31,52,93,66,64,80,83,51,85,19,44,50,66,24,35),
"Control 2"=c(20,48,90,46,24,95,70,55,48,55,35, 8,30,56,21)),
booktabs=TRUE,
caption="Notas obtenidas por 15 estudiantes en diferentes actividades de evaluación.")
```




Como podemos observar, se trata de un experimento de bloques   completos aleatorios: hemos escogido de manera aleatoria unos bloques (los estudiantes) y para cada estudiante hemos apuntado el valor de cada uno de los niveles del factor a estudiar (las notas en las diferentes actividades).`Este diseño es el adecuado para este problema, puesto que hay una gran variabilidad en las notas obtenidas por  estudiantes diferentes, desde matrículas a suspensos. Al tomar bloques, es decir, al considerar todas las notas de un conjunto aleatorio fijo de estudiantes, eliminamos el efecto de esta variabilidad.

Vamos a construir un *data frame* con estos datos. Este *data frame* tendrá tres variables: `notas}, con las notas obtenidas por los estudiantes; `acts`, con los tipos de actividades de evaluación realizados; y `bloques`, con el indicador de cada estudiante.
Entraremos las notas siguiendo las filas de la tabla anterior, y por lo tanto tenemos que construir estos dos factores entrando sus niveles en el orden adecuado: el factor `acts` ha de estar formado por 15 copias de la fila de actividades, y el factor `bloques` ha de estar formado por 5 copias de `1`, 5 copias de `2`, y así hasta 5 copias de `15`. Y recordad que `bloques` ha de ser un factor, puesto que queremos usarlo en un ANOVA. 

```{r}
notas=c(48,42,85,31,20,94,86,100,52,48,98,94,93,93,90,60,58,
   71,66,46,52,56,79,64,24,78,73,84,80,95,84,84,94,83,70,83,76,
   99,51,55,75,55,70,85,48,24,49,57,19,55,47,47,64,44,35,53,42,
   78,50,8,82,93,92,66,30,66,62,74,24,56,49,40,74,35,21)
acts=rep(c("Tests","Casa","Talleres","Control 1","Control 2"), times=15)
bloques=as.factor(rep(1:15,each=5)) 
notas.bloques=data.frame(notas,acts,bloques)
str(notas.bloques)
head(notas.bloques)
```



Vamos a dibujar los diagramas de cajas de las notas de las diferentes actividades y de los diferentes estudiantes:

```{r,fig.cap="Diagrama de cajas de las notas en las diferentes actividades."}
boxplot(notas.bloques$notas~notas.bloques$acts)
```

```{r,fig.cap="Diagrama de cajas de las notas de los estudiantes."}
boxplot(notas.bloques$notas~notas.bloques$bloques)
```

Podemos observar diferencias en las notas de algunas  actividades: por ejemplo, las notas del control 2 son muy inferiores a las de los talleres. Asimismo, como nos temíamos, vemos una gran variabilidad en las notas de los estudiantes.

Realicemos ahora el ANOVA.
```{r}
summary(aov(notas~acts+bloques, data=notas.bloques))
```
El p-valor que nos interesa es el de la primera fila, etiquetada con el factor cuyos niveles queremos comparar, en este caso `acts`. Este p-valor es muy pequeño, del orden de $10^{-7}$, lo que es evidencia de que, como nos temíamos, no todas las notas medias de las actividades de evaluación fueron iguales. 


## ANOVA de dos vías {#sec:ANOVA2}

Para ilustrar el ANOVA de dos vías, es decir, de dos factores que pueden interacccionar, analizaremos en primer lugar los resultados de un experimento sobre el efecto de venenos y antídotos.^[Véase  "An analysis of transformations", de G. Box  y D. Cox,  *J. Roy. Stat. Soc. Series B*  26 (1964), pp. 211-252. Véase también  *Practical Regression and Anova using R*, de J. Faraway, p. 182.] En este experimento se usaron tres venenos y cuatro antídotos, y cada combinación de veneno y antídoto se administró a cuatro ratas elegidas de manera aleatoria e independiente. A continuación, se anotó el tiempo de supervivencia de cada animal, en unidades de 10 horas. Se trata, pues, de un diseño experimental de dos factores, en el que cada individuo ha sido asignado al azar a  cada nivel de los dos factores. Por otro lado, no podemos descartar *a priori* que haya interacción entre venenos y antídotos, puesto que un antídoto puede ser más efectivo para un veneno que para otro. 

El objetivo del experimento era contrastar la igualdad de los tiempos medios de supervivencia según el veneno, según el antídoto, y según la combinación veneno-antídoto. Los datos obtenidos en este experimento se han recogido en la tabla `rats` del paquete  **faraway**. 

Pero ahora tenemos un problema con este *data frame*. Si cargamos  en una misma sesión varios paquetes que contengan objetos con el mismo nombre, R entiende en cada momento que  ese nombre refiere al objeto del paquete que se haya cargado más recientemente. Como en esta lección hemos cargado el paquete **UsingR** después del paquete **faraway**,  en estos momentos `rats` refiere a una tabla de datos del paquete **survival** que se ha cargado con **UsingR** y que no tiene nada que ver con los datos de este experimento. Una solución posible sería en este punto volver a cargar el paquete **faraway**. Otra opción más rápida es definir `rats` como el objeto `rats` de **faraway** por medio de la instrucción `rats=faraway::rats`. La construcción `paquete::objeto` invoca el `objeto` (una función, una tabla de datos,...) del **paquete** y sirve para eliminar ambigüedades como la que nos encontramos aquí.

Vamos a cargar y explorar estos datos.

```{r}
rats=faraway::rats
str(rats)
head(rats)
```
En la Ayuda de `rats` nos enteramos de que el factor `treat` contiene el antídoto, con niveles `A`, `B`, `C` y `D`; el factor `poison` contiene el veneno, con niveles `I`, `II` y `III`; y la variable numérica `time` contiene el tiempo de supervivencia de las ratas. Veamos cómo son los diagramas de cajas de estos tiempos, separados por venenos y por antídotos.
```{r,fig.cap="Diagrama de cajas de los tiempos de supervivencia según los venenos."}
boxplot(time~poison, data=rats)
```

```{r, fig.cap="Diagrama de cajas de los tiempos de supervivencia según los antídotos"}
boxplot(time~treat, data=rats)
```

Parece que hay diferencias en los tiempos medios de supervivencia tanto para los diferentes venenos como para los diferentes antídotos. Podemos también dibujar un diagrama de cajas de los tiempos de supervivencia separándolos por combinaciones de veneno y antídoto. La instrucción para hacerlo es la siguiente (observad la fórmula del argumento):
```{r, fig.cap="Diagrama de cajas de los tiempos de supervivencia según las combinaciones de veneno y antídoto."}
boxplot(time~poison:treat, data=rats)
```

El código para efectuar el ANOVA de dos vías del tiempo de supervivencia de las ratas bajo los efectos combinados de los venenos y los antídotos es el siguiente:

```{r,fig.cap="Diagrama de cajas de los tiempos de supervivencia según las combinaciones de veneno y antídoto."}
summary(aov(time~poison*treat, data=rats))
```


En la última columna de la tabla que obtenemos, el primer p-valor es el del contraste de la igualdad de tiempos medios de supervivencia según el veneno (`poison`): su valor, $`r round(summary(aov(time~poison*treat, data=rats))[[1]]$"Pr(>F)"[1],8)`$, nos permite concluir que estos tiempos medios no son todos iguales. El segundo p-valor es el del contraste de la igualdad de tiempos medios de supervivencia según el antídoto (`treat`): que valga $`r round(summary(aov(time~poison*treat, data=rats))[[1]]$"Pr(>F)"[2],7)`$ indica que estos tiempos medios tampoco son todos iguales. Finalmente, el tercer p-valor es el del contraste de  interacción entre veneno y antídoto (`poison:treat`). Recordad que  existe dicha **interacción**  cuando los cambios en la variable de respuesta (en este ejemplo, el tiempo de supervivencia) originados por uno de los factores no son los mismos para diferentes niveles del otro factor: es decir, en el contexto de este experimento, si algún antídoto es más (o menos) efectivo contra algún veneno que contra otro. Como aquí este p-valor vale `r round(summary(aov(time~poison*treat, data=rats))[[1]]$"Pr(>F)"[3],3)`, no obtenemos evidencia estadísticamente significativa de dicha   interacción: aceptamos que cada uno de los antídotos tiene un efecto similar sobre cada veneno (pero no todos los antídotos son igual de efectivos).

Observaréis que la tabla del ANOVA que produce R con esta fórmula no contiene la fila que permite contrastar si hay diferencia entre las medias de las poblaciones definidas por combinaciones de niveles, uno por cada factor: en este ejemplo, por las combinaciones de veneno y antídoto. Si se desea obtener esta fila, basta realizar un ANOVA de un factor que combine los dos factores del experimento. 
```{r}
summary(aov(time~poison:treat, data=rats))
```
Vemos que también hay diferencias estadísticamente significativas entre las combinaciones veneno--antídoto.


En el caso del ANOVA de dos vías, es interesante dibujar el **gráfico de interacción** entre factores. 
En un gráfico de interacción del factor $F_1$ respecto del factor $F_2$ en el ANOVA de una variable numérica $X$ respecto de estos factores,  se dibuja una línea  quebrada para cada nivel de $F_1$. Esta línea une, mediante segmentos, los valores medios que toma la variable $X$ en nuestra muestra para cada nivel de $F_2$ en el nivel de $F_1$ correspondiente. Si no hay ninguna interacción entre estos factores, las líneas resultantes serán paralelas. Cuanto más se alejen de ser paralelas, más evidencia de interacción habrá entre estos dos factores. 

Estos gráficos de interacción se dibujan en R con la instrucción `interaction.plot` aplicada, por este orden, a los dos factores y la variable numérica.  Así, pues, para obtener el gráfico de interacción  de la variable antídoto respecto de la variable veneno en el ANOVA anterior, entraríamos la siguiente instrucción:

```{r, fig.cap="Gráfico de interacción del veneno y el antídoto."}
interaction.plot(rats$treat, rats$poison, rats$time)
```

En el gráfico resultante observamos que los tres niveles del veneno producen líneas casi paralelas, aunque se observa una ligera interacción: la pendiente de la recta correspondiente al veneno II entre los valores medios de C y D es mucho mayor que la de las rectas correspondientes a los otros dos venenos, lo que indica que en el experimento el antídoto D ha sido menos efectivo contra el veneno II que contra los otros dos. No obstante, la diferencia no ha sido estadísticamente significativa. 


Este diagrama no ha quedado muy bonito, ya que las variables aparecen en él  con sus nombres completos, y  los factores no aparecen en la leyenda ordenados alfabéticamente. Una posibilidad para mejorarlo es cambiar las etiquetas de los ejes y la leyenda (eliminándola con `legend=FALSE` y añadiéndola a nuestro gusto con la función `legend`). Ya que estamos, dibujaremos algo más gruesas las líneas. En general, la instrucción `interaction.plot` admite todos los parámetros de `plot` más algunos de específicos que podéis conocer consultando su Ayuda.

```{r, fig.cap="Gráfico de interacción del veneno y el antídoto."}
interaction.plot(rats$treat, rats$poison, rats$time, legend=FALSE,
  xlab="Antídoto", ylab="Tiempo medio de supervivencia", lwd=c(2,2,2))
legend("topright", lty=1:3, cex=0.6, title="Venenos", legend=c("I","II","III"))
```

Veamos otro ejemplo de ANOVA de dos vías.

```{example, label=tooth2}
En el Ejemplo \@ref(exm:bliss) hablábamos de un cierto experimento sobre el efecto de la vitamina C en el crecimiento de los dientes, y de la  tabla de datos  `ToothGrowth` del paquete **UsingR** que recoge los datos obtenidos en  ese experimento.  Ahora vamos a realizar un ANOVA de dos factores sobre esta tabla de datos para contrastar la influencia  en dicho crecimiento de la dosis de vitamina C y del método de suministrarla, teniendo en cuenta que sus efectos  pueden interaccionar.
Recordad de aquel ejemplo que, si queremos llevar a cabo un ANOVA sobre esta tabla que involucre la variable `dose` que contiene la dosis, primero hay que convertir esta variable  en un factor. Volveremos a copiar la tabla `ToothGrowth`  en una nueva tabla `ToothGrowth2` y modificaremos en esta tabla dicha variable. 



```

```{r}
ToothGrowth2=ToothGrowth 
ToothGrowth2$dose=as.factor(ToothGrowth2$dose)
str(ToothGrowth2)
```


Demos ahora una ojeada a los diagramas de cajas de esta tabla, separando las longitudes de los odontoblastos por dosis, por suministro y por ambos.

```{r, fig.cap="Diagrama de cajas de las longitudes de los odontoblastos según la dosis de vitamina C."}
boxplot(len~dose, data=ToothGrowth2)
```


```{r, fig.cap="Diagrama de cajas de las longitudes de los odontoblastos según el método de suministro de vitamina C."}
boxplot(len~supp, data=ToothGrowth2)
```



```{r, fig.cap="Diagrama de cajas de las longitudes de los odontoblastos según la dosis y el método de suministro."}
boxplot(len~dose:supp, data=ToothGrowth2)
```

Podemos observar diferencias tanto  según la dosis y como según el método de suministro. ¿Serán significativas?

```{r}
summary(aov(len~supp*dose, data=ToothGrowth2))
summary(aov(len~supp:dose, data=ToothGrowth2))
```


El resultado muestra que, efectivamente, hay diferencias significativas en el crecimiento de los dientes tanto según la dosis (p-valor prácticamente 0) como según el método de suministro (p-valor $`r round(summary(aov(len~supp*dose, data=ToothGrowth2))[[1]]$"Pr(>F)"[1],4)`$) o  la combinación de ambos (p-valor prácticamente 0), y también que hay evidencia estadística de interacción entre los dos factores (p-valor `r round(summary(aov(len~supp*dose, data=ToothGrowth2))[[1]]$"Pr(>F)"[3],3)`). El gráfico de interacción entre las dosis y el método de suministro que produce la instrucción siguiente refleja esta interacción: la línea correspondiente a la dosis de 2 mg no es paralela a las otras.

```{r, fig.cap="Gráfico de interacción de la dosis de vitamina C y el método de suministro."}
interaction.plot(ToothGrowth2$supp, ToothGrowth2$dose, ToothGrowth2$len,  legend=FALSE,
 xlab="Método de suministro", ylab="Crecimiento medio", lwd=c(2,2,2))
legend("topright",lty=1:3,cex=0.6,title="Dosis",legend=c("0.5","1","2"))
```




El hecho de haber detectado interacción entre los dos factores hace que nos interese estudiar el efecto de los niveles de un factor en el otro. Esto se puede llevar a cabo mediante varios ANOVA de un factor. 

En primer lugar, vamos estudiar, para cada método de suministro de vitamina C, si hay diferencias entre los crecimientos medios de los odontoblastos según la dosis cuando la vitamina C se suministra mediante ese método. Para ello vamos a realizar dos ANOVA de un factor, la dosis, sobre dos *data frames* extraídos de  `ToothGrowth2`, uno con las filas donde `supp` toma el valor `OJ` y otro con las filas donde `supp` toma el valor `VC`. Vamos extraer estas subtablas de datos  usando la instrucción `subset`. Recordad que la construcción 
```{r, eval=FALSE}
subset(df, condición)
```
define un *data frame* con las filas del *data frame* `df` que cumplen la `condición`.
```{r}
TG.OJ=subset(ToothGrowth2, supp=="OJ")
TG.VC=subset(ToothGrowth2, supp=="VC")
```


Ahora realizamos los dos ANOVA:
```{r}
summary(aov(len~dose, data=TG.OJ))
summary(aov(len~dose, data=TG.VC))
```
En ambos casos el p-valor es muy pequeño, lo que indica que para cada tipo de suministro de vitamina C, el crecimiento medio de los dientes varía con la dosis. 

En este tipo de análisis, hay que tener en cuenta que si realizamos $N$ contrastes independientes en un mismo experimento, cada uno de ellos con un nivel de significación $\alpha$, la probabilidad  de rechazar en alguno de ellos la hipótesis nula si en todos  es verdadera es
$$
1-(1-\alpha)^N\approx N\alpha.
$$
Por lo tanto, si queremos garantizar un nivel de significación global $\alpha$, hay que realizar cada contraste con nivel de significación $\alpha/N$. (Para ser exactos, si llamamos $\alpha_c$ al nivel de significación de cada contraste necesario para obtener un nivel de significación global $\alpha$, se cumple que  $1-(1-\alpha_c)^N=\alpha$, de donde podemos despejar $\alpha_c$ y obtenemos $\alpha_c=1-\sqrt[N]{1-\alpha}$, que es un poco mayor que $\alpha/N$. Por ejemplo, para $\alpha=0.05$ y $N=20$ tenemos que $\alpha_c=0.0341$ mientras que $\alpha/N=0.025$.) En este ejemplo concreto, si hubiéramos deseado un nivel de significación global 0.05, deberíamos haber realizado cada uno de los ANOVA con un nivel de significación 0.025, lo que no afecta a las conclusiones, ya que los p-valores han sido realmente muy pequeños.


También podemos estudiar, para cada dosis, si hay diferencias entre las medias de crecimiento según el método de suministro. Como solo hay dos métodos de suministro, para cada dosis tanto da realizar un ANOVA o un simple test t suponiendo que las varianzas son iguales (ya que es una de las condiciones para poder realizar el ANOVA).
```{r}
TG.0.5=subset(ToothGrowth2, dose==0.5)
summary(aov(len~supp, data=TG.0.5))
TG.1=subset(ToothGrowth2, dose==1)
summary(aov(len~supp, data=TG.1))
TG.2=subset(ToothGrowth2, dose==2)
summary(aov(len~supp, data=TG.2))
```


En este caso, para garantizar un nivel de significación global de 0.05, hay que realizar cada contraste con un nivel de significación de 0.05/3=0.017. Obtenemos evidencia de que hay diferencia en el crecimiento medio de los odontoblastos según el tipo de suministro solo para las dosis de 0.5 y 1 mg, que es donde hemos obtenido p-valores inferiores al nivel de significación.

Si la variable `dose` hubiera tenido, pongamos, 20 niveles en vez de 3, definir una subtabla para cada nivel y efectuar el ANOVA correspondiente hubiera sido muy largo. Para automatizar cálculos como estos, lo mejor es definir una función que calcule el p-valor del ANOVA de una variable respecto de un factor sobre la subtabla definida por un nivel variable $x$ de un segundo factor, y luego aplicar esta función al vector de los niveles de este segundo factor.
En este ejemplo concreto usaríamos el código siguiente.
```{r}
Anova_dosis=function(x){
  summary(aov(len~supp, 
       data=subset(ToothGrowth2, dose==x)))[[1]]$"Pr(>F)"[1]
   }
round(sapply(levels(ToothGrowth2$dose), FUN=Anova_dosis),4)
```




## Condiciones del ANOVA

Los resultados de un ANOVA nos permiten extraer conclusiones solo si las poblaciones a las que se aplica cumplen una serie de condiciones; las dos básicas son la normalidad y la igualdad de varianzas. La condición de normalidad requiere que cada nivel de cada factor defina una distribución normal, es decir, que cada muestra aleatoria simple de un tratamiento provenga de una población normal. La condición de igualdad de varianzas requiere que todas estas distribuciones tengan la misma varianza. 

En la Lección \@ref(chap:bondad) ya estudiamos algunos tests específicos de normalidad: los tests de Kolmogorov-Smirnov-Lillliefors,
de Anderson-Darling, de Shapiro-Wilk y de D’Agostino-Pearson. Cada uno tiene sus ventajas y sus inconvenientes: el primero es muy popular, por ser una variante del test de Kolmogorov-Smirnov; el segundo, detecta mejor las discrepancias en los extremos; el test de Shapiro-Wilk es más potente para muestras grandes y menos sensible a repeticiones que los dos anteriores; y el test  de D’Agostino-Pearson permite repeticiones de datos, pero no se puede usar con muestras de tamaño inferior a 20.

Veamos algunos ejemplos de aplicación de tests de normalidad en el contexto de un ANOVA.

```{example, label=SW-auto}
Queremos contrastar si los tiempos de coagulación bajo cuatro dietas dados en la Tabla \@ref(tab:1) provienen de distribuciones normales. Como hay algunas repeticiones de valores en las columnas de la tabla y no llegan a los 20 valores cada una, usaremos el test de Shapiro-Wilk que, recordemos, está implementado en la función `shapiro.test`. Teníamos los datos guardados en un *data frame* llamado coagulacion.


```

```{r}
str(coagulacion)
```

Apliquemos la función `shapiro.test` a los tiempos de coagulación para cada dieta:
```{r}
coagA=coagulacion[coagulacion$diet=="A",]$coag
shapiro.test(coagA)$p.value
coagB=coagulacion[coagulacion$diet=="B",]$coag
shapiro.test(coagB)$p.value
coagC=coagulacion[coagulacion$diet=="C",]$coag
shapiro.test(coagC)$p.value
coagD=coagulacion[coagulacion$diet=="D",]$coag
shapiro.test(coagD)$p.value
```


Los cuatro p-valores que obtenemos son  grandes, mayores que 0.05, lo que quiere decir que en ninguno de los cuatro tests podemos rechazar la hipótesis nula: no hay evidencia de que los tiempos de coagulación bajo alguna de las cuatro dietas no  sigan leyes normales, por lo que aceptamos que sí las siguen. Recordad además que, como hemos efectuado cuatro contrastes, para garantizar un nivel de significación global de 0.05 hemos de realizar cada uno con un nivel de significación de 0.05/4=0.0125: es decir, que con p-valores por encima de este nivel de significación no podríamos rechazar ninguna hipótesis nula.

¿Cómo podríamos haber automatizado el cálculo de estos p-valores, para no tener que definir "a mano" cada vector de tiempos de coagulación? Con el código siguiente, donde definimos una función `Test.SW` que calcula el p-valor del test de Shapiro-Wilk y la aplicamos con un `aggregate` a los tiempos de coagulación  del *data frame* `coagulacion` separados según la dieta. De esta manera obtenemos un *data frame* cuya columna `coag` contiene, para cada nivel de `diet`, el p-valor del test de Shapiro-Wilk para los tiempos de coagulación de la dieta correspondiente.

```{r}
Test.SW=function(x){shapiro.test(x)$p.value}
aggregate(coag~diet, data=coagulacion,FUN=Test.SW)
```



```{example}
Vamos a contrastar si en el ANOVA de dos vías del ejemplo de venenos y antídotos explicado en la Sección \@ref(sec:ANOVA2) se satisface que cada combinación de veneno y antídoto define una población normal. Para ello, en principio deberíamos repetir 12 veces, una para cada combinación de veneno y antídoto, un par de instrucciones como las siguientes:



```

```{r}
time.I.A=rats[rats$poison=="I" & rats$treat=="A", ]$time
shapiro.test(time.I.A)$p.value
```


Para evitarnos trabajo, vamos a usar la estrategia explicada al final del ejemplo anterior: aplicaremos la función  `Test.SW` que hemos definido en dicho ejemplo a la variable numérica `time` del *data frame* `rats`  separándola por las combinaciones de veneno y antídoto.

```{r}
aggregate(time~poison:treat, data=rats, FUN=Test.SW)
```
Todos los p-valores que obtenemos son mayores que 0.05 (y de hecho, para garantizar un nivel de significación global de 0.05, deberíamos realizar cada test de Shapiro-Wilk con un nivel de significación 0.05/12=0.0042), por lo que no podemos rechazar que 
todas las combinaciones de veneno y antídoto definan unos tiempos de supervivencia que sigan leyes normales.

Pasemos al contraste de igualdad de varianzas (su nombre técnico es **contraste de homocedasticidad**), en el que usaremos el llamado **test de Bartlett**. Supongamos,  para fijar ideas, que tenemos $k$ vectores numéricos
$$
x_i=(x_{i1},x_{i2},\ldots,x_{in_i}),\quad i=1,\ldots, k,
$$
cada uno de los cuales es una muestra aleatoria simple de una variable aleatoria normal (se supone que  lo hemos contrastado previamente) de varianza $\sigma_i^2$. El contraste que queremos realizar es:
$$
\left\{
\begin{array}{ll}
H_0: & \sigma_1^2 = \cdots = \sigma_k^2\\
H_1: & \mbox{$\sigma_i^2 \neq \sigma_j^2$ para algunos $i, j$}
\end{array}
\right.
$$
Para realizar este contraste, se usa el **estadístico de Bartlett**:
$$
K^2 = \frac{ (N-k)\ln (\widetilde{s}_p^2)-\sum_{i=1}^k (n_i -1)\ln (\widetilde{s}_i^2)}{ 1+\frac{1}{3(k-1)}\left(\sum_{i=1}^k \left(\frac{1}{n_i
-1}\right)-\frac{1}{N-k}\right)},
$$
donde 
$$
N=\sum_{i=1}^k n_i,\quad \widetilde{s}_p^2 = \frac{\sum_{i=1}^k (n_i -1) \widetilde{s}_i^2}{N-k},\quad \widetilde{s}_i^2 =\frac{\sum_{j=1}^{n_i} (x_{ij}-\overline{x}_{i})^2}{n_i -1},\ i=1,\ldots,k.
$$
Si las variables poblacionales son normales, este estadístico  sigue aproximadamente una distribución $\chi^2_{k-1}$, y si la hipótesis nula es verdadera, su valor esperado es pequeño (puesto que, en este caso, los valores esperados de $\widetilde{s}_1^2,\ldots, \widetilde{s}_k^2$ y $\widetilde{s}_p^2$ son todos iguales). El p-valor del contraste es entonces $P(\chi^2_{k-1} > K^2)$.

R tiene una función que efectúa este test, `bartlett.test`, y puede aplicarse a una fórmula. Por ejemplo, para contrastar si las tres dietas en el ejemplo de la coagulación de la Sección \@ref(sec:ANOVA-1) definen poblaciones con la misma varianza, podemos entrar
```{r}
bartlett.test(coag~diet, data=coagulacion)
```
Como el p-valor es alto, concluimos que las varianzas de los tiempos de coagulación bajo las tres dietas son la misma.

En los ANOVA de dos vías, se ha de comprobar la igualdad de varianzas para las combinaciones de niveles de los dos factores. En la fórmula que se entra al `bartlett.test` esta combinación no se puede especificar con `:`, como hemos hecho hasta ahora, sino con  `interaction`. Por ejemplo, para contrastar por medio del test de Bartlett si las 12 combinaciones veneno-dieta en el ejemplo de venenos y antídotos  explicado en la Sección \@ref(sec:ANOVA2)  tienen la misma varianza, tenemos que entrar:
```{r}
bartlett.test(time~interaction(poison,treat), data=rats)
```
Obtenemos un p-valor de $`r round(bartlett.test(time~interaction(poison,treat), data=rats)$p.value,7)`$, por lo tanto, hemos de rechazar la igualdad de varianzas: las conclusiones de ese ANOVA de dos vías no tienen en principio ninguna validez, puesto que no hay evidencia de que se cumpla una de las condiciones para realizarlo.

Otros tests de igualdad de varianzas son el de  Fligner-Killeen, implementado en la función 
`fligner.test` (y que tiene la misma particularidad que  `bartlett.test` a la hora de especificar combinaciones de factores) y el test de Levene, implementado en la función 
`leveneTest` del paquete **car** (donde las combinaciones de factores sí que se pueden especificar con `:`). En cada caso, el test produce un p-valor con el significado usual. El test de Levene es muy popular, porque no requiere que las poblaciones sean normales, pero  en el contexto del ANOVA esta propiedad no tiene ningún interés; además, cuando las poblaciones son normales, el test de Bartlett es más potente que el de Levene. 



## Comparaciones de pares de medias {#sec:pares}

Si en un ANOVA hemos rechazado la hipótesis nula de la igualdad de todas las medias, es posible que nos interese determinar qué medias son diferentes. Para ello podemos llevar a cabo algunos tests. En esta sección explicamos las instrucciones de R para tres de ellos.


### Tests t por parejas

En los tests t por parejas, realizamos un contraste de medias para cada par de niveles del factor, teniendo en cuenta que, para garantizar un nivel de significación global $\alpha$, se ha de ajustar el nivel de significación de cada test. Hay diversos métodos para llevar a cabo este ajuste: los  más populares son el método de Bonferroni, el de Holm, que es más potente, y el de Hochberg, que, para contrastes ANOVA que no sean de bloques, aún tiene mayor potencia.

La función que efectúa  tests t por parejas es `pairwise.t.test`. Su sintaxis es
```{r,eval=FALSE}
pairwise.t.test(var.numérica, factor, paired=..., p.adjust.method=...)
```
El valor de `p.adjust.method` es el método de ajuste de p-valores que deseamos usar, y se ha de entrar entrecomillado. El valor por defecto es el de Holm, indicado por `"holm"` aunque no es necesario especificarlo. Para usar el de Bonferroni, hay que especificar `p.adjust.method="bonferroni"`, y para usar el de Hochberg, `p.adjust.method="hochberg"`. Si no queréis ningún ajuste, tenéis que  usar `p.adjust.method="none"`. Para conocer qué otros métodos se pueden usar, sus ventajas e inconvenientes, consultad  la Ayuda de `p.adjust`.

Por lo que refiere al parámetro `paired`, sirve para indicar si las muestras son independientes (igualándolo a `FALSE`, que es su valor por defecto) o emparejadas  (igualándolo a `TRUE`); cuando el ANOVA es de bloques, las muestras son emparejadas, y por lo tanto
se ha de especificar `paired=TRUE`.

Veamos algunos ejemplos de su uso.

```{example}
Para determinar cuáles de las tres dietas en el ejemplo de la coagulación de la Sección \@ref(sec:ANOVA-1) dan tiempos medios de coagulación diferentes, vamos a efectuar un test t por parejas de Bonferroni.


```

```{r}
pairwise.t.test(coagulacion$coag,coagulacion$diet,p.adjust.method="bonferroni")
```
Los p-valores del resultado ya han sido ajustados, y por lo tanto se han de comparar directamente con el nivel de significación $\alpha$ global. Por ejemplo, con un nivel de significación $\alpha=0.05$, obtenemos evidencia de que los tiempos medios para los pares de dietas (A,B), (A,C), (B,D), (C,D) son diferentes, y en cambio no hay evidencia de que los tiempos medios para los pares (A,D) y (B,C) sean diferentes.


```{example}
Para determinar qué actividades de evaluación  de las consideradas en el Ejemplo \@ref(exm:notes) dan notas medias diferentes, vamos a realizar un test t por parejas de Holm. Primero vamos a  comprobar que dichas actividades de evaluación definen poblaciones normales, condición necesaria para que los tests t por parejas tengan sentido.


```

Contrastaremos la normalidad de las notas para cada actividad de evaluación con el test de Shapiro-Wilk, y para no trabajar más de la cuenta, usaremos la técnica explicada en el Ejemplo \@ref(exm:SW-auto).
```{r}
Test.SW=function(x){round(shapiro.test(x)$p.value,4)}
aggregate(notas~acts, data=notas.bloques, FUN=Test.SW)
```
Todos los p-valores son altos,  los que nos permite aceptar que las notas de cada actividad provienen de una distribución normal. 

Así pues, podemos realizar el test t de Holm. En este caso tenemos que usar `paired=TRUE`, puesto que se trata de un ANOVA de bloques. 

```{r}
pairwise.t.test(notas.bloques$notas, notas.bloques$acts, paired=TRUE) 
```
Los p-valores inferiores a 0.05 son los de los pares de actividades "Ejercicios de casa"-"Control 1",
"Control 1"-"Control 2", "Ejercicios de casa"-"Tests" y "Control 1"-"Tests". Por lo tanto,  a un nivel de significación global de 0.05,
no podemos rechazar que las notas medias de estos pares de actividades sean iguales, mientras que en los otros tenemos evidencia de que son diferentes.


### Test de Duncan


Otro método popular para comparar los pares de medias es el test de Duncan.  Con R se puede efectuar con la instrucción  `duncan.test` del paquete **agricolae**. Su sintaxis básica es
```{r,eval=FALSE}
duncan.test(aov, "factor", alpha=..., group=...)$sufijo
```
donde

* `aov` es el resultado de la función `aov` (sin `summary`) con la que hemos calculado el ANOVA de partida. 

* El `factor` es el factor del ANOVA, y se ha de entrar  entrecomillado y con el mismo nombre que se ha usado en la fórmula del `aov`.

* El parámetro `alpha` sirve para entrar el nivel de significación $\alpha$: por defecto vale 0.05. 

* `group` puede valer `TRUE` o `FALSE`, y hace que el resultado se  presente de forma diferente.

* El `sufijo` tiene que ser `groups` si `group=TRUE` y `comparison` si `group=FALSE`.


Este test no se puede usar en el ANOVA de bloques, puesto que supone que las muestras de los tratamientos son independientes.



```{example}
Vamos a usar el test de Duncan para determinar qué pares de dietas en el ejemplo de la coagulación de la Sección \@ref(sec:ANOVA-1) tienen tiempos medios de coagulación diferentes, con un nivel de significación global del 5%. En primer lugar, guardamos en  un objeto  el ANOVA calculado con `aov`, y luego aplicamos la función  `duncan.test` a este objeto. Aquí lo haremos  dos veces, una para cada posible valor del parámetro `group`, para poder comparar y comentar cómo se muestran los resultados. No entraremos el parámetro `alpha`, puesto que usamos su valor por defecto.


```

```{r}
library(agricolae)
anova.coag=aov(coag~diet, data=coagulacion)
duncan.test(anova.coag, "diet", group=FALSE)$comparison
duncan.test(anova.coag, "diet", group=TRUE)$groups
```
Observemos los resultados:

* Con `group=FALSE` y sufijo `comparison`, obtenemos una tabla donde, para cada pareja de niveles del factor, se da, entre otros datos, un p-valor (la columna `p value`). Estos p-valores tienen el significado usual. En este ejemplo, de los p-valores  se desprende que no podemos rechazar que las medias para los pares A-D y B-C sean iguales (los p-valores son grandes), mientras que, en cambio, hay evidencia significativa de que el resto de los pares de dietas tienen medias diferentes (los p-valores son pequeños). Las dos últimas columnas de la tabla definen intervalos de confianza (del nivel de confianza correspondiente al nivel de significación que hayamos entrado con `alpha`) para las diferencias de la medias en el orden especificado en la primera columna.

* Con `group=TRUE` y sufijo `groups`, obtenemos una tabla donde se agrupan los niveles según la igualdad de medias. En este caso, asigna las dietas B y C al grupo `a` y las dietas A y D al grupo `b`: dietas en el mismo grupo podemos aceptar que tienen medias iguales, dietas en grupos diferentes podemos concluir que tienen medias diferentes.



### Método de Tukey

El **método HSD de Tukey** (las siglas HSD refieren a *Honestly Significant Difference*) es el método más preciso de comparación de parejas de medias para contrastes ANOVA de un factor en los que cada nivel tenga el mismo número de observaciones. Es un test similar al test t, pero usa otro estadístico con una distribución diferente, no vamos a entrar en detalles. Con R se efectúa aplicando la función `TukeyHSD` al resultado de `aov`. 

Para explicar qué información obtenemos con esta función, vamos a aplicar el método de Tukey al ANOVA del ejemplo sobre tiempos de coagulación de la Sección \@ref(sec:ANOVA-1).
```{r}
anova.coag=aov(coag~diet, data=coagulacion)
TukeyHSD(anova.coag)
```


De esta manera, para cada par de niveles obtenemos, entre otra información:

*  La diferencia de sus medias muestrales, en la columna `diff`.

* Los extremos inferior y superior de un intervalo de confianza del 95% para la diferencia de medias poblacionales en el orden dado en la primera columna, en las  columnas `lwr` y `upr`, respectivamente.  El nivel de confianza se puede ajustar con el parámetro `conf.level` dentro de la función.

* El p-valor del contraste de igualdad de medias correspondiente, en la columna `p adj`.

En nuestro ejemplo, los intervalos de confianza para las diferencias de medias de los pares A-B, A-C, B-D y C-D no contienen el valor 0 (y los  p-valores correspondientes son pequeños), lo que indica que podemos rechazar que estas medias sean iguales. En cambio, los intervalos de confianza para las diferencias de medias de los pares A-D y B-C sí que contienen el 0 (y los  p-valores correspondientes son grandes), lo que indica que podemos aceptar que estas medias son iguales. En este caso obtenemos, por lo tanto, la misma conclusión que con el test t de Bonferroni.


## Métodos no paramétricos

Cuando no se satisfacen las condiciones para poder aplicar un ANOVA, hay que usar algún otro método que no las requiera.

En el caso del ANOVA de una vía, el método alternativo no paramétrico recomendado es el **test de Kruskal-Wallis**, que generaliza a más de dos muestras el test de Mann-Whitney-Wilcoxon para dos muestras independientes en el mismo sentido que el ANOVA generaliza el test t; en particular, un test de Kruskal-Wallis para un factor con solo dos niveles es equivalente al test de Mann-Whitney. Con R, el test de Kruskal-Wallis se lleva a cabo con la función `kruskal.test`, con la misma sintaxis que `aov`. Naturalmente, el resultado no es una tabla ANOVA, porque no estamos haciendo un ANOVA, pero da un p-valor que indica si podemos rechazar o no que las medias por niveles sean todas iguales.

Por ejemplo, para efectuar el test de Kruskal-Wallis sobre la tabla de tiempos de coagulación del principio de la Sección \@ref(sec:ANOVA-1), entraríamos:
```{r}
kruskal.test(coag~diet, data=coagulacion)
```


El p-valor nos da evidencia estadísticamente significativa de que no todos los tiempos medios de coagulación son iguales. Para determinar entonces  qué pares de dietas dan tiempos medios diferentes, podríamos llevar a cabo un contraste por parejas. En principio no podremos usar un test t por parejas, ya que este presupone las condiciones de normalidad de las poblaciones e igualdad de las varianzas que son necesarias para el ANOVA y que suponemos que no se satisfacen (o habríamos hecho un ANOVA en vez de un test de Kruskal-Wallis). En su lugar, podemos usar un test de Mann-Whitney por parejas, con la función `pairwise.wilcox.test`. Su sintaxis es la misma que la de `pairwise.t.test`. En particular, hay que indicar con `p.adjust.method` el método de ajuste de los p-valores, siendo de nuevo `"holm"` su valor por defecto. Así, por ejemplo, para 
realizar un test de Mann-Whitney por parejas con ajuste de Bonferroni, entraríamos:

```{r,warning=FALSE}
pairwise.wilcox.test(coagulacion$coag,coagulacion$diet,p.adjust.method="bonferroni")
```

Con un nivel de significación de 0.05, obtenemos evidencia de que los pares de dietas (A,B), (A,C) y (C,D) dan tiempos medios de coagulación diferentes, y no podemos rechazar que los otros pares de dietas den el mismo tiempo de coagulación. El resultado ha sido ligeramente diferente del test t por parejas, donde también obteníamos evidencia de diferencia de medias entre las dietas B y D. Recordad que, en general, los métodos no paramétricos tienen potencia menor y por lo tanto les cuesta más detectar diferencias.

Por lo que refiere al ANOVA de bloques completos aleatorios, el método no paramétrico alternativo más popular es el **test de Friedman**, que generaliza a más de dos muestras el test de Wilcoxon para muestras emparejadas. Está implementado en la función `friedman.test`, cuya  sintaxis es de nuevo la misma que la de `aov` excepto que la suma de los tratamientos y los bloques no se indica en la fórmula con un signo de suma sino con una barra vertical. 

Por ejemplo, para efectuar el test de Friedman sobre la tabla de datos de notas en actividades de evaluación del Ejemplo \@ref(exm:notes), entraríamos:
```{r}
friedman.test(notas~acts|bloques, data=notas.bloques)
```


El p-valor nos indica que hay evidencia estadísticamente significativa de que algunas notas medias fueron diferentes. Para determinar cuáles, podemos llevar a cabo un test de Wilcoxon por parejas, usando la función `pairwise.wilcox.test` con `paired=TRUE`. Por ejemplo, usando de nuevo el ajuste de Bonferroni:

```{r}
pairwise.wilcox.test(notas.bloques$notas, notas.bloques$acts, 
   paired=TRUE, p.adjust.method="bonferroni")
```


Encontramos evidencia de que las notas medias de los Talleres fueron diferentes de las del resto de actividades, y no podemos rechazar que las notas medias de las otras actividades fueran todas iguales.

Finalmente, en el caso de experimentos con dos o más factores, no hay una solución simple cuando no se satisfacen las condiciones necesarias para poder hacer un ANOVA. Una posible salida es usar un método adecuado de *bootstrap*. Sin entrar en detalles, una buena opción en un experimento de dos factores es usar la función `pbad2way` del paquete **WRS2**. Su sintaxis básica es
```{r,eval=FALSE}
pbad2way(fórmula, est="median", nboot=...)
```
En el parámetro `nboot` se ha de entrar el número de muestras que se han de tomar; tened en cuenta que cuántas más toméis, mas tardará la ejecución. El parámetro `est` indica qué parámetro se quiere comparar, la media no está entre los valores disponibles así que aquí usaremos la mediana. Consultad la Ayuda de la función (y las referencias que se citan) para saber qué otros estadísticos podéis usar.

Por ejemplo, para llevar a cabo por medio de un *bootstrap* el contraste del Ejemplo \@ref(exm:tooth2) sobre el efecto de la vitamina C en el crecimiento de los dientes (y recordando que la tabla `ToothGrowth2` contiene los datos con los tratamientos codificados como factores)
podríamos entrar:
```{r}
library(WRS2)
set.seed(42) #Fijamos la semilla de aleatoriedad, para que sea reproducible
pbad2way(len~supp*dose, data=ToothGrowth2, est="median", nboot=500)
```


 *Grosso modo*, los p-valores obtenidos tienen el significado usual en un *bootstrap*: la fracción de muestras en las que obtenemos un valor de un cierto estadístico tan o más extremo que el de la muestra global. Un p-valor pequeño nos da evidencia de que podemos rechazar la hipótesis nula. Los p-valores de las filas con los nombres de  los factores son los de los contrastes de los mismos y el p-valor de la última fila es el del contraste de interacción. Por lo tanto, con este contraste concluimos que hay evidencia de que tanto la dosis de vitamina C como el método de suministrarla influyen en el crecimiento de los odontoblastos (en concreto, en su *mediana*) y que hay interacción entre ambos factores.

Como en el ANOVA, el contraste de la combinación de factores se ha de efectuar aparte: en este caso (ya que no podemos llevar a cabo un ANOVA, o no nos estaríamos complicando tanto la vida) una buena opción es usar un test de Kruskal-Wallis tomando como factor la combinación de factores (entrada en la fórmula por medio de `interaction`).

```{r}
kruskal.test(len~interaction(supp,dose),data=ToothGrowth2)
```


Concluimos que también hay evidencia de que la combinación de la dosis de vitamina C y el método de suministrarla influye en el crecimiento de los odontoblastos.

Para terminar, una advertencia sobre `pbad2way`: a veces da un error numérico, como en la siguiente ejecución en el otro ejemplo de experimento de 2 vías de la Sección \@ref(sec:ANOVA2):


```{r,error=TRUE}
pbad2way(time~poison*treat, data=rats, est="median", nboot=500)
```

En este caso, el inventor del método recomienda emplear el parámetro `pro.dis=TRUE`, que modifica el método de cálculo de los estadísticos que se usan en el *bootstrap* y evita el error.

```{r}
set.seed(42)
pbad2way(time~poison*treat, data=rats, est="median",nboot=500,pro.dis=TRUE)
```


## Guía rápida


* Fórmulas:
     *  `X~F1` ajusta la variable  `X` al factor `F1`.

     *    `X~F1+F2` ajusta la variable    `X`  a los factores `F1` y `F2` sin tener en cuenta la interacción entre los factores.


     * `X~F1*F2` ajusta la variable    `X`  a los factores `F1` y `F2`  teniendo en cuenta además la interacción entre los factores. 

     * `X~F1:F2`  ajusta la variable    `X`  a un factor que tiene como niveles los pares de niveles de  `F1` y `F2`.


* `options(show.signif.stars=...)` sirve para desactivar (usando `FALSE`) o activar (con `TRUE`) los códigos de significación por medio de estrellitas.


* `aov(fórmula)` efectúa el ANOVA indicado por la `fórmula`. El parámetro adicional `data` sirve para indicar el *data frame* del que se extraen los vectores y factores de la `fórmula`.

* `summary(aov(...))` muestra la tabla del ANOVA efectuado con la función `aov`. Añadiendo el sufijo `[[1]]$"Pr(>F)"` nos da directamente la columna de p-valores de la tabla.

* `anova(lm(fórmula, data=...))` también produce la tabla del ANOVA  indicado por la `fórmula`.
Con el sufijo `$"Pr(>F)"`, nos da directamente la columna de p-valores de la tabla.


* `interaction.plot(factor1,factor2,vector)` produce el gráfico de interacción  del `vector` respecto de los dos factores.

* `subset(dataframe,condición)` define un *data frame* con las filas del `dataframe`  que cumplen la `condición`.

* `shapiro.test(x)` realiza el test de normalidad de Shapiro-Wilk sobre el vector numérico `x`.

* `aggregate` sirve para aplicar una función a una o varias variables de un *data frame* agrupando sus entradas por los niveles de uno o varios factores.

* `bartlett.test`, con la misma sintaxis que `aov`, efectúa el test de Bartlett de homocedasticidad.

* `pairwise.t.test(x,factor)` efectúa un test t por parejas de la variable `x` separada por los niveles del `factor`. Dos parámetros importantes:

     * `paired`: se ha de igualar a `TRUE` en los ANOVA de bloques.
     
     * `p.adjust.method`: sirve para indicar el método de ajuste de los p-valores.


* `duncan.test(aov,factor)$sufijo` del paquete **agricolae**, efectúa el test de Duncan del ANOVA `aov`. Sus dos parámetros más importantes para nosotros son  `alpha`, que sirve para indicar el nivel de significación y  `group`, que sirve para indicar cómo queremos que muestre el resultado. El `sufijo` ha de ser  ser `groups` si `group=TRUE` y `comparison` si `group=FALSE`.


* `TukeyHSD(aov)` efectúa el test HSD de Tukey del ANOVA `aov`.

* `kruskal.test` efectúa el test de Kruskal-Wallis. Su sintaxis es la misma que la de `aov`.

* `pairwise.wilcox.test` efectúa un test de Wilcoxon (con `paired=TRUE`) o de Mann-Whitney-Wilcoxon (con  `paired=FALSE`, el valor por defecto de este parámetro)  por parejas. Su sintaxis es la misma que la de  `pairwise.t.test`.
 
* `friedman.test` efectúa el test de Friedman. Su sintaxis es la misma que la de `aov`.

* `pbad2way(fórmula, data=..., est="median")` del paquete **WRS2** lleva a cabo un contraste *bootstrap* de medianas de los datos de un experimento factorial de 2 vías. El parámetro `nboot` sirve para indicar el número de muestras a usar.


## Ejercicios


### Test {-}

*(1)* En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones,  A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población  han sido los siguientes:  A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un ANOVA de 1 vía sobre estos datos. Tenéis que dar el p-valor del contraste de la igualdad de las medias redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las medias. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco.


*(2)* En un experimento queremos comparar la influencia de cuatro tratamientos,  A, B, C y D, sobre una determinada variable fisiológica. Para ello hemos aplicado cada tratamiento a los mismos 5 individuos, pero en órdenes y momentos escogidos al azar, y hemos medido en ellos la variable en cuestión tras cada tratamiento. Los resultados para cada tratamiento (ordenados en cada uno en el mismo orden, correspondiente a los individuos) han sido los siguientes:  A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un ANOVA de bloques sobre estos datos. Tenéis que dar el p-valor del contraste de la igualdad de las medias de los tratamientos redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, la igualdad de todas las medias de los tratamientos. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco.


*(3)* En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones,  A, B, C y D; los individuos de cada población además pueden clasificarse en dos tipos, X e Y, según una determinada característica que creemos que será relevante en nuestro estudio. Hemos tomado una muestra de 5 individuos de cada combinación población-característica y hemos medido en ellos la variable en cuestión. Los resultados para cada combinación población-característica   han sido los siguientes:  A-X=(29.2,30.9,31.3,30.0,30.0), B-X=(29.1,31.3,31.4,34.3,32.3), C-X=(31.3,40.7,32.6,34.8, 38.3), D-X=(32.2,36.0,31.6,31.8,32.6), A-Y=(26.3,25.9,25.6,25.3,24.9), B-Y=(30.7,29.8, 27.9,28.8,28.1), C-Y=(26.2,26.7,27.2,29.3,28.4), y D-Y=(20.5,26.2,27.9,23.3,27.8). Realizad un ANOVA de 2 vías sobre estos datos. Dad el p-valor del contraste de interacción entre el tratamiento y la característica redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decid si podéis concluir (con un SI) o no (con un NO), con un nivel de significación del 5%, que hay interacción entre el tratamiento y la característica. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco.




*(4)* En un experimento queremos comparar el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población  han sido los siguientes:  A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). Realizad un test de Bartlett de homocedasticidad sobre estos datos, para saber si las varianzas poblacionales de las cuatro poblaciones son todas iguales o no. Tenéis que dar el p-valor del contraste de la igualdad de las varianzas redondeado a 3 cifras decimales (sin ceros innecesarios a la derecha) y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%,  la igualdad de todas las varianzas. Dad el p-valor y la conclusión en este orden, separados por un único espacio en blanco.


*(5)* En un experimento hemos comparado el valor medio de una cierta variable en cuatro poblaciones, A, B, C y D. Hemos tomado una muestra de 5 individuos de cada población y hemos medido en ellos la variable en cuestión. Los resultados para cada población  han sido los siguientes:  A=(23.1,17.2,20.1,19.5,18.8), B=(22.6,15.5,19.3,18.5,16.2), C=(21.1,26.4,24.7,21.4,20.9) y D=(20.4,24,23.4,24.9,21.5). 
Realizad un test t por parejas de Bonferroni sobre estos datos con  un nivel de significación del 5%, para determinar qué pares de poblaciones podemos concluir que tienen medias diferentes con este nivel de significación. Tenéis que decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%,  la igualdad de las medias de las poblaciones B y C.



### Respuestas al test {-}


*(1)*  0.013 SI

Fijaos en que en el enunciado nos dan la tabla de datos siguiente por columnas:

```{r,echo=FALSE}
knitr::kable(data.frame(
A=c(23.1,17.2,20.1,19.5,18.8),
B=c(22.6,15.5,19.3,18.5,16.2),
C=c(21.1,26.4,24.7,21.4,21.9),
D=c(20.4,24.0,23.4,24.9,21.5)),
booktabs=TRUE)
```

Por lo tanto para entrarla de manera correcta como un *data frame* copiando los datos del enunciado hay que usar:

```{r}
x=c(23.1,17.2,20.1,19.5,18.8,
    22.6,15.5,19.3,18.5,16.2,
    21.1,26.4,24.7,21.4,21.9,
    20.4,24.0,23.4,24.9,21.5)
pobl=rep(c("A","B","C","D"),each=5) 
X=data.frame(x,pobl)
```

A partir de aquí, la respuesta la obtenemos con

```{r}
summary(aov(x~pobl,data=X))[[1]]$"Pr(>F)"
```

*(2)* 0.026 NO

Nosotros lo hemos resuelto con
```{r}
x=c(23.1,17.2,20.1,19.5,18.8,
    22.6,15.5,19.3,18.5,16.2,
    21.1,26.4,24.7,21.4,21.9,
    20.4,24.0,23.4,24.9,21.5)
pobl=rep(c("A","B","C","D"), each=5)
bloques=as.factor(rep(1:5,4))
X.b=data.frame(x,pobl,bloques)
summary(aov(x~pobl+bloques,data=X.b))[[1]]$"Pr(>F)"[1]
```

*(3)* 0.0234 SI

Nosotros lo hemos resuelto con
```{r}
AX=c(29.2,30.9,31.3,30.0,30.0)
BX=c(29.1,31.3,31.4,34.3,32.3)
CX=c(31.3,40.7,32.6,34.8,38.3)
DX=c(32.2,36.0,31.6,31.8,32.6)
AY=c(26.3,25.9,25.6,25.3,24.9)
BY=c(30.7,29.8,27.9,28.8,28.1)
CY=c(26.2,26.7,27.2,29.3,28.4)
DY=c(20.5,26.2,27.9,23.3,27.8)
y=c(AX,BX,CX,DX,AY,BY,CY,DY)
fact.A=rep(rep(c("A","B","C","D"),each=5),2)
fact.X=rep(c("X","Y"),each=20)
DF=data.frame(y,fact.A,fact.X)
summary(aov(y~fact.A*fact.X))
```


*(4)* 0.881 NO

Nosotros lo hemos resuelto con
```{r}
bartlett.test(x~pobl,data=X)
```

*(5)* SI

Nosotros lo hemos resuelto con
```{r}
pairwise.t.test(X$x,X$pobl,p.adjust.method="holm")
```

```{r,include=FALSE}
options(show.signif.stars=TRUE)
```


<!--chapter:end:09-ANOVA.Rmd-->

# Regresión lineal {#chap:regresion}

En esta lección, explicamos el uso de R para realizar regresiones lineales, tanto simples como múltiples. Presentaremos algunos ejemplos para ilustrar el uso de las funciones de R específicas para esta técnica de modelado estadístico, así como la posterior validación y adecuación del modelo mediante el análisis de los residuos. Aunque en la Lección 3 de la primera parte del curso ya introdujimos la regresión lineal sin entrar en mucha profundidad, es en esta lección donde desarrollaremos más ámpliamente este tema.

## El modelo de regresión lineal en R {#sec:1}

Uno de los problemas recurrentes en estadística es determinar a partir de un conjunto de observaciones de variables si existe alguna relación funcional entre una de las variables, llamada **variable dependiente** o **de respuesta**, y el resto de variables, conocidas como **variables independientes** o **de control**. Los objetivos de encontrar esta relación funcional son, por un lado, entender cómo los valores de la variable dependiente "dependen" de los de las variables independientes y, por otro lado, poder estimar el valor de la variable dependiente sobre un sujeto para el que conozcamos sus valores de las variables independientes. En esta lección nos centramos en el caso en que esta relación funcional es lineal. 

Formalmente, la situación es la siguiente. Sean $X_1$, ..., $X_k$ *k* variables (no necesariamente aleatorias), que serán las independientes, y sea $Y$ la variable dependiente. Llamemos $Y|{x_1,\ldots, x_k}$ a la variable aleatoria que nos da los valores de $Y$ sobre los individuos en los que cada variable independiente $X_i$ toma el correspondiente valor $x_i$, y sea $\mu_{Y|{x_1,\ldots, x_k}}$ el valor esperado de $Y|{x_1,\ldots, x_k}$. Vamos a suponer que $\mu_{Y|{x_1,\ldots, x_k}}$ es una función lineal de $X_1$, ..., $X_k$, y que por lo tanto "en la realidad" existen unos coeficientes reales $\beta_0$, $\beta_1$, ..., $\beta_k$, que desconocemos, tales que, para cada posible vector de valores $(x_1,\ldots,x_k)$ de las variables independientes $X_1$, ...,$X_k$, se tiene que  
$$
\mu_{Y|{x_1,\ldots, x_k}}=\beta_0+\beta_1x_1+\cdots+\beta_kx_k.
$$

De esta manera, si denotamos por $E_{x_1,\ldots,x_k}$ la variable **error**
$$
Y|{x_1,\ldots, x_k}-\mu_{Y|{x_1,\ldots, x_k}}
$$
que, para cada individuo cuyas variables $X_i$ valen $x_i$,  nos da la diferencia entre su valor de $Y$ y el valor esperado de esta variable para los individuos con sus mismos valores de $X_1$, ..., $X_k$, tenemos que
$$
Y|{x_1,\ldots, x_k}=\beta_0+\beta_1x_1+\cdots+\beta_kx_k+E_{x_1,\ldots,x_k}.
$$
Esta ecuación nos dice que el valor de $Y$ sobre un sujeto para el que $X_1=x_1$, ..., $X_k=x_k$, viene dado por dos componentes: por un lado, una componente "fija" definida por el **modelo lineal** $\mu_{Y|{x_1,\ldots, x_k}}=\beta_0+\beta_1x_1+\cdots+\beta_kx_k$, y por otro lado, el error aleatorio $E_{x_1,\ldots,x_k}$. Por la linealidad  de las esperanzas, como el valor esperado de $Y|{x_1,\ldots, x_k}$ es $\beta_0+\beta_1x_1+\cdots+\beta_kx_k$, el valor esperado del error $E_{x_1,\ldots,x_k}$ es 0.


Si disponemos entonces de un conjunto de *n* datos 
$$
(x_{i1},x_{i2},\ldots,x_{ik},y)_{i=1,\ldots,n}
$$ 
donde cada vector $(x_{i1},x_{i2},\ldots,x_{ik},y)$ está formado por los valores de las variables $X_1$, ..., $X_{k}$ e $Y$ sobre un individuo,
podemos usar estos datos para estimar los valores poblacionales de $\beta_0$, $\beta_1$, ..., $\beta_k$. Vamos a suponer que $n>k$, por lo que no podemos esperar encontrar los valores de $\beta_0$, $\beta_1$, ..., $\beta_k$ simplemente resolviendo un sistema de ecuaciones. 

Sean $b_0$, $b_1$, ..., $b_k$ nuestras estimaciones de  $\beta_0$, $\beta_1$, ..., $\beta_k$, respectivamente, a partir de nuestro conjunto de datos. Podemos escribir entonces la **función lineal de regresión**
$$
\widehat{y}=b_0+b_1x_{1}+\cdots+b_kx_{k}
$$
que nos permite estimar por medio de $\widehat{y}$ el valor de $Y$ que esperamos que tenga un sujeto para el que las variables  $X_1$, ..., $X_{k}$ valgan, respectivamente, $x_1$, ..., $x_{k}$. En particular, para cada $i=1,\ldots,n$, llamaremos $\widehat{y}_i$ al valor de $Y$ que estimamos sobre el sujeto $i$-ésimo de nuestro conjunto de datos,  en el que las variables  $X_1$, ..., $X_{k}$ valen, respectivamente, $x_{i1}$, ..., $x_{ik}$:
$$
\widehat{y}_i=b_0+b_1x_{i1}+\cdots+b_kx_{ik}.
$$
Para cada $i=1,\ldots,n$, denotaremos por $e_i$ el **error** cometido con la estimación sobre el individuo *i*-ésimo de nuestro conjunto de datos, definido como la diferencia 
$$
y_i-\widehat{y}_i=y_i-(b_0+b_1x_{i1}+\cdots+b_kx_{ik})
$$ 
entre el valor $y_i$ de la variable $Y$ observado sobre dicho sujeto *i*-ésimo  y nuestra estimación $\widehat{y}_i$ de este valor por medio de la función lineal que hemos encontrado. 


La mayoría de las estrategias usadas para calcular las estimaciones $b_0$, $b_1$, ..., $b_k$ se basan en preestablecer un cierto criterio de “bondad” de la estimación de cada $y_i$ por medio del correspondiente $\widehat{y}_i$ y encontrar entonces  los coeficientes que dan las “mejores” estimaciones según este criterio. El criterio más utilizado en este sentido es el de **mínimos cuadrados**, en el que se minimiza la suma de los cuadrados de los errores cometidos sobre los sujetos de nuestro conjunto de datos. Es decir, en el **método de mínimos cuadrados** se toman como estimaciones de $\beta_0$, $\beta_1$, ..., $\beta_k$ los valores $b_0$, $b_1$, ..., $b_k$ para los que la suma
$$
SS_e=\sum_{i=1}^n e_i^2=\sum_{i=1}^n(y_i-b_0-b_1x_{i1}-\cdots-b_kx_{ik})^2 
$$
toma su valor mínimo: de ahí la coletilla de **mínimos cuadrados**. Como es el único método que vamos a explicar, a partir de ahora siempre que hablemos de **regresión lineal** nos referiremos a esta regresión lineal por el método de mínimos cuadrados. Además, hablaremos de **regresión lineal simple** si $k=1$ (es decir, cuando usamos una única variable independiente) y de **regresión lineal múltiple** si $k>1$ (es decir, cuando usamos más de una variable independiente). 

La función básica de R para realizar una regresión lineal  es `lm`. Su sintaxis básica es
```{r, eval=FALSE}
lm(formula,data=..., subset=...)
```

donde:

* `formula` refiere a una **fórmula** que relaciona la variable respuesta y las variables independientes del modelo, en el sentido de las fórmulas introducidas en la Sección \@ref(sec:modelos). Su estructura en este caso ha de ser la siguiente. En primer lugar, se escribe la variable dependiente, que necesariamente tiene que ser una variable numérica. A continuación, el símbolo ~ que indica la dependencia de esta variable  respecto de las variables que se indiquen a su derecha. Finalmente, se incluyen las variables independientes separadas por símbolos +.  

*  `data` es un parámetro opcional que sirve para especificar, si es necesario, el *data frame* al que pertenecen  las variables  utilizadas en la fórmula.

* `subset` es otro parámetro opcional que sirve para especificar que la regresión sólo tenga en cuenta un subconjunto de las observaciones, definido mediante alguna condición lógica.


Así, por ejemplo, si tenemos un *data frame* llamado `DF`, con una variable numérica `Y` y tres variables independientes `X1`, `X2` y `X3`, para realizar la regresión lineal de la variable  `Y` respecto de `X1`, `X2` y `X3` se puede ejecutar
```{r,eval=FALSE}
lm(Y~X1+X2+X3,data=DF)
```
o
```{r,eval=FALSE}
lm(DF$Y~DF$X1+DF$X2+DF$X3)
```

Un atajo muy útil en la ejecución de `lm` cuando nuestro *data frame* `DF` tiene muchas variables y queremos obtener la funció de regresión lineal de una variable, `Y`, respecto de todas las otras es entrar

```{r,eval=FALSE}
lm(Y~.,data=DF)
````

El punto a la derecha de la ~ es una abreviatura de "la suma de todas las variables de `DF` diferentes de `Y`".

El siguiente ejemplo ilustra el uso de la función `lm` y de la salida que proporciona.

```{example,label="exempDavis"}
La tabla de datos `Davis` del paquete **car** contiene datos del peso y la altura de 200 hombres y mujeres que realizan ejercicio habitualmente obtenidos de dos formas distintas: los valores medidos con los instrumentos adecuados (báscula y escaliómetro, respectivamente) y los valores que notificaron estas personas antes de realizarse las mediciones.^[Véase C. Davis.  "Body image and weight preocuppation: A comparison between exercising and non-exercising women." *Appetite* 15 (1990), pp. 13-21.] Veamos su estructura.


```


```{r}
library(car)
str(Davis)
```

En la Ayuda del objeto nos enteramos de que el factor `sex` indica el sexo del individuo, con niveles "F" y "M"; las variables cuantitativas `weight` y `height` indican, respectivamente, el peso (en kg.) y la altura (en cm.) *medidas*; y las variables cuantitativas  `repwt` y `repht` indican, respectivamente, el peso (en kg.) y la altura (en cm.) *notificadas* por el individuo. Además, en la Descripción se nos avisa de que hay valores que faltan.

En este primer ejemplo, nos centramos en las variables `weight` y `repwt` y vamos a suponer que el peso esperado de una persona que realiza ejercicio habitualmente (nuestra **población** de interés) es función lineal del peso que dice que tiene, y vamos a estimar los coeficientes de esta función lineal. De esta manera, cuando  una persona de la población de interés nos diga su peso, podremos estimar su peso real.  

Lo primero que haremos será extraer un *data frame* formado por estas dos columnas, a las que renombraremos `pesos`y `pesosnotif`, respectivamente. Además, por si estas variables tienen valores NA, aplicaremos al *data frame* la función `na.omit` para eliminar filas que no correspondan a observaciones completas.

```{r}
datospeso=data.frame(Davis$weight,Davis$repwt)
colnames(datospeso)=c("peso","pesonotif")
head(datospeso)
datospeso=na.omit(datospeso)
dim(datospeso)
```

Han sobrevivido `r dim(datospeso)[1]` de las 200 filas originales.

Para calcular la recta de regresión por mínimos cuadrados de la variable `peso` respecto de la variable `pesonotif`, usaremos la función `lm`. 
```{r}
lm(peso~pesonotif,data=datospeso)
```
El resultado obtenido significa que la recta de regresión buscada tiene término independiente (`Intercept`, el valor en el que la recta resultante corta el eje de ordenadas) $b_0=`r round(lm(peso~pesonotif,data=datospeso)$coefficients[1],4)`$ y coeficiente de la variable `peso` $b_1=`r round(lm(peso~pesonotif,data=datospeso)$coefficients[2],4)`$. Es decir, tenemos
$$
\widehat{\hbox{peso}}=`r round(lm(peso~pesonotif,data=datospeso)$coefficients[1],4)`+`r round(lm(peso~pesonotif,data=datospeso)$coefficients[2],4)`\cdot \hbox{pesonotif}.
$$

Vamos a representar gráficamente los puntos de la muestra conjuntamente con la recta de regresión encontrada: 
```{r, fig.cap='Gráfico de los pares de observaciones (pesonotif,peso) junto con la correspondiente recta de regresión.'}
plot(datospeso$pesonotif, datospeso$peso, pch=20, xlab="peso notificado", ylab="peso medido")
abline(lm(peso~pesonotif,data=datospeso), col="red")
```
Como se puede observar, la recta de regresión se ajusta notablemente a los puntos de la muestra indicando un buen comportamiento del modelo a nivel visual. Sin embargo, destaca una de las observaciones que se encuentra muy alejada tanto del patrón seguido por el resto de puntos como de la recta de regresión. Para obtener a qué observación corresponde ese punto, podríamos utilizar la función `identify` de R. Si inmediatamente después de generar el gráfico anterior ejecutáis
```{r,eval=FALSE}
identify(datospeso$pesonotif,datospeso$peso)
````
al pulsar con el cursor sobre un punto del gráfico podéis saber sus coordenadas. Como en este documento esto es imposible, lo que haremos será determinar ese punto anómalo como el que tiene ordenada mayor que 160:

```{r}
which(datospeso$peso>160)
datospeso[which(datospeso$peso>160),]
```
El punto anómalo, correspondiente al individuo 12 de la muestra, ¡dijo que pesaba 56 kg y en realidad pesaba 166! Más adelante, en la Sección \@ref(sec:diagn), trataremos el procedimiento a seguir en estas situaciones.


Veamos un segundo ejemplo.

```{example,label="exempUSA"}
El fichero de datos "USA2012.txt" que podéis descargar desde el enlace  https://raw.githubusercontent.com/AprendeR-UIB/Material/master/USA2012.txt contiene datos demográficos, sociales y económicos de los 50 estados de los Estados Unidos más el distrito de Columbia correspondientes al año 2012, el año que Barack Obama ganó sus segundas elecciones presidenciales. Estos datos han sido recopilados de diversas fuentes como el *United States Census Bureau*, el *Pew Research Center* o el *Bureau of Labor Statistics*. Vamos a cargarlo en un *data frame* y consultar su estructura.     


```


```{r}
library(RCurl)
datos=getURL("https://raw.githubusercontent.com/AprendeR-UIB/Material/master/USA2012.txt")
USA=read.table(text=datos,header=TRUE)
str(USA)
```

Calculemos, por ejemplo, la recta de regresión del porcentaje de voto a Obama (variable `obama`) con respecto al porcentaje de graduados universitarios del estado (variable `grad_univ`):
```{r}
lm(obama~grad_univ,data=USA)
```
Se obtiene la recta 
$$
\widehat{\hbox{obama}}=`r round(lm(obama~grad_univ,data=USA)$coefficients[1],3)`+`r round(lm(obama~grad_univ,data=USA)$coefficients[2],3)`\cdot \hbox{grad_univ}.
$$
Como podemos observar, a mayor porcentaje de graduados universitarios, el modelo nos predice un mayor porcentaje de voto a Obama. Añadamos ahora al modelo la variable `veteranos` que indica el porcentaje de veteranos de guerra en la población del estado. 
```{r}
lm(obama~grad_univ+veteranos,data=USA)
```
En este caso, la ecuación de regresión es
$$
\widehat{\hbox{obama}}=`r round(lm(obama~grad_univ+veteranos,data=USA)$coefficients[1],3)`+`r round(lm(obama~grad_univ+veteranos,data=USA)$coefficients[2],3)`\cdot \hbox{grad_univ}-`r abs(round(lm(obama~grad_univ+veteranos,data=USA)$coefficients[3],3))`\cdot \hbox{veteranos}.
$$
Así, mientras que un mayor porcentaje de graduados universitarios sigue correspondiéndose a un mayor porcentaje de voto a Obama, un mayor porcentaje de veteranos en la población lleva asociada una disminución de este porcentaje de voto. 


Ya hemos visto que los coeficientes de la recta de regresión se pueden obtener  simplemente ejecutando la función `lm`. Sin embargo, esta función nos puede proporcionar mucha información adicional, aplicando `summary` al resultado de `lm`. Volvamos al Ejemplo \@ref(exm:exempDavis).

```{r}
summary(lm(peso~pesonotif,data=datospeso))
```

En esta salida encontramos la siguiente información:

* En `Residuals`, se proporciona un resumen descriptivo de los residuos o errores $e_i$ del modelo, concretamente, sus valores mínimo y máximo y sus cuartiles.

* En la tabla de `Coefficients`, en la columna `Estimate` se dan las estimaciones de los  coeficientes de cada variable de la función de regresión, junto a sus respectivos errores típicos en la columna `Std. Error`. A continuación, las columnas `t value` y `Pr(>|t|)` proporcionan para cada coeficiente $\beta_i$ el valor del estadístico y el p-valor del contraste
$$
\left\{\begin{array}{ll} 
H_0:& \beta_i=0\\ H_1:& \beta_i\neq 0
\end{array}\right.
$$
R indica el significado estadístico de cada p-valor a su derecha mediante el código usual de estrellas.

* Después de la tabla, encontramos el `Residual standard error` que corresponde a la raíz del valor estimado de la varianza común de los residuos $S=\sqrt{\frac{SS_E}{n-k-1}}$, junto con los grados de libertad $n-k-1$.

* En la siguiente fila, tenemos los valores `Multiple R-squared` y `Adjusted R-squared`, es decir, los coeficientes de determinación $R^2$ y de determinación ajustado $R^2_{adj}$, respectivamente. Recordemos que 
$$
R^2=\dfrac{s_{\widehat{y}}^2}{s_{y}^2}, \quad R^2_{adj}=\frac{(n-1)R^2-k}{n-k-1}
$$
donde $s_{y}^2$  y  $s_{\widehat{y}}^2$ denotan la varianza de los valores de $y$ en nuestra muestra y de los valores de $y$ que predice nuestro modelo sobre los puntos de la muestra, respectivamente. Cuánto mejor aproxime la función de regresión el conjunto de puntos, más cercanos a 1 serán los valores de estos índices ya que representan la fracción de variabilidad de $y$ explicada por el modelo de regresión lineal. 

* En la última fila, aparecen el valor del estadístico $F$, los grados de libertad, 1 y  *n-k-1*, y el p-valor, en este orden, del siguiente contraste ANOVA:
$$
\left\{\begin{array}{ll} 
H_0:&\beta_1=\cdots=\beta_k=0\\ H_1:& \hbox{existe al menos un } \beta_i\neq 0
\end{array}\right.
$$ 
En la regresión lineal simple, este contraste es equivalente al contraste para $\beta_1$ dado en la tabla `Coefficients`. 


En este ejemplo concreto, hay que destacar que el valor de $R^2$ es `r round(summary(lm(peso~pesonotif,data=datospeso))$r.squared,4)`, un valor relativamente bajo. También podemos inferir que $\beta_1\neq 0$ ya que el p-valor correspondiente al contraste para la variable `pesonotif` es pequeño. 
Observad que si $\beta_1$ fuera 0,  los valores de la variable `peso` no dependerían de los de la variable `pesonotif` y el modelo carecería de sentido. Por lo tanto, hemos obtenido evidencia de que  el modelo lineal puede ser válido. 

La información proporcionada por `summary(lm( ))` se puede extraer individualmente ya que se trata de una `list`. Veamos algunos ejemplos.

* El coeficiente de determinación se obtiene con el sufijo `$r.squared`:
```{r}
summary(lm(peso~pesonotif,data=datospeso))$r.squared 
```

* El coeficiente de determinación ajustado se obtiene con el sufijo `$adj.r.squared`:
```{r}
summary(lm(peso~pesonotif,data=datospeso))$adj.r.squared 
```

* La tabla de coeficientes se obtiene con el sufijo `$coefficients`:
```{r}
summary(lm(peso~pesonotif,data=datospeso))$coefficients 
```

* Las estimaciones de los coeficientes forman la primera columna de la tabla anterior, y por lo tanto se obtienen con el sufijo `$coefficients[,1]`:
```{r}
summary(lm(peso~pesonotif,data=datospeso))$coefficients[,1] 
```

* Los residuos se obtienen con el sufijo `$residuals`:
```{r}
residuos=summary(lm(peso~pesonotif,data=datospeso))$residuals
head(residuos)
```

* Los residuos también se obtienen a partir del resultado de `lm` con el mismo sufijo `$residuals`:
```{r}
recta_regresion=lm(peso~pesonotif,data=datospeso)
Residuos=recta_regresion$residuals
head(Residuos)
```


* Los valores $\widehat{y}_i$ de la variable dependiente predichos por el modelo sobre los sujetos de la muestra se obtienen añadiendo al resultado de `lm` el sufijo `$fitted.values`:
```{r}
estimados=recta_regresion$fitted.values
head(estimados)
```

Recordemos que en este caso, el valor del coeficiente de determinación $R^2$ no ha sido muy alto. Sin embargo, hemos observado que visualmente la gran mayoría de observaciones se ajustan a la recta de regresión salvo la observación anómala 12. A continuación, calcularemos la recta de regresión sin tener en cuenta esta observación mediante el uso del parámetro `subset` de la función `lm`.
```{r}
summary(lm(peso~pesonotif,data=datospeso,subset=-12))
``` 
```{r, fig.cap='Gráfico de los pares de observaciones (pesonotif,peso) junto con las  rectas de regresión teniendo en cuenta el valor anómalo (roja continua) y sin tenerlo en cuenta (azul discontinua).'}
plot(datospeso$pesonotif, datospeso$peso, pch=20, xlab="peso notificado", ylab="peso medido")
abline(lm(peso~pesonotif,data=datospeso), col="red")
abline(lm(peso~pesonotif,data=datospeso,subset=-12), col="blue",lty=2)
```



Excluyendo la observación 12 del cálculo de la recta de regresión hemos obtenido un coeficiente de determinación mucho mayor que el inicial.  Por otro lado, en el gráfico vemos como la observación anómala "atrae" la recta de regresión disminuyendo su pendiente. 

Hemos utilizado la función `lm` para calcular la nueva recta de regresión. Otra posibilidad hubiera sido usar la función `update`, que permite recalcular la recta de regresión a partir de una recta de regresión anterior. Su sintaxis es la siguiente:
```{r,eval=FALSE}
update(x, formula., subset=...)
```

donde

* `x` es un modelo de regresión lineal, es decir, la salida de una función `lm`.

* `formula.` es un parámetro opcional que indica un cambio en la `formula` especificada para obtener el nuevo modelo: si no cambiamos la fórmula, no hay que añadirlo. Es muy útil en regresión lineal múltiple para eliminar una de las variables consideradas. En este caso entraríamos como `formula.` la expresión `.~.-X`, donce `X` es la variable que queremos eliminar. Esta construcción indica que se ha de utilizar la misma `formula` que  en el modelo `x` pero ahora sin tener en cuenta la variable independiente `X`.

* `subset` también es un parámetro opcional y tiene el mismo significado que en la función `lm`.  

Naturalmente, se tiene que especificar al menos uno de estos dos parámetros opcionales, o del contrario no estaremos modificando el modelo lineal `x`. 

La salida de la función `update` es similar a la de `lm`. Así, con la instrucción siguiente, se obtiene el coeficiente de determinación anterior:
```{r}
summary(update(recta_regresion,subset=-12))$r.squared
```

Vamos a calcular a continuación los valores de $R^2$ y $R^2_{adj}$ de los modelos considerados en el Ejemplo \@ref(exm:exempUSA).
```{r}
summary(lm(obama~grad_univ,data=USA))$r.squared
summary(lm(obama~grad_univ,data=USA))$adj.r.squared
summary(lm(obama~grad_univ+veteranos,data=USA))$r.squared
summary(lm(obama~grad_univ+veteranos,data=USA))$adj.r.squared
```

Sobre el uso de los valores $R^2$ y $R^2_{adj}$ hay varias consideraciones a realizar. En primer lugar, hay que tener en cuenta que el valor de $R^2$ se mantendrá o crecerá si añadimos nuevas variables independientes al modelo. Esto es debido a que se plantea un modelo más general que incluye el anterior y por lo tanto, siempre se explicará como mínimo el mismo porcentaje de variabilidad de $y$. Así se puede observar en este ejemplo,  en el que el modelo `obama~grad_univ+veteranos` obtiene un valor mayor de $R^2$ que `obama~grad_univ`. Teniendo en cuenta este hecho, y con el objetivo de conseguir un equilibrio entre la complejidad del modelo y el ajuste a los datos, se recomienda usar los valores de $R^2_{adj}$ para comparar modelos de regresión lineal múltiple. Este coeficiente penaliza la adición de una nueva variable a no ser que esta adición suponga una mejora sustancial del ajuste del modelo a los datos. Así, como el modelo  `obama~grad_univ` tiene un valor de $R^2_{adj}$ mayor, se considera que es un modelo mejor al que contempla también la variable `veteranos`. 

Veamos finalmente qué ocurre si consideramos todas las variables sociales, demográficas y económicas para explicar el porcentaje de voto a Obama en un estado. Para simplificar nuestra tarea, definimos el *data frame* `USA2`  que contiene solo estas variables, eliminando de la tabla `USA` original  las variables que dan el estado, la región y el número de diputados.
```{r}
USA2=USA[,-c(1,2,21)]
summary(lm(obama~.,data=USA2))$adj.r.squared
``` 
El modelo mejora de forma muy significativa. Hemos usado la fórmula `obama~.` para considerar el resto de variables de `USA2` como variables independientes.

## Intervalos de confianza en el modelo de regresión lineal

Nuestro siguiente objetivo es calcular intervalos de confianza para los coeficientes $\beta_i$ del modelo lineal así como para el valor esperado $\mu_{Y|{x_1 \ldots x_k}}$ y el valor estimado $y_0$ de $Y$ sobre un sujeto en el que $(X_1,\ldots,X_k)=(x_1,\ldots,x_k)$. 

La función de R que calcula intervalos de confianza para los coeficientes $\beta_i$ es `confint`. Su sintaxis básica es
```{r,eval=FALSE}
confint(objeto, parm, level=...)
```

donde:

* `objeto` es el resultado de una función `lm`.

* `parm` permite indicar para qué parámetros se tienen que calcular los intervalos de confianza. Tanto se puede igualar a un vector de números (empezando a contar por el término independiente) como a un vector con los nombres de las variables independientes correspondientes a dichos parámetros. Por defecto se calculan los intervalos de confianza para todos los parámetros.

*  `level` es el nivel de confianza. Por defecto, se calculan intervalos de confianza al 95%.

Veamos una aplicación de la función `confint`. Vamos a calcular los intervalos de confianza del 95% para los parámetros $\beta_0$ y $\beta_1$ del Ejemplo \@ref(exm:exempDavis). Recordad que hemos llamado `recta_regresion` al resultado correspondiente de la función `lm`.
```{r}
confint(recta_regresion)
```

La salida de la función es una matriz  cuyas filas son los extremos inferior y superior de los intervalos de confianza de los coeficientes deseados. Las filas se indican con el nombre de la variable dependiente correspondiente o con `(Intercept)` en el caso del término independiente, y las columnas se indican mediante el nivel del cuantil utilizado para el cálculo del intervalo. Así, el intervalo de confianza para $\beta_0$ es (`r round(confint(recta_regresion)[1,],3)`) y para $\beta_1$, (`r round(confint(recta_regresion)[2,],3)`). 

Si solo queremos calcular el intervalo de confianza para $\beta_1$ podemos usar cualquiera de las tres instrucciones siguientes (aunque la tercera en realidad calculará todos los intervalos de confianza y solo nos mostrará el de $\beta_1$):

```{r}
confint(recta_regresion, parm=2)
confint(recta_regresion, parm="pesonotif")
confint(recta_regresion)[2,]
```

Una de las utilidades básicas de los intervalos de confianza es poder contrastar si la variable correspondiente aporta información al modelo o no. Este hecho, además de venir determinado por el p-valor obtenido en la tabla de coeficientes en la salida de `lm`, también se puede decidir comprobando si 0 pertenece al intervalo de confianza para el coeficiente. En caso afirmativo, no se puede descartar que la $\beta_i$ correspondiente sea 0 y en consecuencia, podría ocurrir que la variable $X_i$ no influyera en el modelo. En este ejemplo concreto, 0 no pertenece al intervalo de confianza del 95% para $\beta_1$, y por lo tanto podemos concluir (con un nivel de significación del 5%) que el peso de un individuo está relacionado con el peso que afirma que tiene. Fijaos en que  0 sí que pertenece al intervalo de confianza del 95% para $\beta_0$, por lo que con este nivel de significación no podemos rechazar que $\beta_0=0$, pero esto no afecta la conclusión anterior. 

Si hacemos lo mismo para la regresión lineal del voto de Obama en función de las variables `grad_univ` y `veteranos`, obtenemos:
```{r}
recta_reg=lm(obama~grad_univ+veteranos,data=USA)
confint(recta_reg)
summary(recta_reg)
```
El intervalo de confianza para el coeficiente $\beta_2$, el correspondiente a la variable `veteranos`, contiene el 0 y por lo tanto, no se puede descartar que esta variable de hecho no influya en el modelo. Observad que el p-valor de esta variable en la tabla de coeficientes es muy grande, por lo que podemos llegar a esta conclusión a partir de este p-valor. Esto es coherente con la disminución del valor de $R^2_{adj}$ cuando añadíamos la variable `veteranos` al modelo. El ajuste mejora pero no compensa el aumento de complejidad y de hecho, no está claro que esta variable influya en la variable de respuesta. 

Llegados a este punto, vamos a introducir la función `predict` que sirve para calcular intervalos de confianza para las estimaciones de la variable dependiente.  La sintaxis de esta función es la siguiente:
```{r,eval=FALSE}
predict(objeto, newdata, interval=..., level=...)
```   
donde:

* `objeto` es una salida de la función `lm`.

* `newdata` es un *data frame* del que la función `predict` toma los valores de las variables independientes de los individuos para los que queremos predecir el valor de la variable dependiente. Por lo tanto, el *data frame* que entremos en este parámetro tiene que tener como columnas las variables independientes del modelo, y `predict` calculará un intervalo de confianza para cada una de sus filas.

* `interval` es un parámetro con tres posibles valores: 

    * `"none"`, con el que simplemente se calcula el valor de la variable dependiente que predice la función lineal de regresión para cada individuo descrito en `newdata`.
    
    * `"confidence"`, con el que se calcula el intervalo de confianza para el valor *esperado* de la variable dependiente para cada individuo descrito en `newdata`. 
    
    * `"prediction"`, con el que se calcula el intervalo de confianza para el valor  de la variable dependiente *predicho* por la función de regresión para los individuos cada individuo descrito en `newdata`. 
    

* `level`,  como siempre, indica el nivel de confianza y por defecto vale 0.95.


Volvamos a la recta de regresión encontrada en el Ejemplo \@ref(exm:exempDavis).  Vamos a calcular algunos intervalos de confianza para estimaciones de pesos a partir de valores notificados. Supongamos que tenemos dos individuos de nuestra población de interés, un dice que pesa 70 kg y el otro que pesa 100 kg. Entramos estos datos en un *data frame* de única variable `pesonotif`:
```{r}
individuos=data.frame(pesonotif=c(70,100))
```

Entonces:

* Estimamos que pesan
```{r}
round(predict(recta_regresion,individuos,interval="none"),1)
```
   
* Los intervalos de confianza del 95% para lo que esperamos que pesen son:
```{r}
round(predict(recta_regresion,individuos,interval="confidence"),1)
```

* Los intervalos de confianza del 95% para lo que predecimos que pesan son:
```{r}
round(predict(recta_regresion,individuos,interval="prediction"),1)
```

Como podéis ver, la salida de la función `predict` es una matriz cuyas filas corresponden a los individuos representados en el *data frame *que le entramos, en el mismo orden que las filas de ese *data frame* (o un vector, si el data frame tiene una sola fila) y tres columnas:

* `fit`: los valores predichos por la función de regresión.
* `lwr` y `upr`: los extremos inferior (*lower*) y superior (*upper*), respectivamente, de los intervalos de confianza que se han pedido. 

Así, por ejemplo, tenemos que si una persona dice que pesa 100 kg, entonces nuestra recta de regresión predice que pesará `r round(predict(recta_regresion,individuos,interval="prediction")[2,1],1)` kg con un intervalo de confianza para esta predicción que va de `r round(predict(recta_regresion,individuos,interval="prediction")[2,2],1)` a `r round(predict(recta_regresion,individuos,interval="prediction")[2,3],1)` kg. Además, tenemos una confianza del 95% en que, de media, los individuos que dicen que pesan 100 kg en realidad pesan entre `r round(predict(recta_regresion,individuos,interval="confidence")[2,2],1)` y `r round(predict(recta_regresion,individuos,interval="confidence")[2,3],1)` kg.

Observad que los intervalos de confianza obtenidos con `interval="prediction"` son más anchos que los obtenidos con `interval="confidence"`. Naturalmente, tenemos una mayor incertidumbre a la hora de predecir el peso de un individuo concreto que dice que pesa *X* que al predecir el peso medio de todos los individuos que dicen que pesan *X*, lo que se traduce en las anchuras de los intervalos de confianza.

Veamos una aplicación real de estos intervalos de confianza. Vamos a suponer que el porcentaje de voto demócrata en las elecciones presidenciales del 2016 iba a seguir el mismo modelo que en las elecciones del 2012 y que podemos estimar el porcentaje de voto demócrata a partir de las variables socio-económicas y demográficas de los estados. Disponemos de los valores de las variables independientes para el estado de Virginia correspondientes al año 2015 (últimos valores publicados con anterioridad a las elecciones del 2016), y vamos a calcular los intervalos de confianza para el porcentaje de voto demócrata y para su valor esperado en el estado de Virginia:

```{r}
virginia2015=virginia2015=data.frame(densidad=81.4,veteranos=9.4,mujeres=50.8,
grad_instituto=88.5,grad_univ=36.7,afro=19.7,asia=6.3,hispanos=8.9,
blancos=63.1,evangelicos=30,protestantes=16,relig_afro=12,catolicos=12,
mormones=2, jubilados=13.8,paro=4.3,salario=66155)
predict(lm(obama~.,data=USA2),virginia2015,interval="prediction")
predict(lm(obama~.,data=USA2),virginia2015,interval="confidence")
``` 

```{r,include=FALSE}
Virg1=predict(lm(obama~.,data=USA2),virginia2015,interval="prediction")
Virg2=predict(lm(obama~.,data=USA2),virginia2015,interval="confidence")
```

El modelo predice que en 2016 el partido demócrata iba a obtener  un `r round(Virg1[1],2)`% de los votos, con un intervalo de confianza del 95% del `r round(Virg1[2],2)`% al `r round(Virg1[3],2)`%. Además, un intervalo de confianza del 95% para el valor *esperado* de este porcentaje de votos (es decir, la media de los porcentajes de votos que obendría el partido demócrata en estados con los mismos datos socio-económicos y demográficos que Virginia en 2015 si repitiésemos muchísimas veces las elecciones) va del `r round(Virg2[2],2)`% al `r round(Virg2[3],2)`%. El porcentaje de voto demócrata en Virginia en las elecciones del 2016 fue del 49.73%, así que la predicción del modelo en este caso es bastante acertada.

## Selección del modelo en base al ajuste de los datos {#sec:seleccion}


En los modelos de regresión lineal múltiple, es decir, en aquellas situaciones en las que existen varias variables independientes candidatas a intervenir en el modelo de regresión lineal, surge la pregunta de qué modelo se debe seleccionar en base al ajuste de los datos. No existe una respuesta definitiva a esta cuestión debido a la complejidad del tema. Aunque un modelo sea efectivo a la hora de ajustar los datos y por lo tanto, tenga un valor grande de  $R^2$, puede ocurrir que algunas de las variables independientes seleccionadas no sean en realidad relevantes en el modelo. Son las conocidas como **variables redundantes**. Estas variables dificultan la interpretación del modelo y es conveniente eliminarlas. Sin embargo, surge la duda de determinar de forma objetiva cuál es el mejor modelo, en el sentido de cuál es el modelo más sencillo que explique la mayor cantidad de varianza posible. Así, llegamos a la cuestión de cómo comparar dos modelos de regresión múltiple con un número distinto de variables. Tenemos disponibles tres métodos para este fin:

* Comparación de los $R^2_{adj}$: Ya hemos hablado de este método en la Sección \@ref(sec:1). Recordemos que en base a este criterio, el modelo con un mayor $R^2_{adj}$ es el óptimo.

* Las medidas **AIC** (*Akaike's Information Criterion*) y **BIC** (*Bayesian Information Criterion*): La medida AIC, definida como
$$
AIC=n\ln(SS_e/n)+2k,
$$
donde $SS_e$ indica la suma de los cuadrados de los errores, cuantifica cuánta información de la variable dependiente se pierde con el modelo y el número de variables utilizado. En cambio, la medida BIC, definida como
$$
BIC=n\ln(SS_e/n)+k\ln(n),
$$
también tiene en cuenta el tamaño de la muestra al penalizar el número de variables independientes. Cuánto menor sea el valor de AIC o BIC, mejor se considera el modelo.

Las medidas AIC y BIC se calculan con la función de R
```{r,eval=FALSE}
extractAIC(x,k)[2]
```
donde `x` es un modelo lineal obtenido mediante la función `lm` y `k` es el peso que afecta al número de variables del modelo. El valor de *k* por defecto es 2 y corresponde al criterio AIC. Si se indica `k=log(n)`, donde *n* es el número de observaciones, `extractAIC` calcula la medida BIC. 

Los tres criterios no son equivalentes y por lo tanto, pueden darse resultados contradictorios entre ellos. En ese caso, conviene escoger uno de los criterios y explicar la preferencia por él, o realizar más análisis para determinar qué modelo es mejor.

Estudiemos a continuación los siguientes modelos de regresión lineal múltiple a partir de los datos del Ejemplo \@ref(exm:exempUSA):
```{r}
modelo1=lm(obama~.,data=USA2)
modelo2=update(modelo1,.~.-jubilados)
modelo3=update(modelo1,.~.-paro)
```

El modelo 1 considera todas las variables socio-económicas y demográficas de los estados norteamericanos, mientras que los modelos 2 y 3 no tienen en cuenta, respectivamente, las variables `jubilados` y `paro`. Veamos los valores de $R^2_{adj}$, AIC y BIC para decidir cuál de estos tres modelos es el mejor. Organizaremos estos valores en un *data frame* para facilitar su comparación y usaremos la función `kable` del paquete **knitr** para que nos muestre el data frame en el documento final como una tabla bien formateada.

```{r}
R2.adj=c(summary(modelo1)$adj.r.squared,
summary(modelo2)$adj.r.squared,
summary(modelo3)$adj.r.squared)
AIC=c(extractAIC(modelo1)[2],
extractAIC(modelo2)[2],
extractAIC(modelo3)[2])
BIC=c(extractAIC(modelo1,k=log(dim(USA2)[1]))[2],
extractAIC(modelo2,k=log(dim(USA2)[1]))[2],
extractAIC(modelo3,k=log(dim(USA2)[1]))[2])
Medidas=data.frame(R2.adj,AIC,BIC,row.names=c("modelo1","modelo2","modelo3"))
names(Medidas)=c("Coef. Det. ajustado","AIC","BIC")
knitr::kable(Medidas)
```

En este caso, los tres indicadores coinciden en que el modelo que no tiene en cuenta la variable `jubilados` es el mejor de los tres: es el que tiene mayor valor de $R^2_{adj}$ y menor valor de las medidas AIC y BIC.

En un problema en el que se dispongan de *k* variables independientes, existen $2^{k}-1$ modelos de regresión lineal múltiple que deberían ser evaluados para encontrar cuál es la mejor combinación de variables. El proceso manual es tedioso y el número de casos es demasiado elevado. Para ayudarnos en esta tarea, en R disponemos de la función
```{r,eval=FALSE}
step(x,direction=...,scope=...,k=..., trace=...)
```     
que, a partir de un modelo lineal dado `x`, va probando diferentes modelos añadiendo o eliminando variables independientes hasta encontrar un modelo óptimo (entre todos los modelos que se puedan obtener de esta manera). Sus otros argumentos son:

* `direction` indica la metodología que ha de utilizar R para generar los nuevos modelos a evaluar en la siguiente iteración. Tiene 3 valores posibles:

    * `"backward"`  indica que en cada iteración se han de evaluar y comparar el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él eliminando una de sus variables independientes; 
    * `"forward"`  indica que en cada iteración se han de evaluar y comparar el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él añadiéndole una nueva variable independiente; 
    * `"both"`  indica que en cada iteración se han de evaluar y comparar el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él añadiéndole una nueva variable independiente o eliminando una de sus variables independientes.
    
* `scope` define el rango de modelos que se tienen que considerar. Se introduce con el formato  `list(lower=formula1,upper=formula2)` donde `formula1` y `formula2` son los modelos que constituyen los extremos del rango a considerar en el sentido de que los modelos que se tendrán en cuenta han de contener las variables independientes de la  `formula1` y sus variables independientes han de aparecer a la derecha de la tilde en la `formula2`.

* `k` tiene el mismo significado que en la función `extractAIC` e indica si la evaluación de los modelos se realiza en base al AIC (entrando `k=2`,  el valor por defecto) o al BIC (entrando `k=log(n)` con `n` el el número de observaciones).

* `trace` es un parámetro lógico. Su valor por defecto es `TRUE` y va mostrando en la consola la información de cada iteración. Igualado a `FALSE` solo da el modelo final. 

Esta función realiza un proceso iterativo de eliminación o adición de variables consiguiendo en cada paso del algoritmo un modelo con un valor de AIC (o BIC) menor al modelo obtenido en el paso anterior. El proceso se para cuando no encuentra ningún modelo mejor.

Ejecutemos esta función con el modelo que considera todas las variables socio-económicas y demográficas, usando el AIC para valorar los modelos y con `direction="backward"`, es decir, eliminando variables una a una. Omitimos la información en consola de algunas iteraciones intermedias (indicado en la salida con `[...]`) para ahorrar espacio vertical.

```{r,eval=FALSE}
step(modelo1,direction="backward")
````

```{r,eval=FALSE}
Start:  AIC=175.91
obama ~ densidad + veteranos + mujeres + grad_instituto + grad_univ + 
    afro + asia + hispanos + blancos + evangelicos + protestantes + 
    relig_afro + catolicos + mormones + jubilados + paro + salario

                 Df Sum of Sq     RSS    AIC
- jubilados       1     0.168  792.69 173.92
- afro            1     1.209  793.73 173.99
- catolicos       1     9.637  802.16 174.53
- relig_afro      1    12.544  805.07 174.71
- veteranos       1    12.555  805.08 174.72
- blancos         1    13.715  806.24 174.79
- salario         1    14.878  807.40 174.86
- protestantes    1    24.473  817.00 175.46
- hispanos        1    25.006  817.53 175.50
<none>                         792.52 175.91
- grad_instituto  1    32.482  825.01 175.96
- evangelicos     1    43.177  835.70 176.62
- mujeres         1    57.840  850.36 177.51
- grad_univ       1    73.797  866.32 178.45
- densidad        1    84.717  877.24 179.09
- mormones        1    85.509  878.03 179.14
- asia            1   173.269  965.79 184.00
- paro            1   287.526 1080.05 189.70

Step:  AIC=173.92
obama ~ densidad + veteranos + mujeres + grad_instituto + grad_univ + 
    afro + asia + hispanos + blancos + evangelicos + protestantes + 
    relig_afro + catolicos + mormones + paro + salario

                 Df Sum of Sq     RSS    AIC
- afro            1     1.349  794.04 172.01
- catolicos       1     9.740  802.43 172.55
- blancos         1    13.567  806.26 172.79
- veteranos       1    13.870  806.56 172.81
- relig_afro      1    13.991  806.68 172.82
- salario         1    14.765  807.46 172.87
- protestantes    1    24.346  817.04 173.47
- hispanos        1    24.992  817.68 173.51
<none>                         792.69 173.92
- grad_instituto  1    33.638  826.33 174.04
- evangelicos     1    43.398  836.09 174.64
- densidad        1    84.719  877.41 177.10
- mormones        1    86.199  878.89 177.19
- grad_univ       1    96.666  889.36 177.79
- mujeres         1   114.224  906.92 178.79
- asia            1   180.199  972.89 182.37
- paro            1   300.135 1092.83 188.30

Step:  AIC=172.01
obama ~ densidad + veteranos + mujeres + grad_instituto + grad_univ + 
    asia + hispanos + blancos + evangelicos + protestantes + 
    relig_afro + catolicos + mormones + paro + salario

                 Df Sum of Sq     RSS    AIC
- catolicos       1    10.218  804.26 170.66
- salario         1    14.423  808.46 170.93
- veteranos       1    15.412  809.45 170.99
- protestantes    1    22.997  817.04 171.47
- blancos         1    23.551  817.59 171.50
- relig_afro      1    24.048  818.09 171.53
<none>                         794.04 172.01
- grad_instituto  1    34.693  828.73 172.19
- hispanos        1    39.340  833.38 172.48
- evangelicos     1    42.987  837.03 172.70
- mormones        1    87.616  881.66 175.35
- densidad        1    92.800  886.84 175.65
- grad_univ       1    95.935  889.98 175.83
- mujeres         1   113.018  907.06 176.80
- asia            1   239.541 1033.58 183.46
- paro            1   309.656 1103.70 186.80

[...]                 

Step:  AIC=165.22
obama ~ densidad + mujeres + grad_instituto + grad_univ + asia + 
    hispanos + evangelicos + mormones + paro

                 Df Sum of Sq     RSS    AIC
<none>                         879.45 165.22
- hispanos        1     37.14  916.59 165.33
- grad_univ       1     70.03  949.47 167.13
- evangelicos     1     84.02  963.47 167.87
- densidad        1    103.52  982.97 168.90
- grad_instituto  1    119.32  998.77 169.71
- mujeres         1    285.19 1164.63 177.54
- paro            1    305.27 1184.71 178.42
- mormones        1    492.00 1371.45 185.88
- asia            1    775.31 1654.76 195.46

Call:
lm(formula = obama ~ densidad + mujeres + grad_instituto + grad_univ + 
    asia + hispanos + evangelicos + mormones + paro, data = USA2)

Coefficients:
   (Intercept)        densidad         mujeres  grad_instituto       grad_univ  
    -3.263e+02       1.555e-03       5.087e+00       1.056e+00       4.207e-01  
          asia        hispanos     evangelicos        mormones            paro  
     7.629e-01       1.379e-01      -1.596e-01      -3.967e-01       1.854e+00  

```

Al inicio del algoritmo, tenemos en `Start` el modelo inicial, entrado a la función `step`,  y su valor de AIC. A continuación, en cada paso se disponen en una tabla las variables y el valor de AIC que obtendría el modelo si se eliminara la variable en cuestión. Las variables aparecen en la tabla ordenadas en orden ascendente del AIC que se obtiene al eliminarlas, y el modelo actual se indica por medio de `<none>` (no se elimina *ninguna* variable). Así, en la primera iteración se eliminará la variable `jubilados` ya que su eliminación proporciona un modelo con un valor mínimo AIC=173.92 que es  menor que el valor del modelo inicial. Luego, al principio de cada  iteración sucesiva  se indica la formula correspondiente al modelo en ese momento. En la segunda iteración,  ya se ha eliminado la variable  `jubilados`, se calculan los AIC y como la variable `afro` da un valor mínimo de AIC, será la variable que se eliminará. Y así sucesivamente.

El algoritmo finaliza cuando si se elimina cualquiera de las variables restantes, aumenta el AIC empeorando el modelo: corresponde a la situación en la que la variable `<none>`  aparece en la primera fila de la tabla de valores AIC. En este ejemplo, se eliminan `jubilados`, `afro` y `catolicos`, `veteranos`, `protestantes`, `salario`, `religion_afro` y `blancos` resultando un modelo de regresión con nueve variables independientes y con un valor de AIC=165.22. La última parte de la salida de la función nos da las variables y los coeficientes de la función de regresión correspondiente. El vector de coeficientes de la función lineal resultante se puede obtener, usando `trace=FALSE`, con el sufijo `$coefficients` y el modelo con el sufijo `$call`.

```{r}
step(modelo1,direction="backward",trace=FALSE)$coefficients
step(modelo1,direction="backward",trace=FALSE)$call
```


Ejecutemos a continuación la función `step` pero ahora con `direction="forward"` desde un modelo sin variables independientes (que se indica con ~1) y un rango que permita llegar al modelo completo. Para ahorrar espacio vertical, vamos a omitir de la salida la información de las iteraciones que realiza R.

```{r}
modelo_vacio= lm(obama~1,data=USA2)
step(modelo_vacio,direction="forward",scope=list(lower=modelo_vacio, upper=modelo1),trace=FALSE)
```


En esta aplicación de `step` se vuelve a obtener un modelo lineal con 9 variables independientes, aunque diferente al obtenido en la aplicación anterior ya que las variables `protestantes` y `jubilados` son ahora consideradas en lugar de `grad_instituto` e `hispanos`. 

Evidentemente la función `step` no considera todos los modelos posibles. Para considerar todos los modelos posibles con un número máximo de variables independientes, se puede utilizar la función `regsubsets` del paquete **leaps**. La sintaxis de la función es la siguiente:

```{r,eval=FALSE}
regsubsets(x, nbest=..., nvmax=...)
```
donde:

* `x` es un modelo lineal obtenido mediante la función `lm`.

* `nbest` es la cantidad de modelos que queremos que nos muestre para cada número de variables independientes considerado, por defecto un modelo para cada número de variables. 

* `nvmax` es el número máximo de variables independientes que queremos que se consideren.


La función evalúa todos los modelos lineales con *k* variables aleatorias, para *k* entre 1 y `nvmax`, en base a su valor de BIC. Para cada valor de *k*, se muestran los `nbest`  modelos con un menor valor de BIC. Para una  visualización sencilla de los resultados, se puede utilizar la función `plot`.

A modo de ejemplo, vamos a determinar cuál és el mejor modelo lineal con hasta 9 variables independientes, que es la cantidad de variables que se han obtenido con las dos ejecuciones de `step`, para los datos del Ejemplo \@ref(exm:exempUSA).

```{r,eval=FALSE}
library(leaps)
plot(regsubsets(obama~.,data=USA2,nbest=1,nvmax=9))
``` 

```{r,echo=FALSE,fig.cap="Variables independientes involucradas en el mejor modelo según BIC para una cantidad fija de variables independientes entre 1 y 9. Los modelos se ordenan por filas en base al valor BIC."}
library(leaps)
plot(regsubsets(obama~.,data=USA2,nbest=1,nvmax=9))
``` 

En el gráfico resultante, se han coloreado, por filas, las variables que intervienen en los distintos modelos. Las filas están ordenadas según el valor BIC del modelo que representan. Así, el mejor modelo con una única variable independiente corresponde a la fila inferior y  es el que solo usa la variable `grad_univ`. Para *k*=9, resulta que el mejor modelo corresponde al obtenido con la función `step`  con `direction="backward"` (fila 5 desde la parte inferior). Sin embargo, el mejor modelo según esta función es el correspondiente a la fila superior y que considera las 8 variables `mujeres`, `grad_univ`, `asia`, `evangelicos`, `protestantes`, `mormones` y `paro`.  


## Diagnósticos de regresión {#sec:diagn}

Para acabar esta lección, vamos a tratar la verificación de los requisitos que dotan de significación al modelo de regresión lineal. Como sabéis, la estimación y la inferencia a partir de un modelo de regresión lineal tienen sentido solo cuando se satisfacen una serie de varias hipótesis. Estas hipótesis tienen que ser comprobadas utilizando los llamados **diagnósticos de regresión**.  Los problemas potenciales que puede sufrir un modelo de regresión lineal se clasifican en tres categorías:

* **Relativos a los residuos**: Los errores del modelo han de seguir una distribución normal con media 0 y varianza $\sigma^2$ constante (en el sentido de que no dependa del valor de las variables independientes) y ser incorrelados.

*  **Relativos al modelo**: Los puntos han de ajustarse a la estructura lineal considerada.

* **Relativos a las observaciones anómalas**: Puede que algunas de las observaciones no se ajusten al modelo, comprometiendo su validez general.


El primer paso para llevar a cabo los diagnósticos de regresión es utilizar la función `plot`  con entrada el modelo lineal generado por la función `lm`. Esta instrucción generará cuatro gráficos:

(1) **Residuos vs valores predichos** (*Residuals vs Fitted*): Puede ser utilizado para comprobar la hipótesis de linealidad del modelo. El gráfico representa los puntos $(\hat{y}_i,e_i)$, donde recordemos que  $e_i$ e $\hat{y}_i$ son, respectivamente,  el valor estimado de la variable dependiente en el sujeto *i*-ésimo de la muestra y el error cometido en esta estimación, añadiendo una regresión local (o **regresión móvil**) de estos puntos, representada por una curva de color rojo, que permite comprobar si existe algún tipo de patrón o tendencia en los mismos. Si la curva se acerca a la recta horizontal $y=0$ es indicativo de que los errores son muy pequeños y que por lo tanto se puede asumir la linealidad del modelo.

(2) **Q-Q-plot** (*Normal Q-Q*): Como ya explicamos en la Sección \@ref(sec:qqplot), este gráfico sirve para examinar la normalidad de los residuos. Recordemos que se puede aceptar la normalidad si los Q-Q-puntos están próximos a la recta cuartil-cuartil, representada en este gráfico por una línea discontinua.

(3) **Escala-Localización** (*Scale-Location*): El tercer gráfico sirve para comprobar la **homocedasticidad** del modelo, es decir, si la varianza de los errores es constante y no depende del valor de las variables independientes. Si se cumple la condición de homocedasticidad, se dice que el modelo es **homocedástico**, mientras que si no la cumple, se dice que es **heterocedástico**. En el gráfico se representan los puntos $(\hat{y}_i,\sqrt{e_i^*})$ donde $e_i^*$ son los llamados **residuos estandarizados**. No entraremos en detalles de cómo se calculan los residuos estandarizados pero, de forma simplificada, son el cociente entre $e_i$ y una estimación de su desviación típica. Este  gráfico incluye también, como en el caso del primero, la regresión móvil de los puntos y podremos aceptar que se satisface la homocedasticidad si esta curva es horizontal y los puntos se distribuyen de forma homogénea a su alrededor.

(4) **Residuos vs Apalancamiento** (*Residuals vs Leverage*): El cuarto gráfico se utiliza para identificar observaciones **influyentes**, es decir, los valores extremos que podrían influir de forma significativa en el modelo lineal cuando son incluidos o excluidos del modelo. Estas observaciones suelen ser *outliers* (valores atípicos, anómalos) y  *leverage points* (puntos de apalancamiento). Expliquemos el significado de estos términos.

    * **Outlier**: Es una observación que tiene un valor anómalo de la variable dependiente condicionado a los valores de las variables independientes, y por lo tanto un residuo muy grande.

    * **Leverage point**: Sin entrar en detalles, el *leverage* es una medida de la anomalía de los valores de las variables independientes, y un *leverage point* es un  punto con alto valor de *leverage*. Los *leverage points* son puntos que están lejos del rango mayoritario de los valores de las variables independientes. 
    
  
     El gráfico representa los puntos $(e_i^*,h_i)$, donde los $e_i^*$  son los residuos estandarizados de los que ya hemos hablado y los $h_i$ indican el grado de *leverage* de cada observación. Esto permite reconocer los *outliers* y los *leverage points* ya que los primeros se identifican como aquellas observaciones tales que $|e_i^*|>3$, mientras que los segundos se definen como aquellos puntos para los que  $h_i>2(k+1)/n$ donde *k* es el número de variables independientes y *n* el de  observaciones. 
     
     Para identificar las observaciones influyentes también se utiliza la conocida como **distancia de Cook**. No vamos a definirla, pero existe el consenso de que si la distancia de Cook de un punto es mayor que 1, la observación correspondiente es influyente, mientras que si está entre 0.5 y 1 podría serlo y debe ser investigada . En este gráfico también se incluyen unas curvas discontinuas que marcan las regiones de los puntos con distancia de Cook entre 0 y 0.5, entre 0.5 y 1 y mayores que 1.


En todos los gráficos anteriores, se identifican por defecto las tres observaciones más extremas según el criterio que evalúa cada gráfico. Estas observaciones podrían ser problemáticas y deberían ser evaluadas de forma individual para comprobar si la observación tiene algún tipo de característica diferencial o si simplemente es un error de entrada de datos.


Realicemos el diagnóstico de regresión de los Ejemplos \@ref(exm:exempDavis) y \@ref(exm:exempUSA). Empecemos por el primero. Vamos a tomar los gráficos que produce por defecto R. Si queréis modificarlos, por ejemplo traduciendo los textos a otro idioma, consultad la Ayuda de la función `plot.lm`.


```{r, fig.width=10, fig.cap="Gráficos del diagnóstico de regresión del modelo lineal del Ejemplo \\@ref(exm:exempDavis)."}
par(mfrow=c(2,2))
plot(lm(peso~pesonotif,data=datospeso))
```

Como ya se podía intuir, la observación 12, que era muy optimista con respecto al peso real (o un olvido de la cifra de las centenas), distorsiona claramente el análisis destacando en cada gráfico y es la única observación que podría ser influyente según el cuarto gráfico. Veamos qué ocurre si eliminamos esta observación que claramente constituye un caso muy especial que no responde a la lógica.


```{r, fig.width=10, fig.cap="Gráficos del diagnóstico de regresión del modelo lineal del Ejemplo \\@ref(exm:exempDavis)  sin la observación 12."}
par(mfrow=c(2,2))
plot(lm(peso~pesonotif,subset=-12,data=datospeso))
```
Para este modelo los gráficos son mucho más interpretables. El primer gráfico muestra que la linealidad del modelo es razonable, aunque es cierto que existen pequeñas desviaciones respecto a una línea horizontal. El segundo gráfico claramente demuestra que los residuos no siguen una distribución normal. Comprobemos esta hipótesis aplicando el test de Shapiro-Wilk a los residuos del modelo:
```{r}
shapiro.test(lm(peso~pesonotif,subset=-12,data=datospeso)$residuals)
```
Podemos rechazar con un nivel de significación del 5% que los residuos siguen una distribución normal. Por otra parte, el tercer gráfico nos indica que el modelo presenta heterocedasticidad al haber regiones considerables sin puntos y con una curva roja claramente no horizontal. Finalmente, no hay ninguna observación influyente en base al cuarto gráfico. En conclusión, parece ser que fallan dos hipótesis imprescindibles para realizar cualquier inferencia o estimación con este modelo lineal: la normalidad y la homocedasticidad de los residuos.



A continuación, analicemos los gráficos generados en el diagnóstico de regresión del modelo obtenido por la función `regsubsets` en la Sección \@ref(sec:seleccion) para los datos del Ejemplo \@ref(exm:exempUSA). 

```{r, fig.width=10, fig.cap="Gráficos del diagnóstico de regresión del modelo lineal obtenido con la función `regsubsets` a partir de los datos del Ejemplo \\@ref(exm:exempUSA)."}
par(mfrow=c(2,2))
modelo_obama=lm(obama~mujeres+grad_univ+asia+evangelicos+protestantes+mormones+paro,
                data=USA2)
plot(modelo_obama)
```
El primer gráfico no aporta pruebas muy evidentes de que el modelo no es lineal. Cuando puedan surgir dudas, para los modelos de regresión lineal múltiple, se puede utilizar la función `crPlots` del paquete **car**. Esta función, que se aplica a un modelo lineal, representa los gráficos de residuos parciales útiles para detectar la no linealidad en un modelo de regresión. Se definen los residuos parciales $e_{ij}$ para una variable independiente $x_j$ como 
$$
e_{ij}=e_i+b_jx_{ij}.
$$ 
Los residuos parciales se dibujan contra los valores de $x_j$ y se calcula su recta de regresión lineal simple, representada en azul en el gráfico. Además, se realiza una recta de regresión no paramétrica suave (las variables independientes no están predeterminadas y se construyen con los datos), representada  en color morado. Si estas curvas divergen considerablemente en alguna de las variables independientes, no se cumple la linealidad  del modelo. En nuestro caso coinciden en general y podemos suponer que el modelo sí  es lineal:

```{r,include=FALSE}
par(mfrow=c(1,1))
```


```{r,fig.width=10, fig.cap="Gráficos de residuos parciales del modelo lineal obtenido con la función `regsubsets` a partir de los datos del Ejemplo \\@ref(exm:exempUSA)."}
crPlots(modelo_obama)
```  
El Q-Q-plot parece indicar normalidad de los residuos. Lo comprobamos con el test de Shapiro-Wilk:
```{r}
shapiro.test(modelo_obama$residuals)
```
Sin embargo, parece que nuevamente el modelo presenta heterocedasticidad. También se aprecia la existencia de una observación influyente, la observación 12. Esta observación corresponde al estado de Hawaii, que además de ser el único estado no continental de Estados Unidos, tiene un porcentaje de población de origen asiático mucho más elevado  que el resto de estados.



## Guía rápida



* `lm(fórmula)` realiza la regresión lineal de la variable dependiente en la parte izquierda de la `fórmula` respecto de las variables independientes indicadas a la derecha de la misma. El parámetro opcional `data` sirve para indicar el *data frame* del que se extraen las variables de la `fórmula` y el parámetro opcional `subset` sirve para indicar el subconjunto de filas del *data frame* que se ha de usar como muestra. Su salida es una `list` y sus dos componentes más interesantes son:

    * `coefficients`: los coeficientes de la función de regresión lineal.
    * `residuals`: los errores o residuos.
    * `fitted.values`: los valores estimados de la variable dependiente en los puntos de la muestra.


* `summary(lm(...))` muestra la información calculada con la función `lm`. Las componentes más interesantes de su salida son las siguientes:

    * `r.squared`: el coeficiente de determinación $R^2$.
    * `adj.r.squared`: el coeficiente de determinación ajustado $R^2_{adj}$.
    * `coefficients[,1]`: los coeficientes de la función de regresión lineal.
    * `coefficients[,4]`: los p-valores de los contrastes bilaterales de nulidad para dichos coeficientes.
    * `sigma`: la estimación de la desviación típica común de los residuos.
    * `p.value`: el p-valor del ANOVA asociado a la regresión lineal realizada.
    
* `update`  permite recalcular una función de regresión lineal modificando la fórmula que describe su modelo (con el parámetro `formula.`) o reduciendo la muestra usada (con el parámetro `subset`).

* `confint` calcula los intervalos de confianza de los coeficientes de una función de regresión lineal. Se aplica a una salida de la función `lm` y admite además los parámetros `parm`, para especificar de qué coeficientes se piden los intervalos de confianza (por defecto, de todos) y `level` para indicar el nivel de confianza (por defecto, del 95%).

* `predict` calcula intervalos de confianza de predicciones usando una función de regresión lineal. Se aplica a una salida de la función `lm` y a los parámetros siguientes:

    * `newdata`: un *data frame* cuyas filas dan los valores de las variables independientes de los puntos sobre los que queremos calcular los intervalos de confianza.

    * `ìnterval`: indica de qué queremos calcular los intervalos de confianza. Admite 3 posibles valores: `none`, con el que no calcula intervalos de confianza, solo predicciones de valores;    `confidence`, con el que calcula intervalos de confianza de valores esperados; y    `prediction`, con el que  calcula intervalos de confianza de valores predichos.

    * `level`, como siempre, indica el nivel de confianza y su valor por defecto es 95%.

* `extractAIC(...)[2]` calcula la medida AIC (por defecto) o BIC (entrando el parámetro `k=log(n)`) de un modelo de regresión lineal.

* `step` prueba de manera iterativa una serie de modelos de regresión lineal obtenidos añadiendo o quitando variables independientes al que se le entra y da el mejor modelo de los que considera (por defecto, el de menor valor de AIC). Sus parámetros principales son:

    * `direction`: indica la metodología que ha de utilizar R para generar los nuevos modelos a evaluar en la siguiente iteración. Sus valores posibles son: `"backward"`, que  indica que en cada iteración se evalúan y comparan el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él  eliminando una de sus variables independientes; `"forward"`, que  indica que en cada iteración se evalúan y comparan el modelo obtenido en la iteración anterior y todos los modelos obtenidos a partir de él añadiendole una nueva variable independiente; y  `"both"`, que  indica que en cada iteración se prueban la unión de todos lo modelos que usarían las dos opciones anteriores.
    
    * `scope`: define el rango de modelos que se van a considerar. 

    * `k`: tiene el mismo significado que en la función `extractAIC`.

    * `trace`: igualado a `FALSE`, no da en consola información de las iteraciones, solo  el modelo obtenido al final. 


* `regsubsets` del paquete **leaps** prueba de manera sistemática todos los modelos de regresión lineal con un número máximo de variables independientes y da para cada uno número de variables considerado los mejores modelos según el criterio BIC. Se aplica a una salida de la función `lm` y a los dos parámetros siguientes:

    * `nvmax`: indica el número máximo de variables independientes que queremos que se consideren.

    * `nbest`: indica la cantidad de modelos que ha de mostrar para cada número de variables independientes considerado, por defecto 1.
    
* `plot(regsubsets(...))` del paquete **leaps** representa gráficamente de manera adecuada el resultado de una aplicación  de la función `regsubsets`. 

* `plot(lm(...))` produce los gráficos *Residuals vs Fitted*, *Normal Q-Q*, *Scale-Location* y *Residuals vs Leverage* que permiten realizar el diagnóstico del modelo lineal calculado con la función `lm`. 

* `crPlots(lm(...))` genera el gráfico de residuos parciales del modelo lineal calculado con la función `lm`.

* `kable` del paquete **knitr** produce tablas y *data frames* bien formateados al compilar el fichero *R Markdown*.

* `na.omit` aplicado a un *data frame* elimina las filas que contienen algún valor NA.


## Ejercicios


### Test {-}


*(1)* El *data frame* `BostonHousing` del paquete **mlBench** contiene información de propiedades inmobiliarias de la ciudad de Boston en el año 1970. Cada propiedad está caracterizada por 13 variables que indican las características de la misma, así como por la variable `medv` que representa una estimación del valor de la propiedad en miles de dólares. Realizad una regresión lineal múltiple de la variable `medv` en función del resto de variables excepto las variables `chas` y `rad`. Tenéis que dar el coeficiente de la variable `crim` en la ecuación lineal encontrada redondeado a 4 cifras decimales  sin ceros innecesarios a la derecha  y decir si podéis rechazar (con un SI) o no (con un NO), con un nivel de significación del 5%, que el coeficiente de esta variable sea 0. Dad el coeficiente y la conclusión en este orden, separados por un único espacio en blanco. 

*(2)* El *data frame* `BostonHousing` del paquete **mlBench** contiene información de propiedades inmobiliarias de la ciudad de Boston en el año 1970. Cada propiedad está caracterizada por 13 variables que indican las características de la misma, así como por la variable `medv` que representa una estimación del valor de la propiedad en miles de dólares. Sin tener en cuenta las variables `chas` y `rad`, encontrad el mejor modelo lineal con exactamente 3 variables independientes en base al valor de la medida BIC y calculad el valor de $R^2_{adj}$ del mismo. Además, tenéis que decir si la variable `ptratio` se usa (con un SI) o no (con un NO) en el modelo. Dad el valor de $R^2_{adj}$ redondeado a 4 cifras decimales  sin ceros innecesarios a la derecha y la respuesta a la pregunta planteada en este orden, separados por un único espacio en blanco.

*(3)* El *data frame* `abalone`, disponible en el paquete **AppliedPredictiveModeling**, contiene información de 4177 abulones (también conocidos como orejas de mar). Dispone de la variable `Type` de tipo factor que diferencia entre especímenes machos (M), hembras (F) y jóvenes (I) y el resto son variables numéricas. Teniendo en cuenta sólo los especímenes hembra, encontrad el modelo lineal que explica la variable `Rings` en función del resto de variables numéricas y determinad si los residuos son normales (con un SI) o no (con un NO) con un nivel de significación del 5% utilizando el test de Shapiro-Wilk. Determinad también las observaciones que pueden influyentes en el modelo, entendidas como aquéllas con distancia de Cook mayor que 0.5. Dad el p-valor del contraste de normalidad redondeado a 4 cifras decimales sin ceros innecesarios a la derecha, la conclusión del contraste y las observaciones influyentes en orden creciente, separados por un único espacio en blanco. En el caso que no haya observaciones influyentes, contestad NO.

*(4)* El *data frame* `abalone`, disponible en el paquete **AppliedPredictiveModeling**, contiene información de 4177 abulones (también conocidos como orejas de mar). Dispone de la variable `Type` de tipo factor que diferencia entre especímenes machos (M), hembras (F) y jóvenes (I). Encontrad el mejor modelo lineal que explica la variable `Rings` mediante la función `step` partiendo del modelo completo con todas las variables numéricas con parámetro `direction="backward"`. A partir de este modelo, dad en este orden los intervalos de confianza para el valor estimado y para el valor esperado de `Rings` de una oreja de mar con las siguientes características: `LongestShell`=0.44, `Diameter`=0.45, `Height`=0.15, `WholeWeight`=0.8, `ShuckedWeight`=0.36, `VisceraWeight`=0.2, `ShellWeight`=0.4. Dad en primer lugar el extremo izquierdo y a continuación el extremo derecho del intervalo para el valor predicho y a continuación, de forma análoga para el valor esperado, todos los valores sucesivos separados por un único espacio en blanco y redondeados a tres decimales  sin ceros innecesarios a la derecha.


### Respuestas al test {-}


```{r, include=FALSE}
library(mlbench)
data(BostonHousing)
str(BostonHousing)
boston=BostonHousing[,-c(4,9)]
summary(lm(medv~.,data=boston))
round(summary(lm(medv~.,data=boston))$coefficients["crim",],4)
R1.1=round(summary(lm(medv~.,data=boston))$coefficients["crim",1],4)
#
plot(regsubsets(medv~.,data=boston,nbest=1,nvmax=3))
R2.1=round(summary(lm(medv~rm+ptratio+lstat,data=boston))$adj.r.squared,4)
#
library(AppliedPredictiveModeling)
data(abalone)
str(abalone)
abaloneF=abalone[abalone$Type=="F",-1]
modelo=lm(Rings~.,data=abaloneF)
R3.1=round(shapiro.test(modelo$residuals)$p.value,4)
plot(modelo)
#
abalone2=abalone[,-1]
modelo_completo=lm(Rings~.,data=abalone2)
step(modelo_completo,direction="backward",trace=FALSE)
modelo_step=update(modelo_completo,.~.-LongestShell)
datos=data.frame(LongestShell=0.44,Diameter=0.45,Height=0.15,WholeWeight=0.8,
ShuckedWeight=0.36,VisceraWeight=0.2,ShellWeight=0.4)
R4.1 =round(predict(modelo_step,datos,interval="prediction")[2],3)
R4.2 =round(predict(modelo_step,datos,interval="prediction")[3],3)
R4.3 =round(predict(modelo_step,datos,interval="confidence")[2],3)
R4.4 =round(predict(modelo_step,datos,interval="confidence")[3],3)
```

*(1)* `r R1.1` SI

Nosotros lo hemos resuelto mediante

```{r}
library(mlbench)
data(BostonHousing)  #Cargamos la tabla de datos
str(BostonHousing)   #Consultamos la tabla de datos
boston=BostonHousing[,-c(4,9)] #Eliminamos las variables chas y rad 
summary(lm(medv~.,data=boston))
```

*(2)* `r R2.1` SI 

Nosotros lo hemos resuelto mediante

```{r}
plot(regsubsets(medv~.,data=boston,nbest=1,nvmax=3)) 
```
La fila con tres variables independientes es la superior y vemos que involucra las variables `rm`, `ptratio` y `lstat`. Ahora realizamos la regresión lineal correspondiente y calculamos el valor de $R^2_{adj}$: 

```{r}
round(summary(lm(medv~rm+ptratio+lstat,data=boston))$adj.r.squared,4)
```


*(3)* `r R3.1` NO 2052

Nosotros lo hemos resuelto mediante

```{r,fig.width=10}
library(AppliedPredictiveModeling)
data(abalone) #Cargamos la tabla de datos
str(abalone) #Consultamos la tabla de datos
abaloneF=abalone[abalone$Type=="F",-1] # Nos quedamos con las hembras y eliminamos el factor Type
modelo=lm(Rings~.,data=abaloneF) #Calculamos la regresión lineal
round(shapiro.test(modelo$residuals)$p.value,4)
par(mfrow=c(2,2))
plot(modelo)
par(mfrow=c(1,1))
```
Vemos la única observación influyente es la 2052. 


*(4)* `r R4.1` `r R4.2` `r R4.3` `r R4.4`

Nosotros lo hemos resuelto mediante

```{r}
abalone2=abalone[,-1] #Eliminamos el factor Type
modelo_completo=lm(Rings~.,data=abalone2) #Calculamos la regresión lineal
step(modelo_completo,direction="backward",trace=FALSE) #Realizamos el step pedido
modelo_step=update(modelo_completo,.~.-LongestShell) #Modificamos la regresión lineal
datos=data.frame(LongestShell=0.44,Diameter=0.45,Height=0.15,WholeWeight=0.8,
ShuckedWeight=0.36,VisceraWeight=0.2,ShellWeight=0.4) #Nuevo individuo
round(predict(modelo_step,datos,interval="prediction"),3) #Intervalo de confianza del valor predicho
round(predict(modelo_step,datos,interval="confidence"),3) #Intervalo de confianza del valor esperado
```


<!--chapter:end:10-Regresion.Rmd-->

# *Clustering* básico {#chap:clustering}


En esta lección explicamos cómo usar R para clasificar objetos (o individuos, observaciones, etc.). En la práctica, supondremos que estos objetos están representados por medio de las filas de una tabla de datos de variables cuantitativas. A partir de estas descripciones como vectores numéricos, calcularemos de alguna manera la diferencia o semejanza entre cada par de objetos  y las usaremos para clasificarlos, de manera  que cada clase agrupe  objetos parecidos. De esta manera, reducimos el problema de describir muchos objetos al de describir unas pocas clases. En este contexto, a las clases resultantes se las llama *clusters* y a la clasificación, *clustering*.

Veremos dos tipos de procedimientos de *clustering*: el **método de k-medias**, o  *k-means*, donde hay que especificar *a priori* el número de *clusters* que queremos formar, y los **métodos jerárquicos aglomerativos**,  que producen un árbol que indica el orden en el que se van agrupando los objetos de manera jerárquica, empezando por los más cercanos. 

## Método de k-medias o *k-means*

El método de k-medias produce una partición de una serie de objetos, representados mediante puntos $x_1,\ldots,x_n$ de un espacio $\mathbb{R}^p$,  en un número prefijado *k* de *clusters*. En su versión básica, que es de la que toma su nombre genérico, se usa la distancia euclídea para comparar estos puntos y se identifica cada *cluster* con su **centro**: el punto medio (*mean*) de sus elementos. El objetivo es conseguir una clasificación **óptima** en el sentido siguiente. Dado un *clustering* de un conjunto de puntos, llamaremos  $SSC$ a la suma  de los cuadrados de las distancias de los puntos a los centros de los *clusters* a los que han sido asignados. En símbolos, si los *clusters* resultantes son $C_1,\ldots,C_k$, de centros $c_1,\ldots,c_k$ respectivamente, y, para cada $i=1,\ldots,k$, el *cluster* $C_i$ está formado por los puntos $C_i=\{x_{i,1},\ldots,x_{i,n_i}\}$, entonces
$$
SSC=\sum_{i=1}^k\sum_{l=1}^{n_i} \|x_{i,l}-c_i\|^2
$$
donde $\|x-y\|^2$ indica la distancia euclídea al cuadrado entre los vectores $x$ e $y$ (recordad que 
si $x=(x_1,\ldots,x_p)$ e $y=(y_1,\ldots,y_p)$, su distancia euclídea al cuadrado es $\|x-y\|^2=\sum\limits_{i=1}^p (x_i-y_i)^2$).  Entonces, el objetivo es conseguir un *clustering* con $SSC$ mínima. Ya que estamos, denotaremos por $SSC_i$ el sumando de $SSC$ correspondiente al *cluster* $C_i$; es decir,
$$
SSC_i=\sum_{l=1}^{n_i} \|x_{i,l}-c_i\|^2.
$$


No hay ningún algoritmo que resuelva este problema de manera eficiente, por lo que se han propuesto varios métodos  heurísticos que calculan rápidamente *clusterings* aproximadamente óptimos. A continuación explicamos las líneas básicas de algunos de ellos.

**Algoritmo de Lloyd**: Para empezar, se escogen *k* centros, se calculan las distancias euclídeas de cada punto a cada centro, y se asigna a cada centro el *cluster* formado por los puntos que están más cerca de él que de los otros centros. A continuación, en pasos sucesivos se itera el bucle  (1)--(3) siguiente hasta que en una iteración los *clusters* no se modifican:

(1) Se sustituye cada centro por el punto medio de los puntos que forman su  *cluster*.
      
(2) Se calculan las distancias euclídeas de cada punto a cada centro.
      
(3) Se asigna a cada centro el *cluster* formado por los puntos que están más cerca de él que de los otros centros.
    
El *clustering* resultante está formado entonces por los últimos  *clusters* construidos.      

**Algoritmo de Hartigan-Wong**: Este algoritmo empieza igual que el de Lloyd: se escogen *k* centros, se calculan las distancias euclídeas de cada punto a cada centro, y se asigna a cada centro el  *cluster* de puntos que están más cerca de él que de los otros centros. A continuación, en pasos sucesivos se itera el bucle (1)--(5) siguiente hasta que en una iteración del mismo los *clusters* no cambian:

(1) Se sustituye cada centro por el punto medio de los puntos asignados a su *cluster*.

(2) Se calculan las distancias euclídeas de cada punto a cada centro.

(3) Se asigna (temporalmente) a cada centro el  *cluster* formado por los puntos que están más cerca de él que de los otros centros.

(4) Si en esta asignación algún punto ha cambiado de *cluster*, digamos que el punto $x_i$ se ha incorporado al *cluster* $C_j$ de centro $c_j$, entonces:
           
    * Se calcula el valor $SSE_j$ que se obtiene multiplicando $SSC_j$ por 
$n_j/(n_j-1)$ (donde $n_j$ indica el número de elementos del *cluster* $C_j$).

    * Se calcula, para todo otro *cluster* $C_k$, el correspondiente valor $SSE_{i,k}$ como si $x_i$ hubiera ido a parar a $C_k$. 

    * Si algún $SSE_{i,k}$ resulta menor que $SSE_j$, $x_i$ se asigna definitivamente al *cluster* $C_k$ que da valor mínimo de $SSE_{i,k}$. 

(5) Una vez realizado el procedimiento anterior para todos los puntos que han cambiado de *cluster*, estos se asignan a sus  *clusters* definitivos y se da el bucle por completado.

Como en el algoritmo de Lloyd, el *clustering* resultante está formado por los últimos  *clusters* construidos.      

**Algoritmo de McQueen**: Es el mismo método que el de Lloyd salvo por el hecho de que no se recalculan todos los  *clusters*  y sus centros de golpe, sino elemento a elemento. Es decir, se empieza igual que en los dos algoritmos anteriores:  se escogen *k* centros, se calculan las distancias euclídeas de cada punto a cada centro, se asigna a cada centro el *cluster* de puntos que están más cerca de él que de los otros centros, y se sustituye cada centro por el punto medio de los puntos asignados a su *cluster*.  A partir de aquí, en pasos sucesivos se itera el bucle siguiente (recordemos que los puntos a clasificar son $x_1,\ldots,x_n$, y los supondremos ordenados por su fila en la tabla de datos):

* Para cada $i=1,\ldots,n$, se mira si el punto $x_i$ está más cerca del centro de otro *cluster* que del centro del *cluster* al que está asignado. 

* Si no lo está, se mantiene en su *cluster* y se pasa al punto siguiente,  $x_{i+1}$. Si se llega al final de la lista de puntos y todos se mantienen en sus  *clusters*, el algoritmo se para.
 
* Si  $x_i$ está más cerca de otro centro, se traslada al *cluster* definido por este centro, se recalculan los centros de los dos *clusters* afectados (el que ha abandonado $x_i$ y aquél al que se ha incorporado), y se reinicia el bucle, empezando de nuevo con $x_1$.

El *clustering* resultante está formado por los  *clusters* existentes en el momento de parar.      

Ninguno de estos algoritmos garantiza un *clustering* óptimo, en el que la $SSC$ resultante sea mínima. Lo que se suele hacer entonces es repetir varias veces el algoritmo con distintos conjuntos iniciales de *k* centros, y cruzar los dedos para que la $SSC$ más pequeña que se obtenga en alguna de estas repeticiones sea efectivamente mínima. En todo caso, se ha demostrado que el algoritmo de Hartigan-Wong es, en general, más rápido y también más eficiente, en el sentido de que con mayor probabilidad da un *clustering* óptimo.^[Véase, por ejemplo, N. Slonim, E. Aharoni, K. Crammer, "Hartigan's K-Means Versus Lloyd's K-Means---Is It Time for a Change?". *Proceedings of the XXIII International Joint Conference on Artificial Intelligence* (2013), pp. 1677-1684. Accesible (mayo 2019) en http://www.ijcai.org/Proceedings/13/Papers/249.pdf.]

El método de k-medias está implementado en  R en la función `kmeans`. Su sintaxis básica es la siguiente:

```{r,eval=FALSE}
kmeans(x, centers=..., iter.max=..., algorithm =...)
```
donde:

* `x` es la matriz o el *data frame* cuyas filas representan los objetos; en ambos casos, todas las variables han de ser numéricas.

* `centers` sirve para especificar los centros iniciales, y se puede usar de dos maneras: igualado a un número *k*, R escoge aleatoriamente los *k* centros iniciales, mientras que igualado a una matriz de *k* filas y el mismo número de columnas que `x`, R toma las filas de esta matriz como centros de partida.

* `iter.max` permite especificar el número máximo de iteraciones a realizar; su valor por defecto es 10. Al llegar a este número máximo de iteraciones, si el algoritmo aún no ha acabado porque los *clusters* aún no hayan estabilizado, se para y da como resultado los *clusters* que se han obtenido en la última iteración.

* `algorithm` indica el algoritmo a usar. Puede tomar como valor cualquiera de los que hemos explicado, y se ha de entrar entrecomillado. El método por defecto, que usa si no especificamos ninguno, es el de Hartigan-Wong.


Otros parámetros se pueden consultar en la Ayuda de la función.

Para ilustrar el funcionamiento de esta función, usaremos la tabla de datos `saving` del paquete **faraway**, que nos da 5 indicadores económicos de 50 países en el período 1960--1970. Dichos indicadores son:

* `sr`:  la tasa de ahorro de cada país.
* `pop15`:  su porcentaje de población menor de 15 años.
* `pop75`:  su porcentaje de población  mayor de 75 años.
* `dpi`:  su renta per cápita  en dólares.
* `ddpi`:  su tasa  de crecimiento, como porcentaje de su renta per cápita.


Echemos un vistazo a la tabla de datos:
```{r}
library(faraway)
str(savings) 
head(savings)
```

En este ejemplo, para poder representar gráficamente los resultados obtenidos, sólo usaremos los indicadores *tasa de ahorro* (`sr`)  y *renta per cápita* (`dpi`) de los países. 

```{r}
savings2=savings[,c(1,4)]
```


Vamos a clasificar los 50 países en 4 *clusters*. Este número lo hemos elegido por ahora de manera arbitraria, en la próxima sección ya explicaremos cómo se puede hallar el número más adecuado de *clusters* en el que clasificar una determinada tabla de datos. Vamos a usar el método por defecto de R, y le dejaremos generar al azar los centros iniciales, pero fijaremos la semilla de aleatoriedad con la función `set.seed` para que el resultado sea reproducible.
```{r}
set.seed(100)
estudio.paises=kmeans(savings2, centers=4) 
estudio.paises
```

Veamos en detalle el contenido de `estudio.paises`. En primer lugar, R nos dice que los *clusters*  están formados por `r estudio.paises$size[1]`, `r estudio.paises$size[2]`, `r estudio.paises$size[3]` y `r estudio.paises$size[4]` países, respectivamente. Seguidamente nos da la matriz `Cluster means`, cuyas filas son las coordenadas de los centros de los *clusters*, en el orden correspondiente. A  continuación, el `Clustering vector` nos indica a qué *cluster* pertenece cada país: en este caso, Australia está en el *cluster* `r estudio.paises$cluster[1]`, Austria, en el `r estudio.paises$cluster[2]`, y así sucesivamente. Luego nos da las "sumas de cuadrados de los clusters", `Within cluster sum of squares by cluster`; es decir, el vector de las  $SSC_i$.


A modo de ejemplo, comprobemos que efectivamente la primera entrada del vector `Within cluster sum of squares by cluster` es $SSC_1$. Vamos a guardar en un *data frame* `savings.1` la subtabla de datos de los 
países que pertenecen al primer *cluster*, llamaremos `centro.1` al vector de coordenadas del centro del primer *cluster*, y finalmente sumaremos los cuadrados de las distancias euclídeas de las filas de `savings.1` a  `centro.1`. Para ello, vamos a usar  que el resultado de `kmeans` es una `list` que contiene, por un lado, la componente `cluster` formada por el  `Clustering vector` de las asignaciones de objetos a *clusters* (y por lo tanto, podemos especificar las filas que corresponden a países que pertenecen al primer  `cluster` con `estudio.paises$cluster==1`) y, por otro lado, la componente `centers` formada por la matriz `Cluster means` de centros (y por lo tanto las coordenadas del centro del primer  `cluster` se obtienen con `estudio.paises$centers[1,]`). 

```{r}
savings.1=savings2[estudio.paises$cluster==1, ]   #Miembros del primer cluster
centro.1=estudio.paises$centers[1,]   #Centro del primer cluster
dist.euclid2=function(x,y){sum((x-y)^2)} #Distancia euclídea al cuadrado
distancia_a_centro.1=function(x){dist.euclid2(x, centro.1)}  #Distancia de un punto al centro del primer cluster
sum(apply(savings.1, MARGIN=1, FUN=distancia_a_centro.1))  #SSC_1
```
Coincide con la primera entrada del vector `Within cluster sum of squares by cluster`. Como el resultado de `kmeans` contiene la componente `withinss` formada por el vector de las $SSC_1$, el valor de $SSC_1$ se puede obtener directamente  con
```{r}
estudio.paises$withinss[1]
```

Debajo del vector de las $SSC_i$, R nos da el porcentaje `between_SS / total_SS`, en este caso, un 91.8%. Aquí, `total_SS` representa la suma $SST$ de los cuadrados de las distancias de los puntos al centro $M$ del conjunto de todos los puntos, y `between_SS` indica la suma $SSB$ de los cuadrados de las distancias de los centros de los *clusters* a $M$, contada cada una de ellas tantas veces como elementos tiene *cluster*. Es decir, si los puntos de partida son $x_1,\ldots,x_n$ y cada *cluster* $C_i$ tiene centro $c_i$ y está formado por $n_i$ puntos, entonces
$$
SST=\sum_{i=1}^n \|x_i-M\|^2,\qquad
SSB=\sum_{i=1}^k n_i\|c_i-M\|^2.
$$
Los valores de $SST$, $SSC$ y $SSB$ son las componentes `totss`, `tos.withinss` y `betwenss`, respectivamente, del resultado de `kmeans`.

Resulta que se tiene la identidad de sumas de cuadrados
$$
SST=SSC+SSB
$$
que representa que la variabilidad total de los datos es igual a la suma de las variabilidades dentro de los *clusters* (la suma $SSC$) más la variabilidad de los centros de los *clusters* ($SSB$). Por lo tanto, el porcentaje
`between_SS / total_SS` indica la fracción de la variabilidad total que explica la variabilidad de los centros de los  *clusters*.
Como, dada una tabla de datos, el valor de $SST$ es fijo, una mayor fracción $SSB/SST$ es equivalente a una menor $SSC$, y por lo tanto a un mejor *clustering*.

Comprobemos todas estas afirmaciones en nuestro ejemplo. Vamos a empezar calculando a mano $SST$ y comprobando que coincide con `estudio.paises$totss`. Para ello, calculamos el punto medio de todo el conjunto de datos y a continuación la suma de las distancias al cuadrado de todos los puntos a este centro global. Usaremos la función `dist.euclid2` que hemos definido hace un rato para calcular las distancias euclídeas al cuadrado.




```{r}
centro.global=apply(savings2, MARGIN=2, FUN=mean)   #El punto medio global, M
centro.global
dist_a_centro=function(x){dist.euclid2(x,centro.global)}
SST=sum(apply(savings2, MARGIN=1, FUN=dist_a_centro))
SST
estudio.paises$totss  #La SST contenida en el resultado de la función kmeans
```


El valor de $SSC$ ha de ser la suma de las $SSC_i$, que ya hemos visto que forman el vector `estudio.paises$withinss`.

```{r}
SSC=sum(estudio.paises$withinss)
SSC
estudio.paises$tot.withinss  #La SSC contenida en el resultado de la función kmeans
```

Finalmente, vamos a calcular la $SSB$. Recordemos que la matriz de los centros de los *clusters* es el objeto `estudio.paises$centers` y se tiene que los tamaños de los *clusters* forman el vector `estudio.paises$size`.

```{r}
Centros=estudio.paises$centers #La matriz de centros
Distancias_centros_centroglobal=apply(Centros,MARGIN=1,FUN=dist_a_centro)  #Distancias de los centros a M
SSB=sum(Distancias_centros_centroglobal*estudio.paises$size)  
SSB
estudio.paises$betweenss  #La SSB contenida en el resultado de la función kmeans
```

Comprobemos finalmente la identidad $SST=SSC+SSB$:

```{r}
SST
SSB+SSC
```

y comprobemos que el cociente $SSB/SST$ es el 91.8% que nos ha dado R:
```{r}
SSB/SST
```


Así pues, el resultado de una aplicación de `kmeans` es una `list`. R nos da sus componentes al final del resultado de `kmeans`,  bajo `Available components:`.  A modo de resumen, vamos a recordar las componentes más interesantes para nuestros propósitos:

* `size`: vector de  tamaños de los *clusters*.

* `cluster`: vector de enteros indicando a qué *cluster* pertenece cada fila de la tabla de datos.

* `centers`: matriz de filas los vectores de coordenadas de los centros de los *clusters*.

* `totss`: la $SST$.

* `withinss`: el vector de las $SSC_i$.

* `tot.withinss`: la $SSC$, es decir, la suma del vector anterior.

* `betweenss`: la $SSB$.

Antes de continuar, tenemos de hacer un inciso. Cuando usamos un algoritmo de k-means partiendo de una configuración aleatoria de  los centros iniciales, los resultados no tienen por qué ser los mismos cada vez que se ejecuta el algoritmo, debido a que distintas configuraciones iniciales de centros pueden desembocar en  *clusterings* diferentes. Por este motivo, en el ejemplo anterior hemos usado `set.seed(100)` para que fuera reproducible. Con otra semilla de aleatoriedad, podría haber dado un resultado diferente, o no.
Por ejemplo, con `set.seed(2000)` obtenemos un *clustering* con porcentaje $SSB/SST$ más alto, y por lo tanto mejor (lo confirmamos comprobando que su $SSC$ es menor que la anterior).

```{r}
set.seed(2000)
estudio.paises2=kmeans(savings2, centers=4)
estudio.paises2
estudio.paises2$tot.withinss  #Nueva SSC
estudio.paises$tot.withinss  #Anterior SSC
```

La moraleja es que conviene ejecutar unas cuantas veces la función `kmeans` con centros iniciales aleatorios y quedarnos con el mejor *clustering* que obtengamos. En nuestro caso, en el bloque de código siguiente realizamos  10000 ejecuciones aleatorias y no  encontramos ningún *clustering* con $SSC$ inferior a `estudio.paises2$tot.withinss`, por lo que consideraremos `estudio.paises2` como una clasificación óptima de nuestros datos en 4 clases.

```{r}
fun.SSC=function(x){
set.seed(x)  
kmeans(savings2, centers=4)$tot.withinss
}
X=sample(10^8,10000)   #10000 semillas aleatorias
min(sapply(X,FUN=fun.SSC))  #Ejecutamos 10000 kmeans aleatorios y calculamos su mínimo SSC
```


Para visualizar gráficamente un *clustering* de datos bidimensionales, lo más sencillo es producir el gráfico de dispersión de los puntos coloreándolos según los *clusters*.

```{r,eval=FALSE}
plot(savings2, col=estudio.paises2$cluster, xlab="Tasa de ahorro", ylab="Renta per cápita",pch=20)
text(savings2, rownames(savings2), col=estudio.paises2$cluster, cex=0.6, pos=1) #Añadimos los nombres de los países a los puntos
```

Obtenemos la Figura \@ref(fig:paises). Observamos que los países "cercanos" están coloreados con el mismo color. Así, por ejemplo, los países más desarrollados están coloreados de color rojo y están en la parte superior del  gráfico, ya que tienen la renta per cápita más alta, mientras que los países menos desarrollados están coloreados de color verde y  están en la parte inferior del gráfico al tener la renta per cápita más baja.

```{r,echo=FALSE,results="hide", label=paises, fig.cap='Representación gráfica de las variables "Tasa de ahorro" y "Renta per cápita" agrupadas según el clustering `estudio.paises2`.'}
plot(savings2, col=estudio.paises2$cluster, xlab="Tasa de ahorro", ylab="Renta per cápita",pch=20)
text(savings2, rownames(savings2), col=estudio.paises2$cluster, 
  cex=0.6, pos=1) #Añadimos los nombres de los países a los puntos
```


Existen paquetes que aportan funciones para dibujar *clusterings* más informativos. Por ejemplo, la función `clusplot` del paquete `cluster` enmarca los *clusters* con elipses.  Su sintaxis básica es
```{r,eval=FALSE}
clusplot(x, vector, parámetros)
```
donde

* `x` es la matriz o la tabla de datos numéricos.
*  El `vector` es la componente `cluster` del resultado de `kmeans`.
* Algunos otros parámetros útiles:

     * `shade`: un parámetro lógico que igualado a `TRUE` sombrea las elipses según su densidad  (número de puntos dividido entre área de la elipse): más densamente cuanto más densas.
     
     * `color`: un parámetro lógico que igualado a `TRUE` colorea las elipses según su densidad, por defecto de azul a granate por orden creciente de intensidad.
     
     * `labels`, que puede tomar (entre otros) los valores siguientes: 0 (no añade ninguna etiqueta), 2 (etiqueta los puntos y las elipses), 3 (etiqueta solo los puntos), 4 (etiqueta solo las elipses).
    * `col.clus`: los colores de las elipses.
    * `col.p`: los colores de los puntos.
    *  `col.txt`:  los colores de las etiquetas.
    *  `lines`, permite añadir líneas uniendo los *clusters* y  puede tomar los valores siguientes: 0 (no añade ninguna línea), 1 (las líneas unen los centros) y 2 (las líneas unen las fronteras de las elipses). Estas líneas nos permiten hacernos una idea de las distancias entre los  *clusters*.
    *  `cex` y  `cex.text` permiten modificar el tamaño de los puntos y de las etiquetas.

Esta función dispone de muchos más parámetros, que pueden consultarse en la Ayuda de `clusplot.default`. A modo de ejemplo, el código
```{r,eval=FALSE}
library(cluster)
clustering=estudio.paises2$cluster
clusplot(savings2, clustering, shade=TRUE, lines=0, labels=3, color=TRUE,
          col.p="black", main="", sub="", cex.txt=0.75)
```
produce la Figura \@ref(fig:clusplot) (con `main=""` y `sub=""` hemos eliminado el título y subtítulo por defecto). Observaréis que los puntos de *clusters* diferentes se han representado por medio de símbolos diferentes, y que los ejes de coordenadas no se corresponden con las variables originales. En realidad, `clusplot` ha realizado un **Análisis de Componentes Principales** (**ACP**) de los datos, que para dos variables simplemente significa que ha centrado los datos y ha tomado como eje de abscisas la variable con mayor varianza. En este caso, tras centrar los datos, ha tomado como eje de abscisas (la Componente Principal 1)  la "Renta per cápita" y como eje de ordenadas (la Componente Principal 2) la "Tasa de ahorro"; si no os gustan las etiquetas de los ejes por defecto, podéis modificarlas con los parámetros adecuados. Estudiaremos con detalle el Análisis de Componentes Principales en la siguiente lección.

```{r,echo=FALSE,results="hide", label=clusplot, fig.cap='Representación gráfica del clustering "estudio.paises2" usando la función "clusplot".'}
library(cluster)
clustering=estudio.paises2$cluster
clusplot(savings2, clustering, shade=TRUE, lines=0, labels=3, color=TRUE,
          col.p="black", main="", sub="", cex.txt=0.75)
```



## Elección del número de *clusters*

No se conoce ninguna manera de saber *a priori* el número correcto de *clusters* que tenemos que usar para clasificar un conjunto de datos. En cambio, sí que se dispone de algunos métodos más o menos aceptados para decidir *a posteriori*, una vez clasificados los datos en diversos números de *clusters*, cuál es el más adecuado.

El procedimiento general es el siguiente:

(1) Tomamos una secuencia de valores consecutivos de *k*, usualmente de 2 hasta un tercio del número total de puntos (para que el número medio de puntos por *cluster* no sea inferior a 3).

(2) Para cada *k*, ejecutamos varias veces la función `kmeans` con diferentes configuraciones iniciales de *k* centros,  y nos quedamos con un *clustering* que tenga el menor valor de $SSC$ de los obtenidos para esta *k*. Llamaremos $SSC(k)$ al valor  mínimo de las $SSC$ obtenidas para *k* *clusters*.

(3) A partir del vector de los $SSC(k)$, medimos "algo" y a partir de esta medición decidimos la *k*.

Por ejemplo, una regla bastante extendida, y claramente arbitraria, es la **regla del 90%**: "se toma el valor de *k* mas pequeño para el que  $(SST-SSC(k))/SST\geq 0.9$" (observad que $SST-SSC(k)$ es la $SSB$ de un *clustering* que tenga $SSC=SSC(k)$).

Vamos a organizar la información necesaria para aplicar los métodos que vamos a explicar en esta sección a nuestra tabla de datos `savings2`.  Como la tabla contiene información sobre 50 países, probaremos los valores para *k* de 2 a 17. Para cada valor de *k*, calcularemos 500 *clusterings* partiendo de *k* centros escogidos aleatoriamente  con valores de `set.seed` tomados de un vector aleatorio de 500 entradas escogidas de manera equiprobable (con `sample`) entre 1 y 50000. De esta manera, los resultados serán bastante aleatorios, pero como hemos fijado el vector de semillas en un bloque de código anterior, serán reproducibles. Entonces, para cada *k* guardaremos la semilla del primer *clustering* que dé el valor mínimo de $SSC$ entre las 500 repeticiones (para poder volver a calcularlo si es necesario) y su valor de $SSC$. Finalmente, organizaremos la información en una matriz `Resultados` de 3 columnas: una para los valores de *k*, una para los primeros valores de las semillas de aleatoriedad que han dado, para cada *k*, la $SSC$ mínima, y una tercera columna con los correspondientes valores mínimos $SSC(k)$.

```{r}
x=sample(50000,500,rep=FALSE) #Las 500 semillas aleatorias que usaremos
Resultados=c() 
for(k in 2:17){
  SSCs=c()	
  for (i in x){
    set.seed(i)
    SSCs=cbind(SSCs,c(i,kmeans(savings2,k)$tot.withinss))
    }	
  SSC.min=SSCs[,which.min(SSCs[2,])]
  Resultados=rbind(Resultados,c(k,SSC.min))}
dimnames(Resultados)=list(NULL, c("k", "semilla", "SSC"))
Resultados
```

Si ahora, por ejemplo, quisiéramos aplicar la regla del 90%, calcularíamos para cada *k* el valor de $(SST-SS(k))/SST$ y tomaríamos el primer *k* para el que este cociente fuera mayor de 0.9
```{r}
SST #Recordemos el valor de SST
SSB=SST-Resultados[,3]
Resultados[min(which(SSB/SST>0.9)),1]
```
Con la regla del 90% hubiéramos escogido $k=4$, que es justamente lo que hemos hecho en la sección anterior.

Un método muy popular para elegir el valor de *k* es el **método del "codo"**, que consiste en lo siguiente. Si representamos en un gráfico los puntos $(k,SS_C(k))$, la 
línea quebrada que se obtiene uniéndolos es más o menos cóncava. Si podemos detectar un valor de *k* a  partir del cual $SS_C(k)$ disminuya más lentamente que antes de él, ese *k* será el  número recomendable de *clusters* a usar; si, en cambio, no existe ningún *k* con esta propiedad, es señal de que no existe ninguna clasificación natural de los objetos bajo estudio. Esta regla se llama el método del "codo" porque si superpusiésemos un 
brazo en la representación gráfica de los puntos $(k,SS_C(k))$, el punto correspondiente al *k* elegido sería dónde colocaríamos el codo.

Apliquemos este método a nuestra tabla de datos `savings2`. El código siguiente produce la Figura \@ref(fig:CODO), que muestra que el número *k* adecuado de *clusters* parece ser 5:
```{r, label=CODO, fig.cap='Aplicación del método del codo para hallar el número óptimo de clusters en la tabla de datos "savings2".'}
plot(Resultados[,c(1,3)], xlab="k", ylab="SSC(k)", type="b", col="red") 
```


El último test del que queremos hablar para determinar el número adecuado de *clusters* es el llamado **test F**, el cual, aunque es un método muy usado, en nuestra opinión no tiene una justificación teórica suficiente. En este test, para cada  *k*, se calcula el estadístico siguiente:
$$
 F_k=\frac{SS_C(k)-SS_C(k+1)}{SS_C(k+1)/(n-k-1)},
$$
donde, recordemos, $n$ es el número de filas de nuestra tabla de datos. Se toma entonces como p-valor del contraste, para cada *k*, la probabilidad de que una variable con distribución F de Fisher con $p$ y $p(n-k-1)$ grados de libertad (donde $p$ es el número de variables de nuestra tabla de datos)  tome un valor mayor que $F_k$:
$$
\mbox{p-valor}_k=P(F_{p,p(n-k-1)} > F_k).
$$
Una vez hallado este p-valor para cada *k*,  escogemos como *k* adecuado aquel cuyo p-valor sea el más pequeño. 

Vamos a aplicar este test F a nuestra tabla de datos. Recordemos que ya hemos calculado los valores $SSC(k)$, en la tercera columna de la matriz `Resultados`.

```{r}
k=2:16
n=dim(savings2)[1] 
p=dim(savings2)[2]
c(n,p)
SSC=Resultados[ ,3]
F_k=(SSC[-16]-SSC[-1])/(SSC[-1]/(n-k-1))
p.valores=1-pf(F_k,p,p*(n-k-1))
F.test=cbind(k,F_k,p.valores)
F.test
```

El p-valor más pequeño se obtiene para $k=2$, por lo que éste sería el número adecuado de *clusters* según este test. Para terminar esta sección, vamos a dibujar los *clusters* que se obtienen para $k=2$ y $k=5$:
```{r,fig.cap='Clustering óptimo de la tabla de datos "savings2" con 2 clusters.'}
#k=2
set.seed(48465)
kmeans2=kmeans(savings2,centers=2)
plot(savings2, col=kmeans2$cluster, xlab="Tasa de ahorro", ylab="Renta per cápita",pch=20)
text(savings2, rownames(savings2), cex=0.6, pos=1, col=kmeans2$cluster) 
```

```{r,fig.cap='Clustering óptimo de la tabla de datos "savings2" con 5 clusters.'}
#k=5
set.seed(48465)
kmeans5=kmeans(savings2,centers=5)
plot(savings2, col=kmeans5$cluster, xlab="Tasa de ahorro", ylab="Renta per cápita",pch=20)
text(savings2, rownames(savings2), cex=0.6, pos=1, col=kmeans5$cluster) 
```



## Métodos jerárquicos aglomerativos

Dada una tabla de datos numéricos, un **método de** *clustering* **jerárquico aglomerativo** usa una matriz $D$ de distancias entre las filas de la tabla de datos para ir agrupando secuencialmente los objetos que representan estas filas.   El procedimiento básico es el siguiente: 

(1) Partimos de $n$ objetos y de la matriz $D$ de distancias entre ellos, de orden $n\times n$.
(1) Consideramos que, inicialmente, cada objeto forma un *cluster* de un solo elemento.
(1) Hallamos los dos *clusters* $C_1$ y $C_2$ que están a distancia mínima.
(1) Unimos estos *clusters* $C_1$ y $C_2$ en un nuevo *cluster* $C_1+C_2$.
(1) Eliminamos $C_1$ y $C_2$ de la lista de *clusters*. 
(1) Recalculamos la distancia de $C_1+C_2$ a los demás *clusters*, con lo que obtendremos una matriz de distancias de un orden inferior a la que teníamos;  la manera de recalcular estas distancias es la que da lugar a  algoritmos diferentes.
(1) Repetimos los pasos (3)--(6) hasta que sólo quede un único *cluster*.


La matriz de distancias $D$ entre los objetos  se puede calcular con la función `dist`, cuya sintaxis básica es 
```{r,eval=FALSE}
dist(x, method=...)
```
donde:

* `x` es nuestra tabla de datos (una matriz o un *data frame* de variables cuantitativas).

* `method` sirve para indicar la distancia que queremos usar, cuyo nombre se ha de entrar entrecomillado. La distancia por defecto es la euclídea que hemos venido usando hasta ahora. Otros posibles valores son (en lo que sigue, $x=(x_1,\ldots,x_m)$ e $y=(y_1,\ldots,y_m)$ son dos vectores de $\mathbb{R}^m$):
     * La **distancia de Manhattan**, `"manhattan"`, que entre $x$ e $y$ vale $\sum\limits_{i=1}^m |x_i-y_i|$.
     * La **distancia del máximo**, `"maximum"`, que entre $x$ e $y$ vale $\max_{i=1,\ldots,m} |x_i-y_i|$.
     * La **distancia de Canberra**, `"canberra"`, que entre $x$ e $y$ vale 
$$
\sum_{i=1}^m \frac{|x_i-y_i|}{|x_i|+|y_i|}.
$$
     * La **distancia de Minkowski**, `"minkowski"`, que depende de un parámetro $p>0$ (que se ha de especificar en la función `dist` con `p` igual a su valor), y que entre $x$ e $y$ vale
$$
\sqrt[p]{\sum_{i=1}^m  |x_i -y_i|^p}
$$
Observad que cuando $p=1$ se obtiene la distancia de Manhattan y cuando $p=2$ se obtiene la distancia euclídea usual.
     * La **distancia binaria**, `"binary"`, que sirve básicamente para comparar vectores binarios (si los vectores no son binarios, R los entiende como binarios sustituyendo cada entrada diferente de 0 por 1). La distancia binaria entre $x$ e $y$ binarios es el número de posiciones en las que estos vectores tienen entradas diferentes, dividido por el número de posiciones en las que alguno de los dos vectores tiene un 1.

 
Una vez calculada la matriz de distancias, los diferentes métodos de *clustering* jerárquico aglomerativos están implementados en R en la función `hclust`, cuya sintaxis básica es la siguiente:
```{r,eval=FALSE}
hclust(d, method=...)
```
donde:

* `d` es la matriz de distancias entre nuestros objetos  calculada con la función `dist`.
 
* `method` sirve para especificar cómo se define la distancia de la unión de dos  *clusters* al resto de  los *clusters* en el paso (6) del algoritmo general. El nombre del método se ha de entrar entrecomillado. La función `hclust` dispone de muchos métodos en este sentido, los más populares son los siguientes:

     * El **método de enlace completo**, `"complete"`, que es el método usado por `hclust` por defecto: dados tres *clusters* $C_1,C_2,C$, la distancia de $C_1+C_2$ a $C$ es  
$$
d(C_1+C_2,C)=\max\{d(C,C_1),d(C,C_2)\}
$$
donde (en este método y en los que siguen) las distancias entre *clusters* $d(C,C_1)$  y $d(C,C_2)$ se conocen de pasos anteriores: o bien vienen dadas por la matriz de distancias $D$, si son *clusters* individuales, o bien se han calculado por este mismo método al formarse por la unión de  *clusters* más pequeños.

     * El **método de enlace simple**, `"single"`: dados tres *clusters* $C_1,C_2,C$, la distancia de $C_1+C_2$ a $C$ es  
$$
d(C_1+C_2,C)=\min\{d(C,C_1),d(C,C_2)\}.
$$

     * El **método de enlace promedio**, `"average"`, más conocido  en filogenética como **UPGMA** (*Unweighted Pair Group Method Using Arithmetic
averages*): dados tres *clusters* $C_1,C_2,C$, la distancia de $C_1+C_2$ a $C$ es  
$$
d(C_1+C_2,C)=\frac{|C_1|}{|C_1|+|C_2|}d(C,C_1)+\frac{|C_2|}{|C_1|+|C_2|}d(C,C_2),
$$
 donde $|\ |$ denota el cardinal de cada *cluster*.
 

     * El **método de Ward clásico**, `"ward.D"`: dados tres *clusters* $C_1,C_2,C$, la distancia de $C_1+C_2$ a $C$ es  
$$
\begin{array}{rl} 
d(C_1+C_2,C)= & \displaystyle \frac{|C|+|C_1|}{|C|+|C_1|+|C_2|}d(C,C_1)+\frac{|C|+|C_2|}{|C|+|C_1|+|C_2|}d(C,C_2)\\[2ex] & \displaystyle -\frac{|C|}{
(|C|+|C_1|+|C_2|)^2}d(C_1,C_2).
\end{array}
$$


Para ilustrar cómo usar los métodos aglomerativos con R usaremos la famosa tabla de datos `iris` que reúne las longitudes y amplitudes de los sépalos y pétalos de $150$ flores de tres  especies de iris diferentes: Setosa, Versicolor y Virginica. Vamos a escoger al azar (pero fijando la semilla de aleatoriedad, para que sea reproducible) 5 flores de cada especie, calcularemos las distancias euclídeas entre los vectores numéricos de sus longitudes y amplitudes de sépalos y pétalos, y usaremos estas distancias para calcular algunos *clusterings* jerárquicos aglomerativos de estas flores, con diversos métodos.

```{r}
set.seed(100) 
iris.setosa=iris[iris$Species=="setosa",] #Subtabla de flores setosa
iris.versicolor=iris[iris$Species=="versicolor",] #Subtabla de flores versicolor
iris.virginica=iris[iris$Species=="virginica",] #Subtabla de flores virginica
flores.setosa=iris.setosa[sample(1:50,5),] #Muestra de 5 flores setosa
flores.versicolor=iris.versicolor[sample(1:50,5),] #Muestra de 5 flores versicolor
flores.virginica=iris.virginica[sample(1:50,5),] #Muestra de 5 flores virginica
flores=rbind(flores.setosa,flores.versicolor,flores.virginica) #Tabla de datos con las 15 flores
```

En el *data frame* `flores`, las filas han heredado sus números de la tabla `iris` original, como podemos comprobar:
```{r}
head(flores)
```

Renumeremos las filas del 1 al 15:
```{r}
rownames(flores)=1:15
head(flores)
```

Vamos a guardar en una matriz llamada `distancias.iris` las distancias euclídeas entre los vectores de longitudes y amplitudes de pares de miembros de la tabla `flores`. 

```{r}
distancias.iris=dist(flores[ ,1:4])
```

Calculamos finalmente el *clustering* jerárquico de estas 15 flores usando el método del enlace completo, y lo llamamos `estudio.flores` para poder referirnos a él en lo sucesivo:
```{r}
estudio.flores=hclust(distancias.iris)
```


Para poder comprender un *clustering* jerárquico, lo mejor es dibujarlo en forma de árbol binario o, en este contexto, de **dendrograma**, con los objetos que clasificamos en las hojas. Para ello le aplicamos simplemente la función `plot`. Al usar esta función para representar gráficamente un *clustering* jerárquico, hay dos parámetros que hay que tener en cuenta (aparte de los usuales):

* `hang`, que controla la situación de las hojas del dendrograma respecto del margen inferior.

* `labels`, que permite poner nombres a los objetos; por defecto, se identifican en la representación gráfica por medio de sus números de fila en la matriz o el *data frame* que contiene los datos.

Demos una ojeada al *clustering* `estudio.flores`; usaremos primero el valor de `hang` por defecto (que es `hang=0.1`) y luego  `hang=-1` para que veáis la diferencia. Pondremos además nombres a las flores para identificar su especie, etiquetas adecuadas en los ejes, y con `main=""`y `sub=""` eliminaremos el título y el subtítulo que añade R por defecto.
```{r}
nombres.flores=c(paste("Set",1:5,sep=""), paste("Vers",1:5,sep=""), paste("Vir",1:5,sep=""))
nombres.flores
```

```{r, fig.cap='Dendrograma para 15 flores de iris producido con el método de enlace completo y dibujado con hang=0.1.'}
plot(estudio.flores, labels=nombres.flores, xlab="flores", ylab="distancias", main="", sub="")
```

```{r, label=hclustiris, fig.cap='Dendrograma para 15 flores de iris producido con el método de enlace completo y dibujado con hang=-1.'}
plot(estudio.flores, labels=nombres.flores, xlab="flores", ylab="distancias", main="", sub="", hang=-1)
```

Vemos cómo el *clustering* jerárquico separa, en nuestra muestra, las flores setosa del resto, mientras que las versicolor y las virginica aparecen algo mezcladas. 


¿Cuál sería el resultado de usar otros métodos para calcular las distancias entre *clusters*? Veamos los resultados con los métodos de enlace simple, promedio y Ward.

```{r, fig.cap='Dendrograma para 15 flores de iris producido con el método de enlace simple.'}
plot(hclust(distancias.iris,method="single"), hang=-1, 
  labels=nombres.flores, xlab="flores", ylab="distancias", main="", sub="") 
```

```{r, fig.cap='Dendrograma para 15 flores de iris producido con el método de enlace promedio.'}
plot(hclust(distancias.iris,method="average"), hang=-1, main="", sub="")
```

```{r, fig.cap='Dendrograma para 15 flores de iris producido con el método de Ward.'}
plot(hclust(distancias.iris,method="ward.D"), hang=-1, 
  labels=nombres.flores, xlab="flores", ylab="distancias", main="", sub="")
```

Observamos que cada método ha dado lugar a una jerarquía de *clusters* diferente.


Volviendo a la función `hclust`, veamos la estructura del *clustering* `distancias.iris`:
```{r}
str(estudio.flores)
```

Como vemos, un *clustering* jerárquico producido con `hclust` es una `list`. Sus dos componentes más interesantes para nosotros son las siguientes.

Por un lado, la componente `merge` es una matriz de dos columnas que indica el orden en el que se han ido agrupando los objetos de dos en dos. En esta matriz, los objetos originales se representan con números negativos, y los nuevos *clusters* con números positivos que indican el paso en el que se han creado. En nuestro ejemplo:
```{r} 
estudio.flores$merge
```
Este resultado nos dice que: en el primer paso del algoritmo, se han agrupado las flores 1 y 3; en el segundo paso del algoritmo, se han agrupado las flores 7 y 12; en el tercer paso del algoritmo, se han agrupado el *cluster* formado en el primer paso (flores 1 y 3) y la flor 2; en el cuarto paso, se han agrupado el *cluster* formado en el tercer paso (flores 1, 2 y 3) y la flor 4; y así sucesivamente. Es conveniente que dediquéis un rato a comparar esta matriz de emparejamientos con los dendrogramas que representan el *clustering*, como por ejemplo el de la Figura \@ref(fig:hclustiris).

La otra componente que nos interesa es `height`, un vector que contiene las distancias a las que se han ido agrupando los pares de *clusters*, representadas como alturas en el eje de ordenadas en el dendrograma. Por ejemplo:
```{r}
round(estudio.flores$height,4)
```
indica que las dos flores agrupadas en el primer paso estaban a distancia `r round(estudio.flores$height[1],4)`, las dos flores agrupadas en el segundo paso estaban a distancia `r round(estudio.flores$height[2],4)`, el *cluster* y la flor agrupados en el tercer paso estaban a distancia `r round(estudio.flores$height[3],4)`, etc. De nuevo, es conveniente comparar esta lista de alturas con el dendrograma de la Figura \@ref(fig:hclustiris): las líneas horizontales que forman los *clusters* están a las alturas en los que se han unido sus componentes.

Un *clustering* jerárquico puede usarse para definir un *clustering* ordinario, es decir, una clasificación de los objetos bajo estudio. Esto se puede hacer de dos maneras: indicando cuántos *clusters* deseamos, o indicando a qué altura queremos cortar el dendrograma, de manera que *clusters* que se unan a una distancia mayor que dicha altura queden separados. Por ejemplo, en el dendrograma representado en la Figura \@ref(fig:hclustiris), si cortamos a altura 3 obtenemos 3 *clusters*: de izquierda a derecha, uno formado  por las cinco setosa, otro formado por cuatro virginica y una versicolor, y un tercero formado por la virginica y las cuatro versicolor restantes. Lo podéis comprobar imaginando una recta horizontal en el dendrograma a altura 3 y visualizando qué grupos forma.

Con R disponemos de dos funciones básicas para obtener agrupamientos a partir de un *clustering* jerárquico. La primera es la función `cutree`. Su sintaxis básica es
```{r,eval=FALSE}
cutree(hclust, k=..., h=...)
```
donde `hclust` es el resultado de una función homónima, y se ha de especificar o bien el parámetro `k` que indica el número de *clusters* deseado o bien el parámetro `h` que indica la altura a la que queremos cortar. Veamos dos ejemplos. Para cortar a altura 3, usamos:
```{r}
cutree(estudio.flores,h=3) 
```
Y para clasificar en 4 *clusters*, usamos:
```{r}
cutree(estudio.flores,k=4) 
```

Como vemos, el resultado de `cutree` es un vector similar a la componente `cluster` de un `kmeans`.

Otra posibilidad es usar la función `rect.hclust`, que sobre el dendrograma dibujado en la instrucción inmediatamente anterior resalta los grupos enmarcándolos en rectángulos. La sintaxis es similar: se aplica al resultado de un `hclust` y al número `k` de grupos o a la altura `h`. Admite además un parámetro `border` que permite especificar los colores de los rectángulos (por defecto, todos rojos).

Veamos el resultado de su aplicación por defecto en los mismos casos que hemos aplicado `cutree`:

```{r, fig.cap="Clustering de 15 flores de iris a partir de su dendrograma calculado con el método de enlace  completo, cortándolo a altura 3."}
plot(estudio.flores, labels=nombres.flores, xlab="flores", ylab="distancias", main="", sub="", hang=-1)
rect.hclust(estudio.flores,h=3)
```


```{r,fig.cap="Clustering de 15 flores de iris a partir de su dendrograma calculado con el método de enlace  completo, usando 4 grupos."}
plot(estudio.flores, labels=nombres.flores, xlab="flores", ylab="distancias", main="", sub="", hang=-1)
rect.hclust(estudio.flores,k=4)  
```



## Guía rápida

*   `kmeans` aplica el algoritmo de k-means a una tabla de datos. Sus parámetros principales son:

    * `centers`, que sirve para especificar o bien el número de *clusters* o bien las coordenadas de los centros iniciales;
    * `iter.max`, que permite especificar el número máximo de iteraciones a realizar; 
    * `algorithm`, que indica el algoritmo específico a usar.
    
    El resultado es una `list` cuyas componentes principales son:

    *   `cluster`: vector que especifica a qué *cluster* pertenece cada individuo.
    *   `centers`: matriz de los centros de los *clusters*.
    *   `totss`: valor de $SST$.
    *   `withinss`: vector $(SSC_1,\ldots,SSC_k)$ .
    *   `tot.withinss`: valor de $SSC$.
    *   `betweenss`: valor de $SSB$.
    *   `size`: vector de los tamaños de los *clusters*.


* `clusplot` del paquete **cluster**, permite representar gráficamente el resultado de un `kmeans`, enmarcando los *clusters* con elipses. Se aplica a la tabla de datos original y al componente `cluster` del `kmeans`, y dispone de muchos parámetros entre los que destacan, aparte de los usuales para `plot`:

     * `shade`: igualado a `TRUE`, sombrea las elipses según su densidad.
     * `color`: igualado a `TRUE`, colorea las elipses según su densidad.
     * `labels`: permite indicar qué queremos etiquetar en el gráfico.
     * `col.clus`: permite modificar los colores de las elipses.
     * `col.p`: permite modificar los colores de los puntos.
     * `col.txt`:  permite modificar los colores de las etiquetas.
     *   `lines`: permite añadir líneas uniendo los *clusters*.
     *   `cex`: permite modificar el tamaño de los puntos.
     *   `cex.text`: permite modificar el tamaño de las etiquetas.


* `dist` calcula la matriz de distancias entre las filas de una tabla de datos. Su parámetro principal es `method`, con el que se especifica la distancia concreta.

* `hclust`, aplicada a una matriz de distancias calculada con `dist`, produce un *clustering* jerárquico aglomerativo de los objetos representados en la tabla de datos original. Su parámetro principal es `method`, que permite especificar el algoritmo concreto que se desea usar. El resultado es una `list`, cuyas componentes principales son:

     * `merge`: una matriz que indica el orden en el que se han realizado los agrupamientos.

     * `height`: un vector que indica las  distancias a las que se han realizado los agrupamientos.

* `plot`, aplicado al resultado de `hclust`, dibuja su dendrograma. Los parámetros específicos más importantes para esta aplicación de `plot` son:

     * `hang`: controla la situación de las hojas del dendrograma respecto del margen inferior.
     * `labels`: permite modificar las etiquetas de los objetos representados por las hojas del dendrograma.

*  `cutree` permite obtener un *clustering* ordinario a partir del resultado de un `hclust`, bien sea especificando el número de *clusters* con el parámetro `k`, bien sea especificando la altura a la que deseamos cortar el  *clustering* jerárquico con `h`.


* `rect.hclust` resalta, enmarcados en rectángulos sobre el dendrograma dibujado  con `plot` en la instrucción inmediatamente anterior, los *clusters* que se producen al cortar el resultado de un `hclust` en `k` grupos o a la altura `h`.


## Ejercicios


### Test {-}

*(1)* El *data frame* `kanga` del paquete **faraway** contiene diferentes medidas de los cráneos de 148 ejemplares de canguros de 3 especies. Considerad solo las medidas asociadas a su mandíbula (columnas 17 a 19), y como hay valores NA en estas variables, usad solo las filas que no contengan ningún NA. Vamos a llamar **kanga.mand** a la tabla resultante. Usad esta tabla de datos para realizar un *clustering* (de los canguros que queden en ella) con el método de k-means de Hartigan-Wong en $k=3$ clases, usando como centros iniciales los puntos (1000,100,100), (1200,120,120) y (1500,150,150). Cuántos elementos tiene el *cluster* más grande que se obtiene de esta manera?

*(2)* Seguimos con la tabla **kanga.mand**. Realizad un *clustering* de sus canguros con el método de k-means de Hartigan-Wong en $k=3$ clases usando 3 puntos iniciales escogidos al azar pero fijando la semilla de aleatoriedad en 314. Qué vale la $SSC$ del *cluster* que contiene el quinto ejemplar de la tabla de datos (según el orden de las filas)? Dad el resultado redondeado a un entero. 

*(3)* Seguimos con la tabla **kanga.mand** de las dos preguntas anteriores. Cuál de los dos *clusterings* calculados en las preguntas anteriores es mejor, el de la pregunta (1) o el de la pregunta (2)? La respuesta ha de ser 1 si es el de la pregunta (1), 2 si es el de la pregunta (2), o 0 si ninguno es mejor que el otro.

*(4)* Seguimos con la tabla **kanga.mand** de las preguntas anteriores. Realizad un *clustering* jerárquico de sus canguros usando la distancia euclídea y el método de enlace promedio. Si lo cortáis en 3 clases, cuántos elementos tiene el *cluster* más grande que obtenéis?

*(5)*  Seguimos con la tabla **kanga.mand** de las preguntas anteriores. Realizad un *clustering* jerárquico de sus canguros usando la distancia euclídea y el método de Ward clásico. Si lo cortáis a altura 1000, cuántos  *clusters* se forman?

### Respuestas al test {-}

```{r, include=FALSE}
library(faraway)
kanga.mand=kanga[,17:19]
kanga.mand=na.omit(kanga.mand)
str(kanga.mand)
Centros=rbind(c(1000,100,100),c(1200,120,120),c(1500,150,150))
KM1=kmeans(kanga.mand,centers=Centros)
max(KM1$size)
set.seed(314)
KM2=kmeans(kanga.mand,centers=3)
cluster.5=KM2$cluster[5]
round(KM2$withinss[cluster.5])
KM1$totss
KM2$totss
D=dist(kanga.mand)
HC1=hclust(D,method="average")
max(table(cutree(HC1,k=3)))
HC2=hclust(D,method="ward.D")
table(cutree(HC2,h=1000))
```

*(1)* `r max(KM1$size)`

Nosotros lo hemos calculado con
```{r}
library(faraway)
kanga.mand=kanga[,17:19]
kanga.mand=na.omit(kanga.mand)
Centros=rbind(c(1000,100,100),c(1200,120,120),c(1500,150,150))
KM1=kmeans(kanga.mand,centers=Centros)
max(KM1$size)
```

*(2)* `r as.integer(round(KM2$withinss[cluster.5]))`

Nosotros lo hemos calculado con
```{r}
set.seed(314)
KM2=kmeans(kanga.mand,centers=3)
cluster.5=KM2$cluster[5]
round(KM2$withinss[cluster.5])
```

*(3)* 0
 
Nosotros lo hemos resuelto calculando los dos $SSC$ y viendo que son iguales:
```{r}
KM1$totss
KM2$totss
```


*(4)* `r max(table(cutree(HC1,k=3)))`

Nosotros lo hemos calculado con
```{r}
D=dist(kanga.mand)
HC1=hclust(D,method="average")
table(cutree(HC1,k=3))
```


*(5)* `r length(table(cutree(HC2,h=1000)))`

Nosotros lo hemos calculado con
```{r}
HC2=hclust(D,method="ward.D")
table(cutree(HC2,h=1000))
```

<!--chapter:end:11-Clustering.Rmd-->

