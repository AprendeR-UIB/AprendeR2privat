# Introducción a la estadística descriptiva multidimensional

En general, los datos que se recogen en  experimentos son multidimensionales: medimos varias variables aleatorias sobre una misma muestra de individuos, y organizamos esta información en tablas de datos en las que las filas representan los individuos observados y cada columna corresponde a una variable diferente. En las lecciones finales del primer volumen ya aparecieron datos cualitativos y ordinales multidimensionales, 
para los que  calculamos y representamos gráficamente sus frecuencias globales y marginales; en esta lección estudiamos algunos estadísticos específicos para resumir y representar la relación existente entre diversas variables cuantitativas.

### Linia 938

## Matrices de datos cuantitativos

Supongamos  que hemos medido los valores de $p$ variables aleatorias $X_1,\ldots,X_p$ sobre un conjunto de $n$ individuos u objetos. Es decir, tenemos $n$ observaciones de $p$ variables. En cada observación, los valores que toman estas variables forman un vector que será una realización del vector aleatorio $\underline{X}=(X_1,X_2,\ldots,X_p)$. Para trabajar con estas observaciones, las dispondremos en una tabla de datos donde cada fila corresponde a un individuo y cada columna, a una variable. En R, lo más conveniente es definir esta tabla en forma de *data frame*, pero, por conveniencia de lenguaje, en el texto de esta lección la representaremos como una matriz
$$
{X}=\begin{pmatrix}
x_{1 1} & x_{1 2} &\ldots & x_{1 p}\\
x_{2 1} & x_{2 2} &\ldots & x_{2 p}\\
\vdots & \vdots   &   \ddots    &\vdots\\ 
x_{n 1} & x_{n 2} &\ldots & x_{n p}
\end{pmatrix}.
$$
Utilizaremos las  notaciones siguientes:


* Denotaremos la $i$-ésima fila de $X$ por 
$$
{x}_{i\bullet}=(x_{i 1}, x_{i 2}, \ldots, x_{i p}).
$$
Este vector está compuesto por las observaciones de las $p$ variables sobre el $i$-ésimo individuo.
 
* Denotaremos la  $j$-ésima columna de $X$ por
$$
x_{\bullet j}=\begin{pmatrix}x_{1 j} \\ x_{2 j}\\ \vdots \\ x_{n j}
\end{pmatrix}.
$$
Esta columna está formada por todos los valores de la $j$-ésima variable.


Observad que, en cada caso, la bolita $\bullet$ en el subíndice representa el índice "variable>de los elementos del vector o de la columna.

De esta manera, podremos expresar la matriz de datos $X$ tanto por filas como por columnas:
$$
{X}=\begin{pmatrix}{x}_{1\bullet}\\x_{2\bullet}\\\vdots \\
{x}_{n\bullet}\end{pmatrix}=({x}_{\bullet1}, {x}_{\bullet 2}, \ldots, {x}_{\bullet p}).
$$

Con estas notaciones, podemos generalizar al caso multidimensional los estadísticos de una variable cuantitativa,  definiéndolos como los vectores  que se obtienen aplicando el estadístico concreto a cada columna de la tabla de datos.  Así:

* El **vector de medias** de $X$ es el vector formado por las medias aritméticas de sus columnas:
$$
\overline{X}=(\overline{{{x}}}_{\bullet1}, \overline{{x}}_{\bullet 2}, \ldots, \overline{{x}}_{\bullet p}), 
$$
donde, para cada $j=1, \ldots, p$, 
$$
\overline{{x}}_{\bullet j}=\frac{1}{n}\sum\limits_{i=1}^n x_{i j}.
$$

Observemos que
$$
\begin{array}{rl}
\overline{x} & \displaystyle = (\overline{{{x}}}_{\bullet1}, \overline{x}_{\bullet 2},\ldots,\overline{x}_{\bullet p})
= \frac{1}{n}
\Big(\sum_{i=1}^n x_{i 1}, \sum_{i=1}^n x_{i 2},\ldots,
\sum_{i=1}^n x_{i p}\Big)\\[1ex] & \displaystyle =\frac{1}{n} \sum_{i=1}^n
(x_{i 1}, x_{i 2},\ldots,x_{i p} )
=
\frac{1}{n} \sum_{i=1}^n {{x}_{i\bullet}}
\end{array}
$$
Es decir, el **vector de medias** de $X$ es la media aritmética de sus vectores fila.


* El **vector de varianzas**  de $X$ es el vector  formado por  las varianzas  de sus columnas:
$$
s^2_{X}=(s^2_{1}, s^2_2, \ldots, s^2_p), 
$$
donde 
$$
s_j^2=\frac{1}{n}\sum_{i=1}^n {(x_{ij}-\overline{{x}}_{\bullet j})^2}.
$$

* El **vector de varianzas muestrales**  de $X$ está formado por  las varianzas muestrales  de sus columnas:
$$
\widetilde{s}^2_{X}=(\widetilde{s}^2_{1}, 
\widetilde{s}^2_2, \ldots, \widetilde{s}^2_p), 
$$
donde 
$$
\widetilde{s}_j^2=\frac{1}{n-1}\sum_{i=1}^n {(x_{ij}-\overline{{x}}_{\bullet j})^2}=\frac{n}{n-1}s_j^2.
$$


* Los **vectores de desviaciones típicas** $s_{X}$ y **de desviaciones típicas muestrales** $\widetilde{s}_{X}$ de $X$ son los  formados por las desviaciones típicas y las desviaciones típicas muestrales de sus columnas, respectivamente:
$$
\begin{array}{l}
s_{X}=(s_{1}, s_2, \ldots, s_p)=(+\sqrt{\vphantom{s_p^2}{s}^2_{1}}, 
+\sqrt{\vphantom{s_p^2}{s}^2_2}, \ldots, +\sqrt{s_p^2})\\[1ex]
\widetilde{s}_{X}=(\widetilde{s}_{1}, 
\widetilde{s}_2, \ldots, \widetilde{s}_p)=(+\sqrt{\vphantom{s_p^2}\widetilde{s}^2_{1}}, 
+\sqrt{\vphantom{s_p^2}\widetilde{s}^2_2}, \ldots, +\sqrt{\widetilde{s}^2_p})
\end{array}
$$

Como en el caso unidimensional, $\overline{X}$ es un estimador insesgado de $E(\underline{X})=\boldsymbol\mu$. 
Por lo que refiere a ${s}^2_{X}$ y $\widetilde{s}^2_{X}$, ambas son estimadores del vector de varianzas de $\underline{X}$: $\widetilde{s}^2_{X}$ es insesgado y,  cuando todas las variables aleatorias del vector son normales, ${s}^2_{X}$ es el máximo verosímil. 

Estos vectores de estadísticos se pueden calcular con R aplicando la función correspondiente al estadístico a todas las columnas de la tabla de datos. La manera más sencilla de hacerlo en un solo paso es usando la función `sapply`, si  tenemos guardada la tabla como un *data frame*, o  `apply` con `MARGIN=2`, si la tenemos guardada en forma de matriz.



```{example, label=multex0}
Consideremos la tabla de datos
$$
X=\begin{pmatrix}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{pmatrix}
$$
formada por 4 observaciones de 3 variables; por lo tanto, $n=4$ y $p=3$. Vamos a guardarla en un *data frame* y a calcular sus estadísticos. 
```

```{r}
X=data.frame(V1=c(1,1,2,3),V2=c(-1,0,3,0),V3=c(3,3,0,1))
X
sapply(X, mean) #Vector de medias
sapply(X, var) #Vector de varianzas muestrales
sapply(X, sd) #Vector de desviaciones típicas muestrales
var_ver=function(x){var(x)*(length(x)-1)/length(x)} #Varianza "verdadera"
sd_ver=function(x){sqrt(var_ver(x))} #Desv. típica "verdadera"
sapply(X, var_ver) #Vector de varianzas "verdaderas"
sapply(X, sd_ver) #Vector de desviaciones típicas "verdaderas"
```


```{remark} 
De ahora en adelante, supondremos que todos los vectores de datos cuantitativos que aparezcan en lo que queda de lección, incluidas las columnas de tablas de datos, son no constantes y, por lo tanto, tienen desviación típica no nula.
```





## Transformaciones lineales

A veces es conveniente aplicar una transformación lineal a una tabla de datos $X$, sumando a cada columna un valor y luego multiplicando cada columna resultante por otro valor. Los dos ejemplos más comunes de    trasformación lineal son el **centrado** y la **tipificación** de datos.

Para **centrar** una matriz de datos $X$, se resta a cada columna su media aritmética:
$$
\widetilde{X}=
\begin{pmatrix}
x_{1 1}- \overline{x}_{\bullet 1}& x_{1 2}- \overline{x}_{\bullet 2} &\ldots & x_{1 p}-
\overline{x}_{\bullet p}\\
x_{2 1} - \overline{x}_{\bullet 1}& x_{2 2}- \overline{x}_{\bullet 2} &\ldots & x_{2 p}-
\overline{x}_{\bullet p}\\
\vdots & \vdots   & \ddots      &\vdots\\ 
x_{n 1} - \overline{x}_{\bullet 1}& x_{n 2}- \overline{x}_{\bullet 2} &\ldots & x_{n p}-
\overline{x}_{\bullet p}
\end{pmatrix}.
$$
Llamaremos a esta matriz la **matriz de datos centrados** de $X$.



```{example, label=multex1-1}
Consideremos de nuevo la matriz de datos del Ejemplo \@ref(exm:multex0},
$$
{X}=\begin{pmatrix}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{pmatrix}
$$
```
Para centrarla, hemos de restar a cada columna su media:
$$
\overline{x}_{\bullet 1}=\frac{1+1+2+3}{4}=1.75,\
\overline{x}_{\bullet 2}=\frac{-1+0+3+0}{4}=0.5,\
\overline{x}_{\bullet 3}=\frac{3+3+0+1}{4}=1.75.
$$
Por lo tanto, su matriz de datos centrados es
$$
\widetilde{X}=\begin{pmatrix}
1-1.75&-1-0.5&3-1.75\\
1-1.75&0-0.5&3-1.75\\
2-1.75&3-0.5&0-1.75\\
3-1.75&0-0.5&1-1.75
\end{pmatrix}=
\begin{pmatrix}
-0.75&-1.5&1.25\\
-0.75&-0.5&1.25\\
0.25&2.5&-1.75\\
1.25 &-0.5&-0.75
\end{pmatrix}.
$$





Dado un vector de datos formado por una muestra de una variable cuantitativa, su **vector de datos tipificados**  es el vector que se obtiene restando a cada entrada la media aritmética del vector y dividiendo  el resultado por su desviación típica.  De esta manera, se obtiene un vector de datos de  media aritmética 0 y varianza 1. Tipificar un vector de datos es conveniente cuando se quiere trabajar con estos datos sin que influyan ni su media ni las unidades en los que están medidos: al dividir por su desviación típica, los valores resultantes son  adimensionales.
Por lo tanto, tipificar las variables de una tabla de datos permite compararlas dejando de lado las diferencias que pueda haber entre sus valores medios o sus varianzas. 

La **matriz tipificada**  de una matriz de datos $X$ es la matriz $Z$ que se obtiene tipificando cada columna; es decir, para tipificar una matriz de datos $X$, restamos a cada columna su media y a continuación dividimos cada columna por la
desviación típica de la columna original en $X$ (que coincide con la desviación típica de la columna "centrada", puesto que sumar o restar constantes no modifica la desviación típica):
$$
Z=\begin{pmatrix}
\frac{x_{1 1}- \overline{x}_{\bullet 1}}{s_1}& \frac{x_{1 2}- \overline{x}_{\bullet 2}}{s_2} &\ldots & \frac{x_{1 p}- \overline{x}_{\bullet p}}{s_p}\\[2ex]
\frac{x_{2 1} - \overline{x}_{\bullet 1}}{s_1}& \frac{x_{2 2}- \overline{x}_{\bullet 2}}{s_2} &\ldots & \frac{x_{2 p}- \overline{x}_{\bullet p}}{s_p}\\[2ex]
\vdots & \vdots   & \ddots      &\vdots\\ 
\frac{x_{n 1} - \overline{x}_{\bullet 1}}{s_1}& \frac{x_{n 2}- \overline{x}_{\bullet 2}}{s_2} &\ldots & \frac{x_{n p}-
\overline{x}_{\bullet p}}{s_p}
\end{pmatrix}.
$$



```{example, label=multex1-3}
Vamos a tipificar  a mano la tabla de datos 
$$
{X}=\begin{pmatrix}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{pmatrix}
$$
del Ejemplo \@ref(exm:multex0). Ya la hemos centrado en el Ejemplo \@ref(exm:multex1-1). Para tipificarla, tenemos que dividir cada columna por la desviación típica de la columna correspondiente en la matriz original. Las varianzas y desviaciones típicas de estas columnas las hemos calculado con R en el Ejemplo \@ref(exm:multex0}, y son
$$
\begin{array}{l}
\displaystyle s^2_1=\frac{1}{4}(1^2+1^2+2^2+3^2)- \Big(\frac{7}{4}\Big)^2=\frac{11}{16}=0.6875\Rightarrow s_1=\sqrt{0.6875}=0.829156\\[2ex]
\displaystyle  s^2_2=\frac{1}{4}((-1)^2+3^2)-\Big(\frac{1}{2}\Big)^2=\frac{9}{4}=2.25 \Rightarrow s_2=\sqrt{2.25}=1.5\\[2ex]
 \displaystyle s^2_3=\frac{1}{4}(3^2+3^2+1^2)- \Big(\frac{7}{4}\Big)^2=\frac{27}{16}=1.6875 \Rightarrow s_3=\sqrt{1.6875}=1.299038
 \end{array}
$$
```

Dividiendo cada columna de la matriz centrada $\overline{X}$ por la correspondiente desviación típica obtenemos
$$
Z=\begin{pmatrix}
-0.75/0.829156&-1.5/1.5&1.25/1.299038\\
-0.75/0.829156&-0.5/1.5&1.25/1.299038\\
0.25/0.829156&2.5/1.5&-1.75/1.299038\\
1.25/0.829156 &-0.5/1.5&-0.75/1.299038
\end{pmatrix}
=\begin{pmatrix}
-0.9045 &  -1.0000 &  0.9622\\
 -0.9045&  -0.3333 &  0.9622\\
 0.3015 &  1.6667 & -1.3471\\
 1.5076 & -0.3333 & -0.5773
\end{pmatrix}
$$





La manera más sencilla de aplicar con R  una transformación lineal a una tabla de datos $X$, y en particular de centrarla o tipificarla, es usando la instrucción 
```{r,eval=FALSE}
scale(X, center=..., scale=...)
```
donde:

* `X`  puede ser tanto  una matriz como un *data frame*; el resultado será siempre una matriz.


*  El valor del parámetro `center` es el vector que restamos a sus columnas, en el sentido de que cada entrada de este vector se restará a todas las entradas de la columna correspondiente.  Su valor por defecto (que no es necesario especificar, aunque también se puede especificar 
con `center=TRUE`) es el vector $\overline{X}$ de medias de $X$; para especificar que no se reste nada, podemos usar
`center=FALSE`. 

* El valor del parámetro `scale` es el vector por el que dividimos las columnas  de $X$:
cada columna se divide por la entrada correspondiente de este vector.
Su valor por defecto (de nuevo, se puede especificar igualando el parámetro a `TRUE`) es el vector  $\widetilde{s}_X$ de  desviaciones típicas **muestrales**;
para especificar que no se divida por nada, podemos usar
`scale=FALSE`. 


En particular, la instrucción 
`scale(X)`
centra la tabla de datos $X$ y divide sus columnas por sus **desviaciones típicas muestrales**; por lo tanto, no la tipifica según nuestra definición, ya que no las divide por sus desviaciones típicas "verdaderas". 

```{example, label=multex1}
Vamos a centrar la tabla de datos $X$ del Ejemplo \@ref(exm:multex0).
```

```{r}
X
X_centrada=scale(X, center=TRUE, scale=FALSE)
X_centrada
```
Observad la estructura del resultado: en primer lugar nos da la matriz centrada, y a continuación nos dice que tiene un atributo llamado `"scaled:center"` cuyo valor es el vector usado para centrarla. Este atributo no interferirá para nada en las operaciones que realicéis con la matriz centrada, pero, si os molesta, recordad que se puede eliminar sustituyendo el resultado de centrar la matriz en los puntos suspensivos de la instrucción siguiente:
```{r,eval=FALSE}
attr(... , "scaled:center")=NULL
```

```{r}
attr(X_centrada, "scaled:center")=NULL
X_centrada
```

Como ya hemos avisado, para tipificar esta tabla de datos  *no* podemos hacer lo siguiente: 
```{r}
X_tip=scale(X)
X_tip
```

Para hacerlo bien según la definición que hemos dado, tenemos dos opciones. Una es  multiplicar la matriz anterior por $\sqrt{n/(n-1)}$, donde $n$ es el número de filas de la tabla.^[Como
$\widetilde{s}_X=\sqrt{\frac{n}{n-1}}\cdot s_X$, se tiene que
$\frac{1}{s_X}=\sqrt{\frac{n}{n-1}}\cdot \frac{1}{\widetilde{s}_X}$; por lo tanto, si queríamos dividir por $s_X$
y `scale(X)} ha dividido por $\widetilde{s}_X$, basta multiplicar su resultado por $\sqrt{\frac{n}{n-1}}$.]
```{r}
n=dim(X)[1]  #Número de filas de X
n
X_tip=scale(X)*sqrt(n/(n-1)) 
X_tip
```

Otra posibilidad es usar, como valor del parámetro `scale}, el vector  $s_X$ de desviaciones típicas de las columnas.
```{r}
sd_ver=function(x){sqrt(var(x)*(length(x)-1)/length(x))} 
X_dtv=sapply(X, sd_ver) #Desviaciones típicas "verdaderas"
X_tip1=scale(X, scale=X_dtv) #Escalamos dividiendo las columnas por X_dtv
X_tip1
```

Observaréis que la matriz resultante es la misma, pero el atributo que indica el vector por el que hemos dividido las columnas es diferente: en este caso, es el de desviaciones típicas.
Ahora, en ambos casos, podemos usar la función `attr` para eliminar los dos atributos, `"scaled:center"` y `"scaled:scale"`, que se han añadido a la matriz tipificada.
```{r}
attr(X_tip, "scaled:center")=NULL
attr(X_tip, "scaled:scale")=NULL
```



## Covarianzas y correlaciones

La **covarianza** entre dos variables es una medida de la propensión que tienen ambas variables a variar conjuntamente. Cuando la covarianza es positiva, si una de las dos variables crece o decrece, la otra tiene el mismo comportamiento; en cambio, cuando la covarianza es negativa, esta tendencia se invierte: si una variable crece, la otra decrece y viceversa. Como interpretar el valor de la covarianza más allá de su signo es difícil, se suele usar una versión "normalizada>de la misma, la **correlación de Pearson**, que mide de manera más precisa la relación lineal entre dos variables. 
La covarianza generaliza la varianza, en el sentido de que la varianza  de una variable es su covarianza consigo misma. Y como en el caso de la varianza, definiremos dos versiones de la covarianza: la **verdadera** y la **muestral**.
La diferencia estará de nuevo en el denominador.


Formalmente, la **covarianza** de las variables ${x}_{\bullet i}$ y
${x}_{\bullet j}$ de una matriz de datos $X$ es
$$
s_{i j}=\frac{1}{n} \sum_{k =1}^n\big((x_{k i}-\overline{{x}}_{\bullet i})(x_{kj}-\overline{{x}}_{\bullet j})\big)= 
\frac{1}{n} \Big(\sum_{k =1}^n x_{k i} x_{k j}\Big) - \overline{{x}}_{\bullet i} \overline{{x}}_{\bullet j},
$$
y  su **covarianza muestral**  es
$$
\widetilde{s}_{ij} =
\frac{1}{n-1} \sum_{k =1}^n\big((x_{k i}-\overline{{x}}_{\bullet i})(x_{kj}-\overline{{x}}_{\bullet j})\big)= 
\frac{n}{n-1} s_{ij}.
$$


El estadístico $\tilde{s}_{ij}$ es siempre un estimador insesgado de la covarianza $\sigma_{i j}$ de las variables aleatorias $X_i$ y $X_j$, 
mientras que 
$s_{i j}$ es su estimador máximo verosímil  cuando la distribución conjunta de $X_i$ y $X_j$ es normal bivariante. Podéis encontrar la definición en la correspondiente entrada de la [Wikipedia](http://es.wikipedia.org/wiki/Distribución_normal_multivariante}).


Es inmediato comprobar a partir de sus definiciones que ambas covarianzas son simétricas, y que la covarianza de una variable consigo misma es su varianza:
$$ 
s_{i j}= s_{j i}, \quad \widetilde{s}_{i j}= \widetilde{s}_{j i}, \quad
s_{i i}=s_{i}^2, \quad \widetilde{s}_{ii}=\widetilde{s}_i^2.
$$

```{example,label=multex-cov-vect1}
La covarianza de las dos primeras columnas de la matriz de datos
$$
X=\begin{pmatrix}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{pmatrix}
$$
del Ejemplo \@ref(exm:multex0) se calcularía de la manera siguiente:
$$
s_{12}=\frac{1}{4}(1\cdot (-1)+1\cdot 0+2\cdot 3+3\cdot 0)-1.75\cdot 0.5=1.25-0.875=0.375
$$
Su covarianza muestral se obtendría multiplicando por $4/3$ este valor:
$$
\widetilde{s}_{12} = \frac{4}{3} s_{12}=0.5.
$$
```


La covarianza **muestral** de dos vectores numéricos de la misma longitud $n$ se puede calcular con R mediante la función `cov`.
Para obtener su covarianza "verdadera", hay que multiplicar el resultado de `cov` por $(n-1)/n$.
```{r}
X
cov(X$V1, X$V2)   #Covarianza MUESTRAL
(3/4)*cov(X$V1, X$V2)  #Covarianza "verdadera"
```
Queremos recalcar que, como en el caso de la varianza con `var`, R calcula con `cov` la versión muestral de la covarianza. 

Las **matrices de covarianzas**  y de  **covarianzas muestrales**  de una tabla de datos $X$ son, respectivamente, 
$$
{S}=
\begin{pmatrix}  
 s_{1 1} & s_{1 2} & \ldots & s_{1 p}\\
 s_{2 1} & s_{2 2} & \ldots & s_{2 p}\\
  \vdots & \vdots  &   \ddots     & \vdots\\
 s_{p 1} & s_{p 2} & \ldots & s_{p p}
\end{pmatrix},\ 
\widetilde{{S}}=
\begin{pmatrix}  
 \widetilde{s}_{1 1} & \widetilde{s}_{1 2} & \ldots & \widetilde{s}_{1 p}\\
 \widetilde{s}_{2 1} & \widetilde{s}_{2 2} & \ldots & \widetilde{s}_{2 p}\\
  \vdots & \vdots  &  \ddots      & \vdots\\
\widetilde{s}_{p 1} & \widetilde{s}_{p 2} & \ldots & \widetilde{s}_{p p}
\end{pmatrix},
$$
donde cada $s_{i j}$ y cada $\widetilde{s}_{i j}$ son, respectivamente, la covarianza  y la covarianza muestral  de las correspondientes columnas ${x}_{\bullet i}$ y ${x}_{\bullet j}$.
Estas matrices de covarianzas  miden la tendencia a la variabilidad conjunta de
los datos de $X$ y, si $n$ es el número de filas de $X$, se tiene que
$$
S=\frac{n-1}{n}\widetilde{{S}}.
$$

La matriz de covarianzas muestrales $\widetilde{{S}}$ es un estimador insesgado de la matriz de covarianzas $\Sigma$ del vector de variables aleatorias $\underline{X}$, y si $\underline{X}$ tiene distribución normal multivariante, $S$ es un estimador máximo verosímil de $\Sigma$.
Ambas matrices de covarianzas son simétricas y tienen  todos sus valores propios  $\geq 0$. 



```{example,label=multex1-cov}
Continuemos con la matriz 
$$
X=\begin{pmatrix}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{pmatrix}
$$
del Ejemplo \@ref(exm:multex0).
Vamos a calcular a mano su matriz de covarianzas, luego la calcularemos con R.
Para realizar los cálculos a mano, es útil organizar los datos y los cálculos intermedios necesarios en una tabla como la siguiente:
$$
\begin{array}{|c||c|c|c|c|c|c|c|c|c|}
\hline
i&{x}_{\bullet 1}&{x}_{\bullet 2}&{x}_{\bullet 3}&{x}_{\bullet 1}^2&{x}_{\bullet 2}^2&{x}_{\bullet 3}^2&{x}_{\bullet 1}{x}_{\bullet 2}&{x}_{\bullet 1}{x}_{\bullet 3}&{x}_{\bullet 2}{x}_{\bullet 3}
\\\hline\hline
1&1&-1&3&1&1&9&-1&3&-3\\
2&1&0&3&1&0&9&0&3&0\\
3&2&3&0&4&9&0&6&0&0\\
4&3&0&1&9&0&1&0&3&0\\\hline
Suma&7&2&7&15&10&19&5&9&-3\\\hline
Media &${7}/{4}$ & ${2}/{4}$ & ${7}/{4}$ & ${15}/{4}$ & ${10}/{4}$ & ${19}/{4}$ & ${5}/{4}$ & ${9}/{4}$ & $-{3}/{4}$\\\hline
\end{array}
$$
```

Así tenemos que 
$$
\begin{array}{l}
\displaystyle 
s_1^2=\frac{1}{4}\Big(\sum_{i=1}^4 x_{i 1}^2\Big)-\overline{x}_{\bullet 1}^2=\frac{15}{4}
-\left( \frac{7}{4}\right)^2=\frac{11}{16}=0.6875\\[2ex]
\displaystyle 
s_2^2=\frac{1}{4}\Big(\sum_{i=1}^4 x_{i 2}^2\Big)-\overline{x}_{\bullet 2}^2=\frac{10}{4} -\left(
\frac{2}{4}\right)^2=\frac{9}{4}=2.25\\[2ex]
\displaystyle 
s_3^2=\frac{1}{4}\Big(\sum_{i=1}^4 x_{i 3}^2\Big)-\overline{x}_{\bullet 3}^2=\frac{19}{4}
-\left( \frac{7}{4}\right)^2=\frac{27}{16}=1.6875\\
\displaystyle 
s_{1 2}=\frac{1}{4}\Big(\sum_{i=1}^n x_{i 1} x_{i 2}\Big) -\overline{x}_{\bullet 1}
\overline{x}_{\bullet 2}=  \frac{5}{4}-\frac{7}{4}\cdot \frac{2}{4}=\frac{3}{8}=0.375\\[2ex]
\displaystyle 
s_{1 3}=\frac{1}{4}\Big(\sum_{i=1}^n x_{i 1} x_{i 3}\Big) -\overline{x}_{\bullet 1}
\overline{x}_{\bullet 3}=  \frac{9}{4}-\frac{7}{4}\cdot  \frac{7}{4}=-\frac{13}{16}=-0.8125\\[2ex]
\displaystyle 
s_{2 3}=\frac{1}{4}\Big(\sum_{i=1}^n x_{i 2} x_{i 3}\Big) -\overline{x}_{\bullet 2}
\overline{x}_{\bullet 3}=  \frac{-3}{4}-\frac{2}{4}\cdot  \frac{7}{4}=-\frac{13}{8}=-1.625
\end{array}
$$
Y por lo tanto, la matriz de covarianzas es 
$$
{S}= \begin{pmatrix}
0.6875 & 0.375& -0.8125 \\
0.375 & 2.25  & -1.625\\
   -0.8125 & -1.625&  1.6875
 \end{pmatrix}.
$$



La matriz de covarianzas muestrales de $X$ se calcula aplicando la función `cov` al *data frame* o a la matriz que contenga dicha tabla.  Para obtener su matriz de covarianzas "verdaderas", es suficiente multiplicar el resultado de  `cov` por $(n-1)/n$, donde $n$ es el número de filas de $X$.
```{r}
n=dim(X)[1]
cov(X)  #Matriz de covarianzas muestrales
((n-1)/n)*cov(X)  #Matriz de covarianzas "verdaderas"
```

Como la matriz de covarianzas es difícil de interpretar  como medida
de variabilidad, debido a que no es una única cantidad sino toda una matriz, interesa cuantificar  esta variabilidad mediante un único índice. No hay consenso sobre este índice, y entre los que se han propuesto destacamos:

* La **varianza total** de $X$: la suma de las varianzas de sus columnas.

* La **varianza media** de $X$: la media de las varianzas de sus columnas, es decir, la varianza total partida por el número de columnas.

* La **varianza generalizada** de $X$: el determinante de su matriz de covarianzas. 

* La  **desviación típica generalizada** de $X$:
la raíz cuadrada positiva de su varianza generalizada. 



Pasemos ahora a la **correlación lineal de Pearson** (o, de ahora en adelante, simplemente **correlación de Pearson**) de dos variables 
${x}_{\bullet i}$ y ${x}_{\bullet j}$ de $X$, que se define como
$$
r_{i j}=\frac{s_{i j}}{s_i s_j}.
$$
Observad que 
$$
\frac{\widetilde{s}_{i j}}{\widetilde{s}_i\cdot \widetilde{s}_j}=
\frac{\frac{n}{n-1}\cdot {s}_{i j}}{\sqrt{\frac{n}{n-1}}\cdot {s}_i \cdot\sqrt{\frac{n}{n-1}}\cdot{s}_j}=
\frac{s_{i j}}{s_i \cdot s_j}=r_{i j},
$$
y, por lo tanto, esta correlación se puede calcular también a partir de las versiones muestrales de la covarianza y las desviaciones típicas  por medio de la misma fórmula. 

El estadístico $r_{ij}$ es un estimador máximo verosímil de la correlación de Pearson  $\rho_{i
j}=Cor(X_i,X_j)$ de las variables aleatorias $X_i$ y $X_j$
cuando su distribución conjunta es normal bivariante, y es sesgado, aunque su sesgo  tiende a 0 cuando $n$ tiende a $\infty$.
Las propiedades más importantes de $r_{i,j}$ son las siguientes:

* Es simétrica: $r_{i j}=r_{j i}$.

* $-1\leq r_{i j}\leq 1$.

* $r_{i i}=1$.

* $r_{i j}$ tiene el mismo signo que $s_{i j}$.

* $r_{i j}=\pm 1$ si y, sólo si, existe una relación lineal perfecta entre las
variables ${x}_{\bullet i}$ y ${x}_{\bullet j}$: es decir, si, y sólo si, existen valores $a, b\in \mathbb{R}$ tales que 
$$
\left(\begin{array}{c}
x_{1j}\\  \vdots \\ x_{nj}\end{array}\right)=
a\cdot \left(\begin{array}{c}
x_{1i}\\ \vdots \\ x_{ni}\end{array}\right) +b.
$$
La pendiente $a$ de esta relación lineal tiene el mismo signo
que $r_{i j}$.

* El coeficiente de determinación $R^2$ de la regresión lineal por mínimos cuadrados de  ${x}_{\bullet j}$ respecto de ${x}_{\bullet i}$ 
es igual al cuadrado de su correlación de Pearson, $r_{i j}^2$; por lo tanto, cuánto más se aproxime el valor absoluto de $r_{ij}$  a $1$,
más se acercan las
variables ${x}_{\bullet i}$ y ${x}_{\bullet j}$ a depender linealmente la una de la otra.


Así pues, la correlación de Pearson entre dos variables viene a ser una covarianza "normalizada", ya que, como vemos, su valor está entre -1 y 1, y  mide la tendencia de las variables a estar relacionadas según una función lineal. En concreto, cuanto más se acerca dicha correlación  a 1 (respectivamente, a -1), más se acerca una (cualquiera) de las variables a ser función lineal creciente (respectivamente, decreciente) de la otra.  

Con R, la correlación de Pearson de dos vectores se puede calcular aplicándoles la función `cor`.




```{example} 
En ejemplos anteriores hemos calculado la covarianza y las varianzas de las dos primeras columnas de la matriz de datos
$$
{X}=\begin{pmatrix}
1&-1&3\\
1&0&3\\
2&3&0\\
3&0&1
\end{pmatrix}
$$
Hemos obtenido los valores siguientes
$$
s_{12}=0.375,\quad s_1=\frac{\sqrt{11}}{4}=0.82916,\quad s_2= \frac{3}{2}=1.5.
$$
Por lo tanto, su correlación de Pearson es
$$
r_{1 2}=\frac{0.375}{0.82916\cdot 1.5}=0.3015.
$$
```

Ahora vamos a calcularla con R, y aprovecharemos para confirmar su relación con el valor de $R^2$ de la regresión lineal de la segunda columna respecto de la primera.
```{r}
X
cor(X$V1, X$V2)
cor(X$V1, X$V2)^2
summary(lm(X$V2~X$V1))$r.squared
```


La **matriz de correlaciones de  Pearson** de $X$ es
$$
{R}=
\begin{pmatrix}
1 & r_{1 2} & \ldots & r_{1 p}\\
r_{2 1} & 1 & \ldots & r_{2 p}\\
\vdots & \vdots & \ddots & \vdots\\
r_{p 1} & r_{p 2} & \ldots & 1
\end{pmatrix}
$$
donde cada $r_{i j}$ es la correlación de Pearson de las  columnas correspondientes de  $X$.
Esta matriz de correlaciones tiene siempre determinante
$|R|\leq 1$ y  todos sus valores propios  $\geq 0$, y  con R se puede calcular aplicando la misma instrucción `cor` a la tabla de datos.



```{r}
cor(X)
```



R también dispone de la función `cov2cor} que aplicada a la matriz de covarianzas (muestrales o no) calcula la matriz de correlaciones de Pearson:
```{r}
cov2cor(S)
```


Se tiene el teorema siguiente, que se puede demostrar mediante un simple, aunque farragoso, cálculo algebraico:

```{theorem}
La matriz de correlaciones de Pearson de $X$ es igual a  la matriz de covarianzas de
 su matriz  tipificada y también igual a la matriz de covarianzas muestrales de su matriz tipificada obtenida dividiendo por las desviaciones típicas muestrales en vez de por las "verdaderas".
```

La importancia de este resultado es que, si la tabla de datos es muy grande, suele ser más eficiente calcular la matriz de covarianzas de su matriz tipificada que la matriz de correlaciones de Pearson de la tabla original.
Comprobemos que el teorema es cierto para nuestra matriz de datos:
```{r}
cor(X)
cov(scale(X))
n=dim(X)[1]
X_tip=scale(X)*sqrt(n/(n-1)) 
cov(X_tip)*(n-1)/n
```

Cuando se calcula la covarianza o la correlación de Pearson de dos vectores que contienen valores `NA`, lo usual es no tenerlos en cuenta: es decir, si un vector contiene un `NA` en una  posición, se eliminan de los dos vectores sus entradas en dicha posición. De esta manera, se tomaría como covarianza de
$$
\left(\begin{array}{c}
1\\ 2\\ \mbox{\it NA}\\ 4\\ 6\\ 2\end{array}\right)\mbox{ y }
\left(\begin{array}{c} 2\\ 4\\ -3\\ 5\\ 7\\ \mbox{\it NA}\end{array}\right)
$$
la de
$$
\left(\begin{array}{c}1\\ 2\\ 4\\ 6\end{array}\right)\mbox{ y }
\left(\begin{array}{c} 2\\ 4\\ 5\\ 7\end{array}\right).
$$
Al aplicar `cov` o `cor` a un par de vectores que contengan `NA`, se obtiene, por defecto, `NA`. Si se quiere que R calcule el valor sin tener en cuenta los `NA`, se ha de especificar  añadiendo el parámetro `use="complete.obs"` (que le indica que ha de usar las observaciones completas, es decir, las posiciones que no tienen `NA` en ninguno de los dos vectores).
```{r}
x=c(1,2,NA,4,6,2)
y=c(2,4,-3,5,7,NA)
x1=c(1,2,4,6)  #Quitamos las entradas 3a y 6a 
x2=c(2,4,5,7)  #Quitamos las entradas 3a y 6a 
cov(x, y)
cov(x, y, use="complete.obs")
cov(x1, x2)
cor(x, y)
cor(x, y, use="complete.obs")
cor(x1, x2)
```

Al calcular las matrices de covarianzas, covarianzas muestrales o correlaciones de una tabla de datos que contenga `NA`, se suele seguir una de las dos estrategias siguientes, según lo que interese al usuario:

* Para cada par de columnas, se calcula su covarianza o su correlación 
con la estrategia explicada más arriba para dos vectores, obviando el hecho de que forman parte de una tabla de datos mayor; es decir, al efectuar el cálculo para cada par de columnas concreto, se eliminan de cada una de ellas sus entradas `NA` y aquellas en cuya fila la otra tiene un `NA`. 
Esta opción se especifica dentro de la función `cov` o `cor` con el parámetro `use="pairwise.complete.obs"`.

* Antes de nada, se eliminan  las filas de la tabla que contienen algún `NA` en alguna columna, dejando solo en la tabla las filas "completas", las que no contienen ningún `NA`. Luego se calcula la matriz de covarianzas o de correlaciones  de  la tabla resultante. Esta opción se especifica con el parámetro   `use="complete.obs"`. 

Veamos un ejemplo:
```{r}
X=cbind(c(1,2,NA,4,6,2), c(2,4,-3,5,7,NA), c(-2,1,0,2,3,0))
X
cov(X)
cov(X, use="pairwise.complete.obs")
cov(X, use="complete.obs")
Y=cbind(c(1,2,4,6), c(2,4,5,7), c(-2,1,2,3)) #Eliminamos las filas con algún NA
Y
cov(Y)  #Dará lo mismo que con use="complete.obs"
```

Como ya hemos comentado, podemos usar la correlación de Pearson $r_{xy}$ de dos vectores $x$ e $y$, correspondientes a los valores de dos variables cuantitativas $X,Y$ sobre una misma muestra de individuos, para estimar la correlación $\rho_{XY}$ de estas variables poblacionales. Cuando además ambas variables aleatorias son normales, disponemos de una fórmula para calcular  intervalos de confianza para la correlación poblacional y de un método para efectuar contrastes de hipótesis con hipótesis nula $H_0: \rho_{XY}=0$. No vamos a entrar en los detalles de las fórmulas ni de los teoremas en que se basan, pero es importante que recordéis que la función de R que lleva a cabo dichos contrastes "de correlación"
es la función `cor.test`. En particular, esta función calcula el intervalo de confianza asociado a un contraste de estos: si el contraste es bilateral, es decir, con hipótesis alternativa $H_1: \rho_{XY}\neq 0$,  
el intervalo que produce esta función es el intervalo de confianza para $\rho_{XY}$ con nivel de confianza correspondiente al nivel de significación del contraste. 

La sintaxis de `cor.test`   es la misma que la del resto de funciones para realizar contrastes de hipótesis básicos:
```{r, eval=FALSE}
cor.test(x, y, alternative=..., conf.level=...)
```
donde `x` e `y` son los dos vectores de datos, que también se pueden especificar mediante una fórmula. Estos dos vectores han de tener la misma longitud, puesto que se entiende que son mediciones sobre el mismo conjunto de individuos. El parámetro `alternative` puede tomar los tres  valores usuales y su valor por defecto es, como siempre, `"two.sided"`, que corresponde al contraste bilateral. Los valores `alternative="greater"` y  `alternative="less"` permiten contrastar si $X$ e $Y$ tienen correlación positiva o negativa, respectivamente.


```{example} 
Queremos contrastar si hay correlación positiva entre el peso de una madre en el momento de la concepción del hijo y el peso de su hijo en el momento de nacer. Para ello vamos a usar la tabla de datos  `birthwt` incluida en el paquete **MASS** que ya usamos en una lección anterior, que contiene información sobre recién nacidos y sus madres, y que en particular dispone de las variables `bwt`, que da el peso del recién nacido en gramos, y `lwt`, que da el peso de la madre en libras en el momento de su última menstruación. Vamos a suponer que ambos pesos siguen distribuciones normales. Si denotamos por $X$ e $Y$ las correspondientes variables poblacionales, queremos realizar el contraste
$$
\left\{\begin{array}{l}
H_0: \rho_{XY}=0\\
H_1: \rho_{XY}>0
\end{array}\right.
$$
Vamos a usar la función `cor.test`.
```

```{r}
library(MASS)
cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")
```

El p-valor `r round(cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")$p.value,3)`  nos da evidencia estadísticamente significativa de que, en efecto, hay una correlación positiva entre el peso de la madre y el peso del recién nacido. El último valor, `r cor.test(birthwt$bwt, birthwt$lwt, alternative="greater")$estimate`, es la correlación de Pearson de los dos vectores de pesos, y el intervalo de confianza del 95% es el del test unilateral planteado.
 Si hubiéramos querido calcular un intervalo de confianza del 95% para $\rho_{XY}$ que repartiera por igual a ambos lados el 5% de probabilidad de no contener su valor real, hubiéramos podido usar el del contraste bilateral:
```{r}
cor.test(birthwt$bwt, birthwt$lwt)$conf.int
```

La potencia del contraste de correlación se calcula con la función `pwr.r.test` del paquete **pwr**. En este caso, el tamaño del efecto es simplemente la correlación de Pearson, que se entra en la función mediante el parámetro `r`:
```{r}
library(pwr)
dim(birthwt)
cor(birthwt$bwt,birthwt$lwt)
pwr.r.test(n=189,r=0.186,sig.level=0.05,alternative="greater")
```
La probabilidad de error de tipo II en este contraste era de un poco menos del 18\%.


La correlación de Pearson mide específicamente la tendencia de dos variables cuantitativas continuas a depender linealmente una de otra. En circunstancias en las que no esperemos esta dependencia lineal, o en las que nuestras variables sean cuantitativas discretas o simplemente cualitativas, usar la correlación de Pearson para analizar la relación entre dos variables no es lo más adecuado. Entre las propuestas alternativas, la más popular es la **correlación de Spearman**. Este índice asigna a cada valor de cada vector su **rango** (su posición en el vector ordenado de menor a mayor, y en caso de empates la media de las posiciones que ocuparían todos los empates) y calcula la correlación de Pearson de estos rangos. Con R, la correlación de Spearman se calcula directamente con la función `cor` entrándole el parámetro `method="spearman"`.

```{example} 
Vamos a calcular la correlación de Spearman de las dos primeras columnas de la matriz de datos que hemos venido usando en nuestros ejemplos. En la tabla siguiente calculamos los rangos de sus entradas:
$$
\begin{array}{|c|c|c|c|}
\hline
{x}_{\bullet 1}& rango & {x}_{\bullet 2}& rango
\\\hline\hline
1& 1.5 & -1& 1 \\
1&1.5 & 0 & 2.5\\
2&3 & 3& 4 \\
3&4 & 0&  2.5\\\hline
\end{array}
$$
¿Cómo hemos obtenido los rangos? Fijaos por ejemplo en la primera columna: los dos 1 ocuparían la posición 1 y 2, les asignamos a ambos como rango la media de estas posiciones, 1.5; el 2 ocuparía la posición 3 y el 3 ocuparía la posición 4, y estos son también sus rangos.
```

Por lo tanto, la correlación de Spearman de
$$
(1,1,2,3)\mbox{ y }(-1,0,3,0)
$$
es la correlación de Pearson de
$$
(1.5,1.5,3,4)\mbox{ y }(1,2.5,4,2.5)
$$
```{r}
x=c(1,1,2,3)
y=c(-1,0,3,0)
rank(x)  #Los rangos de x
rank(y) #Los rangos de y
cor(x,y,method="spearman")
cor(rank(x),rank(y))
```
Vemos que, efectivamente, coinciden.




## Un ejemplo

Recordaréis el *data frame* `iris`, que tabulaba las longitudes y anchuras de los pétalos y los sépalos de una muestra de flores iris de tres especies. Vamos a extraer una subtabla con sus cuatro variables numéricas y calcularemos sus matrices de covarianzas y correlaciones.
```{r}
str(iris)
iris_num=iris[, 1:4]
cov(iris_num)  #Covarianzas muestrales
n=dim(iris_num)[1]    #Número de filas
n
cov(iris_num)*(n-1)/n  #Covarianzas "verdaderas"
cor(iris_num)    #Correlaciones
```

Observamos, por ejemplo, una gran correlación  de Pearson positiva entre la longitud y la anchura de los pétalos, `r cor(iris_num)[3,4]`, lo que indica una estrecha relación lineal con pendiente positiva entre estas magnitudes. Valdría la pena, entonces, calcular la recta de regresión lineal de una de estas medidas en función de la otra:
```{r}
lm(Petal.Length~Petal.Width, data=iris_num)
```
En cambio, la correlación de Pearson entre la longitud y la anchura de los sépalos es `r cor(iris_num)[1,2]`, muy cercana a cero, lo que es señal de que la variación conjunta de las longitudes y anchuras de los sépalos no tiene una tendencia clara.

Vamos a ordenar ahora los pares de variables numéricas de `iris` en orden decreciente de su correlación  en valor absoluto, para saber cuáles están más correlacionadas (en positivo o negativo). Para ello,  en primer lugar creamos un *data frame* cuyas filas están formadas por pares diferentes de variables numéricas de `iris`, su correlación de Pearson y el valor absoluto de esta última, y a continuación ordenamos las filas de este *data frame* en orden decreciente de estos valores absolutos.  Todo esto lo llevamos a cabo con el código siguiente, que luego explicamos:
```{r}
medidas=names(iris_num)
n=length(medidas)  #En este caso, n=4
indices=upper.tri(diag(n))
medida1=matrix(rep(medidas, times=n), nrow=n, 
  byrow=FALSE)[indices]
medida2=matrix(rep(medidas, times=n), nrow=n, 
  byrow=TRUE)[indices]
corrs=as.vector(cor(iris_num))[indices]  
corrs.abs=abs(corrs)
corrs_df=data.frame(medida1, medida2, corrs, corrs.abs)
corrs_df
corrs_df_sort=corrs_df[order(corrs_df$corrs.abs, 
   decreasing=TRUE), ]
corrs_df_sort
```
Vemos que el par de variables con mayor correlación de Pearson en valor absoluto son `Petal.Length` y `Petal.Width`, como ya habíamos observado, seguidos por  `Petal.Length` y `Sepal.Length`.


Vamos a explicar el código. La función `upper.tri`, aplicada a una matriz cuadrada $M$, produce la matriz **triangular superior** de valores lógicos del mismo orden que $M$, cuyas entradas $(i,j)$ con $i<j$ son todas  `TRUE`  y el resto todas  `FALSE`. Existe una función similar, `lower.tri`, para producir  matrices **triangulares inferiores** de valores lógicos.
```{r}
upper.tri(diag(4))
lower.tri(diag(4))
```
Ambas funciones disponen del parámetro `diag` que, igualado a `TRUE`, define también como `TRUE` las entradas de la diagonal principal.
```{r}
upper.tri(diag(4), diag=TRUE)
```

Si $M$ es una matriz y $L$ es una matriz de valores lógicos del mismo orden, `M[L]` produce el vector construido de la manera siguiente: de cada columna, se queda sólo con las entradas de $M$ cuya entrada correspondiente en $L$ es `TRUE`, y a continuación concatena estas columnas, de izquierda a derecha, en un vector. 
```{r}
M=matrix(1:16, nrow=4, byrow=T)
M
M[upper.tri(diag(4))] #Las entradas del triángulo superior, por columnas
```
Ahora, tenemos las matrices siguientes:
```{r}
matrix(rep(medidas, times=4), nrow=4, byrow=FALSE)
matrix(rep(medidas, times=4), nrow=4, byrow=TRUE)
```
Por lo tanto, al aplicar estas matrices a la matriz de valores lógicos `upper.tri(diag(4))` obtenemos los nombres de las variables correspondientes a las filas y las columnas del triángulo superior, respectivamente, y al aplicar la matriz de correlaciones a esta matriz de valores lógicos, obtenemos sus entradas en este triángulo; en los tres vectores, las entradas siguen el mismo orden. Esto nos permite construir el *data frame* `corrs_df` cuyas filas están formadas por pares diferentes de variables numéricas de `iris`, su correlación de Pearson y, aplicando `abs` a esta última variable, dicha correlación  en valor absoluto.

Finalmente, la función `order` ordena los valores del vector al que se aplica, en orden decreciente si se especifica el parámetro `decreasing=TRUE`. Cuando aplicamos un *data frame* a una de sus variables reordenada de esta manera, reordena sus filas según el orden de esta variable. En este caso hubiéramos conseguido lo mismo con la función `sort`, pero la función `order` se puede aplicar a más de una variable del *data frame*: esto permite ordenar las filas del *data frame* en el orden de la primera variable de manera que, en caso de empate, queden ordenadas por la segunda variable,  y así sucesivamente.



## Representación gráfica de datos multidimensionales


La representación gráfica de tablas de datos multidimensionales tiene la dificultad de las dimensiones; para dos o tres variables es  sencillo visualizar las relaciones entre las mismas, pero para más variables ya no nos bastan nuestras tres dimensiones espaciales y tenemos que usar algunos trucos, tales como representaciones gráficas conjuntas de pares de variables.

La manera más sencilla de representar gráficamente una tabla de datos formada por dos variables numéricas   es aplicando la función `plot` a la matriz de datos  o al *data frame*. Esta función produce el **diagrama de dispersión** (*scatter plot*) de los datos: el gráfico de los puntos del plano definidos por las filas de la tabla.

A modo de ejemplo, si extrajéramos de la tabla `iris` una subtabla conteniendo sólo las longitudes y anchuras de los pétalos y quisiéramos visualizar la relación entre estas dimensiones, podríamos dibujar su diagrama de dispersión de la manera siguiente:
```{r,label=iris1,fig.cap="Diagrama de dispersión  de las longitudes y anchuras de los pétalos  de las flores representadas en la tabla iris."}
iris.pet=iris[ ,c("Petal.Length","Petal.Width")]
plot(iris.pet, pch=20, xlab="Largo", ylab="Ancho")
```


El resultado es la Figura \@ref(fig:iris1), que muestra una clara tendencia positiva: cuanto más largos son los pétalos, más anchos tienden a ser. Esto se corresponde con la correlación de Pearson de `r round(cor(iris_num)[3,4],3)` que hemos obtenido en la sección anterior. 


Para tablas de datos de tres columnas numéricas, podemos usar con un fin similar la instrucción `scatterplot3d` del paquete homónimo, que dibuja un diagrama de dispersión tridimensional. Como `plot`, se puede aplicar a un *data frame* o a una matriz; por ejemplo, para representar gráficamente las tres primeras variables numéricas de `iris`, podríamos hacer lo siguiente:

```{r,label=iris2,fig.cap="Diagrama de dispersión tridimensional de las tres primeras columnas de la tabla iris."}
library(scatterplot3d)
scatterplot3d(iris[ , 1:3], pch=20)
```
Obtendríamos la Figura \@ref(fig:iris2). Podéis consultar la Ayuda de la instrucción para saber cómo modificar su apariencia: cómo ponerle un título, poner nombres adecuados a los ejes, usar colores, cambiar el estilo del gráfico, etc.


Una representación gráfica muy popular de las tablas de datos de tres o más columnas numéricas son las matrices formadas por los diagramas de dispersión de todos sus pares de columnas. Si la tabla de datos es un *data frame*, esta matriz de diagramas de dispersión  se obtiene simplemente aplicando la función `plot` al *data frame*; por ejemplo,
```{r,eval=FALSE}
plot(iris[ , 1:4])
```
produce el gráfico de la Figura \@ref(exm:fig:iris3). En este gráfico, los cuadrados en la diagonal indican  a qué variables corresponden cada fila y cada columna, de manera que podamos identificar fácilmente qué variables compara cada diagrama de dispersión; así, en el diagrama de la primera fila y segunda columna de esta figura, las abscisas corresponden a anchuras de sépalos y las ordenadas a longitudes de sépalos. Observad que la nube de puntos no muestra una tendencia clara y en todo caso ligeramente negativa, lo que se corresponde con la correlación de Pearson entre estas variables  de `r round(cor(iris_num)[1,2],3)` que hemos obtenido en la sección anterior.

```{r,echo=FALSE,label=iris3,fig.cap="Matriz de diagramas de dispersión  de la tabla iris."}
plot(iris[ , 1:4])
```


Podemos usar los parámetros usuales de `plot` para mejorar el gráfico resultante; por ejemplo, podemos usar colores para distinguir las flores según su especie: 
```{r,eval=FALSE}
plot(iris[ , 1:4], col=iris$Species)
```
produce el gráfico de  la Figura \@ref(fig:iris3b).

```{r,echo=FALSE,label=iris3b,fig.cap="Matriz de diagramas de dispersión  de la tabla iris, con las especies distinguidas por colores."}
plot(iris[ , 1:4], col=iris$Species)
```


Para obtener la matriz de diagramas de dispersión  de una tabla de datos multidimensional también se puede usar la función `pairs`: así,  `pairs(iris[, 1:4])` produce exactamente el mismo gráfico que `plot(iris[, 1:4])`. Una de las ventajas de  `pairs` es que se puede aplicar a una matriz para obtener la matriz de diagramas de dispersión de sus columnas, mientras que `plot` no.
 
El paquete **car** incorpora una función que permite dibujar matrices de diagramas de dispersión enriquecidos con información descriptiva extra de las variables de la tabla de datos y que además facilita el  control del gráfico resultante, por lo que os recomendamos su uso frente a las funciones básicas `plot` y `pairs`. Se trata de la función `spm`; por ejemplo, 
```{r,eval=FALSE}
library(car)
spm(iris[ , 1:4], var.labels=c("Long. Sep.", "Ancho Sep.",
   "Long. Pet.", "Ancho Pet."))
```
produce el gráfico de la izquierda de la Figura \@ref(fig:iris5). Observad para empezar que hemos cambiado los nombres que identifican las variables en los cuadrados de la diagonal, con el parámetro `var.labels`, y que en dichos cuadrados aparecen además unas curvas: se trata de la curva de densidad estimada de la variable correspondiente. La información gráfica contenida en estos cuadrados de la diagonal se puede modificar con el parámetro `diagonal`: podemos pedir, por ejemplo, que dibuje un histograma de cada variable (con `diagonal="histogram"`) o su *boxplot* (con `diagonal="boxplot"`). Así, 
```{r,eval=FALSE}
```
produce el gráfico de la derecha de la Figura \@ref(fig:iris5).

```{r,echo=FALSE, fig.width=10, fig.asp = 0.5, label=iris5,fig.cap="Matrices de diagramas de dispersión de la tabla iris producidos con la función spm."}
par(mfrow=c(1,2))
library(car)
spm(iris[ , 1:4], var.labels=c("Long. Sep.", "Ancho Sep.",
   "Long. Pet.", "Ancho Pet."))
spm(iris[ , 1:4], diagonal="boxplot")
```


Observad también que los diagramas de dispersión de la matriz producida con `spm` contienen curvas. La línea recta verde es la recta de regresión por mínimos cuadrados y, sin entrar en detalle sobre su significado exacto, las curvas rojas continuas representan la tendencia de los datos.

A veces querremos agrupar los datos de las variables  numéricas de una tabla de datos. Los  motivos serán los mismos que cuando se trata de una sola variable: por ejemplo, si los datos son aproximaciones de valores reales, o si son muy heterogéneos.  Cuando tenemos dos variables emparejadas agrupadas, se pueden representar gráficamente las frecuencias de sus pares de clases mediante un **histograma bidimensional**, que divide el conjunto de todos los pares de valores en rectángulos definidos por los pares de intervalos
e indica sobre cada rectángulo su frecuencia absoluta, por ejemplo mediante colores o intensidades de gris (dibujar barras verticales sobre las regiones es una mala idea, las de delante pueden ocultar las de detrás). Hay muchos paquetes de R que ofrecen funciones para dibujar histogramas bidimensionales; aquí explicaremos la función `hist2d` del paquete **gplots**. Su sintaxis básica es
```{r,eval=FALSE}
hist2d(x,y, nbins=..., col=...)
```

donde:

* `x` e `y` son los vectores de primeras y segundas coordenadas de los puntos. Si son las dos columnas de un *data frame* de dos variables, lo podemos entrar en su lugar.

* `nbins` sirve para indicar los números de clases: podemos igualarlo a un único valor, y tomará ese número de clases sobre cada vector, o a un vector de dos entradas que indiquen el número de clases de cada vector.

* `col` sirve para especificar los colores a usar. Por defecto, los rectángulos vacíos aparecen de color negro, y el resto se colorean con tonalidades de rojo, de manera que los tonos más cálidos indican  frecuencias mayores. 


Además, podemos usar los parámetros usuales de `plot` para poner un título, etiquetar los ejes, etc.

A modo de ejemplo, vamos a dibujar el histograma bidimensional de las longitudes y anchuras de los pétalos de las flores iris, agrupando ambas dimensiones en los números de clases que da la regla de Freedman-Diaconis (y que calcula la función `nclass.FD`):
```{r,eval=FALSE}
library(gplots)
hist2d(iris$Petal.Length, iris$Petal.Width, 
  nbins=c(nclass.FD(iris$Petal.Length), 
  nclass.FD(iris$Petal.Width)))
```
Obtenemos (junto con una serie de información en la consola que hemos omitido) la Figura \@ref(fig:hist2d1), que podéis comparar con el diagrama de dispersión de los mismos datos de la Figura \@ref(fig:iris1).

```{r,echo=FALSE, label=hist2d1,fig.cap="Histograma bidimensional de longitudes y anchuras de pétalos de flores iris."}
library(gplots)
hist2d(iris$Petal.Length, iris$Petal.Width, 
  nbins=c(nclass.FD(iris$Petal.Length), 
  nclass.FD(iris$Petal.Width)))
```

En los histogramas bidimensionales con muchas regiones de diferentes frecuencias, es conveniente usar de manera adecuada los colores para representarlas. Una posibilidad es usar el paquete **RColorBrewer**, que permite elegir esquemas de colores bien diseñados. Las dos funciones básicas son:

* `brewer.pal(n,"paleta predefinida")`,  que carga en un vector de colores (una **paleta**) una secuencia de $n$ colores  de la **paleta predefinida** en el paquete.  Los nombres y contenidos de todas las paletas predefinidas que se pueden usar en esta función se obtienen, en la ventana de gráficos, ejecutando la instrucción `display.brewer.all()`. Por ejemplo, la paleta de colores de la Figura \@ref(fig:pal1).(a) se define con el código:
```{r}
library(RColorBrewer)
brewer.pal(11,"Spectral")
```


* `colorRampPalette(brewer.pal(...))(m)`, produce una nueva paleta de $m$ colores  a partir del resultado de `brewer.pal}, interpolando nuevos colores. Luego se puede usar la función `rev} para invertir el orden de los colores, lo que es conveniente en los histogramas bidimensionales si queremos que las frecuencias bajas correspondan a tonos azules y las frecuencias altas a tonos rojos. Así,
la paleta de colores que se define con
```{r}[style=item]
rev(colorRampPalette(brewer.pal(11,"Spectral"))(50))
```
es la de la Figura \@ref(exm:fig:pal1}.(b).
\end{itemize}


\begin{figure}[htb]
\abovecaptionskip=1pt
\begin{center}
\begin{tabular}{ccc}
\includegraphics[width=0.36\linewidth]{figpal1} &\hspace*{1cm} &\includegraphics[width=0.28\linewidth]{figpal2}\\[2ex] (a) &\hspace*{1cm} & (b)
\end{tabular}
\end{center}
\caption{(a) Paleta `brewer.pal(11,"Spectral")}; (b) Paleta `rev(colorRampPalette(brewer.pal(11,"Spectral"))} `(50))}.}\label{fig:pal1}
\end{figure}


Vamos a usar esta última paleta en un histograma bidimensional de la tabla de alturas de padres e hijos recogidas por Karl Pearson en 1903 y que tenemos guardada en  el **url}
\url{http://aprender.uib.es/Rdir/pearson.txt}; el resultado es la Figura \@ref(exm:fig:hist2dpearson}.
```{r}
df_pearson=read.table("http://aprender.uib.es/Rdir/pearson.txt", header=TRUE)
hist2d(df_pearson, nbins=30,
   col=rev(colorRampPalette(brewer.pal(11,"Spectral"))(50)))
```
\begin{figure}[htb]
\abovecaptionskip=1pt
\begin{center}
\includegraphics[width=0.45\linewidth]{fighist2d2}
\end{center}
\caption{Histograma bidimensional de las alturas de padres e hijos recogidas por Karl Pearson.}\label{fig:hist2dpearson}
\end{figure}

Para terminar, veamos como producir un gráfico conjunto de un histograma bidimensional y los dos histogramas unidimensionales.\footnote{\hspace*{0.5ex} Se trata de una modificación del gráfico similar explicado en \url{http://www.everydayanalytics.ca/2014/09/5-ways-to-do-2d-histograms-in-r.html}, el cual a su vez  se inspira en un gráfico de la p. 62 de \textsl{Computational Actuarial Science with R} de Arthur Charpentier (Chapman and Hall/CRC, 2014).} Considerad la función siguiente, cuyos parámetros son un *data frame* `df} de dos variables y un número `n} de clases, común para las dos variables:
```{r}
hist.doble=function(df,n){
  par.anterior=par()
  h1=hist(df[,1], breaks=n, plot=F)
  h2=hist(df[,2], breaks=n, plot=F)
  m=max(h1$counts, h2$counts)
  par(mar=c(3,3,1,1))
  layout(matrix(c(2,0,1,3),nrow=2,byrow=T), 
     heights=c(1,3), widths=c(3,1))
  hist2d(df, nbins=n, 
     col=rev(colorRampPalette(brewer.pal(11,"Spectral"))(50)))
  par(mar=c(0,2,1,0))
  barplot(h1$counts, axes=F, ylim=c(0, m), col="red")
  par(mar=c(2,0,0.5,1))
  barplot(h2$counts, axes=F, xlim=c(0, m), col="red", horiz=T)
  par.anterior}
```
Entonces,
```{r}
hist.doble(df_pearson,25)
```
produce la Figura \@ref(exm:fig:hist2dcomplet}.

\begin{figure}[htb]
\abovecaptionskip=1pt
\begin{center}
\includegraphics[width=0.45\linewidth]{fighist2dcomplet}
\end{center}
\caption{Histograma bidimensional con histogramas unidimensionales de las alturas de padres e hijos recogidas por Karl Pearson.}\label{fig:hist2dcomplet}
\end{figure}


Algunas explicaciones sobre el código, por si lo queréis modificar:
\begin{itemize}
\item Hemos "simulado>los histogramas mediante diagramas de barras de sus frecuencias absolutas, para poder dibujar horizontal el de la segunda variable.

\item El parámetro `axes=FALSE} en los `barplot} indica que no dibuje sus ejes de coordenadas.

\item La función `par} establece los parámetros generales básicos de los gráficos. Como con esta función los modificamos, guardamos los parámetros anteriores en `par.anterior} y al final los restauramos. 

\item El parámetro `mar} de la función `par} sirve para especificar, por este orden, los márgenes inferior, izquierdo, superior y derecho  de la próxima figura, en números de líneas. 

\item La instrucción `layout} divide la figura a producir en sectores con la misma estructura que la matriz de su primer argumento.
Dentro de esta matriz, cada entrada indica qué figura de las próximas se ha de situar en ese sector. Las alturas y amplitudes relativas de los sectores
se especifican con los parámetros `heights} y `widths}, respectivamente. Así, la instrucción
\begin{tightquote}
`layout(matrix(c(2,0,1,3),nrow=2,byrow=T), heights=c(1,3),}\\ \hspace*{2ex}`widths=c(3,1))}
\end{tightquote}
divide la figura en 4 sectores. Los sectores de la izquierda serán el triple de anchos que los de la derecha (`widths=c(3,1)}), y los sectores inferiores serán el triple de altos que los superiores (`heights=c(1,3)}). En estos sectores, R dibujará los próximos gráficos según el esquema definido por la matriz del argumento:
$$
\left(\begin{array}{cc}
\mbox{segundo} & \mbox{ninguno}\\
\mbox{primero} & \mbox{tercero}
\end{array}\right).
$$
\end{itemize}

## Guía rápida}


\begin{itemize}
\item `sapply(}*data frame*texttt{,}**función}`)} aplica la **función} a las columnas de un *data frame*.

\item `scale} sirve para aplicar una transformación lineal a una matriz o a un *data frame*. Sus parámetros son:
\begin{itemize}
\item   `center}: especifica  el vector que restamos a sus columnas.

\item  `scale}: especifica  el vector por el que dividimos sus columnas.
\end{itemize}

\item `cov`, aplicada a dos vectores, calcula su covarianza muestral; aplicada a un *data frame* o a una matriz, calcula su matriz de covarianzas muestrales.
Dispone del parámetro `use}, que:
\begin{itemize}
\item Igualado a `"pairwise.complete.obs"}, calcula la covarianza de cada par de columnas  teniendo en cuenta sólo sus observacions completas (las filas en las que ninguna de las dos tiene un `NA`), independientemente del resto de la tabla.
\item Igualado a `"complete.obs"}, calcula las covarianzas de las columnas teniendo en cuenta sólo las filas completas de toda la matriz.
\end{itemize}

\item `cor`, aplicada a dos vectores, calcula su correlación de Pearson; aplicada a un *data frame* o a una matriz, calcula su matriz de correlaciones de Pearson. Se puede usar el parámetro `use}  de `cov`. Usando el parámetro `method="spearman"} calcula la correlación (o las correlaciones, si se aplica a un *data frame* o a una matriz) de Spearman.


\item `cov2cor}, aplicada a la matriz de covarianzas, calcula la matriz de correlaciones de Pearson.

\item `cor.test} realiza un contraste de correlación, con hipótesis nula que la correlación poblacional sea 0. Su sintaxis es la usual en funciones de contrastes.

\item `pwr.r.test}, del paquete `pwr}, sirve para calcular la potencia de un contraste de correlación. Sus parámetros son `n}, el tamaño de las muestras, `r}, su correlación de Pearson, 
`sig.level}, el nivel de significación, `power}, la potencia, y 
`alternative}, el tipo de contraste. Si se entran los valores de tres de los cuatro primeros parámetros, se obtiene el cuarto.

\item `upper.tri}, aplicada a una matriz cuadrada $M$, produce la matriz  triangular superior de valores lógicos del mismo orden que $M$. Con el parámetro `diag=TRUE} se impone que el triángulo de valores `TRUE` incluya la diagonal principal.


\item `lower.tri}, aplicada a una matriz cuadrada $M$, produce la matriz  triangular inferior de valores lógicos del mismo orden que $M$. Dispone del mismo  parámetro `diag=TRUE}.

\item `order} ordena el primer vector al que se aplica, desempatando empates mediante el orden de los vectores subsiguientes a los que se aplica; el parámetro `decreasing=TRUE} sirve para especificar  que sea en orden decreciente. 

\item `plot}, aplicado a un *data frame* de dos variables numéricas, dibuja su diagrama de dispersión; 
aplicado a un *data frame* de más de dos variables numéricas, produce la matriz formada por los diagramas de dispersión de todos sus pares de variables.

\item `pairs} es equivalente a `plot} en el sentido anterior, y se puede aplicar a matrices.

\item`spm}, del paquete `cars}, produce matrices de dispersión más informativas y fáciles de modificar.

\item `scatterplot3d}, del paquete homónimo,  dibuja diagramas de dispersión tridimensionales. 

\item `hist2d}, del paquete `gplots}, dibuja histogramas bidimensionales. Dispone de los parámetros específicos siguientes:
\begin{itemize}
\item `nbins}: indica los números de clases.

\item `col}: especifica la paleta de colores que ha de usar para representar las frecuencias. 
\end{itemize}

\item `brewer.pal(}$n$`, "}**paleta predefinida}`")}, del paquete `RColorBrewer},  carga en una paleta de colores una secuencia de $n$ colores  de la **paleta predefinida} en dicho paquete.  



\item `colorRampPalette(brewer.pal(\ldots))(}$m$`)}, del paquete `RColorBrewer},  genera una nueva paleta de $m$ colores  a partir del resultado de `brewer.pal}, interpolando nuevos colores. 

\item `display.brewer.all()}, del paquete `RColorBrewer}, muestra los nombres y contenidos de todas las paletas predefinidas en dicho paquete. 


\item `par} sirve para establecer los parámetros generales básicos de los gráficos. 

\item `layout} divide en sectores la figura a producir, para que pueda incluir varios gráficos independientes simultáneamente.
\end{itemize}


\newpage
\thispagestyle{empty}


%
%## Ejercicio}
%
% El fichero \url{http://bioinfo.uib.es/~recerca/RMOOC/NotasMatesI14.txt} recoge las notas medias (sobre 100) obtenidas en las diferentes actividades de evaluación de la asignatura Matemáticas I del grado de Biología, en el curso 2013/14, por parte de los estudiantes que fueron considerados "presentados>en la primera convocatoria. Estas actividades consistieron en:
%\begin{itemize}
%\item Dos controles (columnas `Control1} y `Control2}).
%\item Talleres de resolución de problemas (columna `Talleres}).
%\item Ejercicios para resolver en casa (columna `Casa}).
%\item Cuestionarios en línea sobre los contenidos de la asignatura y sobre R 
% (columnas `TestsCont} y `TestsR}, respectivamente).
%\end{itemize}
%Cargad este fichero en un \textsl{data frame}.
%
%
%\begin{enumerate}[(a)]
%\item Calculad el vector de medias y el vector de desviaciones típicas de esta tabla de datos. ¿Cuáles son las actividades de evaluación cuyas notas presentan mayor y menor variabilidad? 
%
%\item Calculad las matrices de covarianzas y de correlaciones de esta tabla de datos. 
%
%\item ¿Qué variable tiene la mayor correlación media con las otras variables? ¿Cuál  tiene la menor?
%
%\item Ordenad los pares de variables de esta tabla por su correlación. ¿Cuáles son los dos pares con mayor correlación positiva?
% ¿Cuáles son los dos pares con menor correlación negativa?
% 
%\item Comprobad en esta tabla de datos que su matriz de correlaciones es igual a la matriz de covarianzas de su tabla tipificada.
%
%\item Dibujad una matriz de diagramas de dispersión de estas notas. ¿Se pueden ver en este diagrama los 
% dos pares de actividades de evaluación con mayor correlación y los dos pares con menor correlación que habéis encontrado en el apartado (d)?
% 
%\end{enumerate}
%


